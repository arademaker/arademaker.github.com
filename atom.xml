<?xml version="1.0" encoding="utf-8" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Alexandre Rademaker</title>
 <link href="http://arademaker.github.com/atom.xml" rel="self"/>
 <link href="http://arademaker.github.com/"/>
 <updated>2018-01-02T09:02:48-02:00</updated>
 <id>http://arademaker.github.com/</id>
 <author>
   <name>Alexandre Rademaker</name>
   <email>arademaker@gmail.com</email>
 </author>

 
 <entry>
   <title>anotações sintáticas</title>
   <link href="http://arademaker.github.com/blog/2017/06/06/dependencias.html"/>
   <updated>2017-06-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2017/06/06/dependencias</id>
   <content type="html">&lt;p&gt;Dada a sentença &amp;#8220;O garoto, que mora na rua 12, estava correndo atrás
  da bola.&amp;#8221;, são várias as análises sintáticas possíveis dependendo do
  formalismo adotado. Estes formalismos dividem-se em duas grandes
  classes: `phrase-structure` ou `dependencies` mas existem variações
  entre teorias dentro destas classes. Vejamos dois formalismos de
  dependências.&lt;/p&gt;
&lt;p&gt;O sistema &lt;a href=&quot;http://visl.sdu.dk/visl/pt/parsing/automatic/dependency.php&quot;&gt;PALAVRAS&lt;/a&gt; tem como saída padrão para a análise por
  dependências da sentença acima, o seguinte trecho:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
O [o] &amp;lt;*&amp;gt; &amp;lt;artd&amp;gt; DET M S @&amp;gt;N #1-&amp;gt;2
garoto [garoto] &amp;lt;Hbio&amp;gt; N M S @SUBJ&amp;gt; #2-&amp;gt;11
, #3-&amp;gt;0
que [que] &amp;lt;clb&amp;gt; &amp;lt;clb-fs&amp;gt; &amp;lt;rel&amp;gt; SPEC M S @SUBJ&amp;gt; #4-&amp;gt;5
mora [morar] &amp;lt;vK&amp;gt; &amp;lt;mv&amp;gt; &amp;lt;np-close&amp;gt; V PR 3S IND VFIN @FS-N&amp;lt; #5-&amp;gt;2
em [em] &amp;lt;sam-&amp;gt; PRP @&amp;lt;SA #6-&amp;gt;5
a [o] &amp;lt;-sam&amp;gt; &amp;lt;artd&amp;gt; DET F S @&amp;gt;N #7-&amp;gt;8
rua [rua] N F S @P&amp;lt; #8-&amp;gt;6
12 [12] &amp;lt;card&amp;gt; NUM M/F P @&amp;lt;SC #9-&amp;gt;5
, #10-&amp;gt;0
estava [estar] &amp;lt;fmc&amp;gt; &amp;lt;aux&amp;gt; V IMPF 3S IND VFIN @FS-STA #11-&amp;gt;0
correndo [correr] &amp;lt;clb&amp;gt; &amp;lt;mv&amp;gt; V GER @ICL-AUX&amp;lt; #12-&amp;gt;11
atrás de [atrás=de] &amp;lt;sam-&amp;gt; PRP @&amp;lt;ADVL #13-&amp;gt;12
a [o] &amp;lt;artd&amp;gt; &amp;lt;-sam&amp;gt; DET F S @&amp;gt;N #14-&amp;gt;15
bola [bola] &amp;lt;cc&amp;gt; &amp;lt;tool&amp;gt; &amp;lt;food-c-h&amp;gt; &amp;lt;act&amp;gt; N F S @P&amp;lt; #15-&amp;gt;13
. #16-&amp;gt;0
&lt;/pre&gt;
&lt;p&gt;Um parser de dependências treinado com um corpus anotado seguindo o
  modelo de dependências &lt;a href=&quot;http://universaldependencies.org&quot;&gt;Universal Dependencies&lt;/a&gt;, neste caso o corpus
  UD_Portuguese (Bosque em UD produzido pelo nosso grupo), produz a
  seguinte saída para a mesma sentença:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
1	O	_	DET	DET	_	2	det	_	_
2	garoto	_	NOUN	NOUN	_	12	nsubj	_	SpaceAfter=No
3	,	_	PUNCT	.	_	5	punct	_	_
4	que	_	PRON	PRON	_	5	nsubj	_	_
5	mora	_	VERB	VERB	_	2	acl:relcl	_	_
6-7	na	_	_	_	_	_	_	_	_
6	en	en	ADP	ADP	_	8	case	_	_
7	a	o	DET	DET	Definite=Def|Gender=Fem|Number=Sing|PronType=Art	8	det	_	_
8	rua	_	NOUN	NOUN	_	5	nmod	_	_
9	12	_	NUM	NUM	NumType=Card	8	appos	_	SpaceAfter=No
10	,	_	PUNCT	.	_	5	punct	_	_
11	estava	_	AUX	AUX	_	12	aux	_	_
12	correndo	_	VERB	VERB	_	0	root	_	_
13	atrás	_	ADV	ADV	_	12	advmod	_	_
14-15	da	_	_	_	_	_	_	_	_
14	de	de	ADP	ADP	_	16	case	_	_
15	a	o	DET	DET	Definite=Def|Gender=Fem|Number=Sing|PronType=Art	16	det	_	_
16	bola	_	NOUN	NOUN	_	13	nmod	_	SpaceAfter=No
17	.	_	PUNCT	.	_	12	punct	_	_
&lt;/pre&gt;
&lt;p&gt;Existem diferenças teóricas e técnicas entre estes dois modelos de
  dependências. As diferenças teórias são as que caracterizam cada
  modelo ou formalismo. Por exemplo, em UD, o `root` da sentença, o nó
  raiz da árvore sintática, é o verbo `correr`. Para o PALAVRAS o root é
  o verbo `estar` que, alias, não é exatamente o root, porque o root
  para o PALAVRAS é um nó 0, que não tem nenhum token
  associado. PALAVRAS identificou `atrás de` como uma MWE funcionando
  como preposição enquanto UD não fez o mesmo agrupamento. Para o
  PALAVRAS, todas as pontuações apontam para o root da sentença. Para
  UD, o tratamento das pontuações não é tão simples. Vale perceber que o
  parser &lt;a href=&quot;http://lindat.mff.cuni.cz/services/udpipe/run.php&quot;&gt;UDPipe&lt;/a&gt;, que usei para produzir a saída acima, errou no
  desmembramento da contração `na`.&lt;/p&gt;
&lt;p&gt;Mas para revisão do corpus, estou agora mais interessado nas
  diferenças técnicas. Ambas as saídas codificam de forma diferente
  várias informações: os links de dependência, as POS tags, as features,
  lemas etc. Ambos representam um token por linha, mas o PALAVRAS
  apresenta as informações de cada token de uma forma mais `flat`, uma
  &lt;i&gt;sopa&lt;/i&gt; de símbolos, apelando para alguns caracteres especiais que
  identificam os tipos de cada símbolo. Lema entre conchetes, tags e
  features em maiusculas, relações sintáticas começam com o símbolo `@`
  e outras tags sintáticas e semânticas entre `&amp;lt;&amp;#8230;&amp;gt;`. Em contrapartida,
  o formato CoNLL-U adotado por UD propõe que cada informação esteja em
  uma coluna de um formato tabular. Em UD, as features são
  explicitamente definidas, por exemplo, `Gender=Fem`. No PALAVRAS o
  símbolo `F` codifica esta mesma informação.&lt;/p&gt;
&lt;p&gt;Pensando na tarefa de revisão de anotações sintáticas, qual formato
  seria mais adequado para edições? Quais outros formatos possíveis
  existem? Esta discussÃo é certamente menos relevante se adotarmos uma
  postura de revisão de corpora centrada no suporte de alguma ferramenta
  de anotação, como &lt;a href=&quot;http://brat.nlplab.org&quot;&gt;Brat&lt;/a&gt; ou &lt;a href=&quot;https://webanno.github.io/webanno/&quot;&gt;Webanno&lt;/a&gt;. Mas a verdade é que nenhuma destas
  ferramentas é tão flexível como a edição direta de arquivos texto com
  suporte de alguma interface de visualização e `debug` da anotação.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ideas of Projects for 2017</title>
   <link href="http://arademaker.github.com/blog/2017/02/22/projects.html"/>
   <updated>2017-02-22T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2017/02/22/projects</id>
   <content type="html">&lt;p&gt;Some possible ideas for students looking for projects: &amp;#8216;iniciação
  científica&amp;#8217;.&lt;/p&gt;
&lt;h2&gt;Text Entailment&lt;/h2&gt;
&lt;p&gt;We propose a project to evaluate different techniques for &lt;a href=&quot;https://en.wikipedia.org/wiki/Textual_entailment&quot;&gt;text
  entailment&lt;/a&gt; using deep parsing. We were particularly interested in
  ‘deep’ linguistic processing of sentences. The goal is the combination
  of linguistic and statistical processing methods for getting at the
  meaning of texts and utterances.  For the experiments, we propose the
  use of the &lt;a href=&quot;http://clic.cimec.unitn.it/composes/sick.html&quot;&gt;SICK&lt;/a&gt; corpus and it was the corpus used in the &lt;a href=&quot;http://alt.qcri.org/semeval2014/task1/&quot;&gt;SemEval 2014&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some tools/ideas under consideration are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.03196&quot;&gt;Universal DepLambda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/percyliang/sempre&quot;&gt;Sempre&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nlp.stanford.edu/pubs/schuster2016enhanced.pdf&quot;&gt;Enhanced Dependencies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/uwnlp/EasySRL&quot;&gt;EasySRL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://amr.isi.edu/&quot;&gt;AMR&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dependency Parser for Portuguese in FreeLing&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://nlp.lsi.upc.edu/freeling/&quot;&gt;Freeling&lt;/a&gt; is a developer-oriented library providing language analysis
  services.  Freeling has already a good support for Portuguese in all
  its base modules (tokenizer, sentence splitter, POS tagger, WSD,
  etc.). We want to extend that support with a dependency parser for
  Portuguese.  This project is about to understand how to train a parser
  in FreeLing and make it for Portuguese, evaluating the result.&lt;/p&gt;
&lt;p&gt;For the training we can use the recently released UD_Portuguese data
  under the &lt;a href=&quot;http://universaldependencies.org/&quot;&gt;Universal Dependencies&lt;/a&gt; project.&lt;/p&gt;
&lt;h2&gt;SUO-KIF translator to TPTP&lt;/h2&gt;
&lt;p&gt;We have rewrote the &lt;a href=&quot;https://github.com/own-pt/cl-krr&quot;&gt;translation&lt;/a&gt; from &lt;a href=&quot;http://www.adampease.org/OP/&quot;&gt;SUO-KIF&lt;/a&gt; logic language to &lt;a href=&quot;http://www.cs.miami.edu/~tptp/&quot;&gt;TPTP&lt;/a&gt;
  language. In this project we want to expand the translation of
  high-order construction to TPTP/THF. In the sequence, we want to make
  the output readable to &lt;a href=&quot;http://www.ai.sri.com/~stickel/snark.html&quot;&gt;SNARK&lt;/a&gt; prover to explore its support to
  Procedural Attachments.&lt;/p&gt;
&lt;p&gt;Ideally, the translator should be written in logic or functional
  programming style using: Prolog, Haskell or Common Lisp, etc.&lt;/p&gt;
&lt;h2&gt;CoNLL-U and Universal Dependencies toolset&lt;/h2&gt;
&lt;p&gt;The creation of an annotated corpus with dependencies is a hard task
  and very time-consuming. We are collaborating with the &lt;a href=&quot;http://universaldependencies.org/&quot;&gt;Universal
  Dependencies&lt;/a&gt; Project, with a Portuguese Corpus (&lt;a href=&quot;http://github.com/universaldependencies/ud_portuguese&quot;&gt;UD_Portuguese&lt;/a&gt;). After
  release 2.0, we are now preparing for the next version expanding and
  solving errors in the current 2.0 corpus.&lt;/p&gt;
&lt;p&gt;In this project, we are interested in improving the necessary tools
  that we use:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/own-pt/cl-conllu&quot;&gt;CL-CONLLU&lt;/a&gt; : a Common Lisp library for work with CoNLL-U files&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/own-pt/conll-workbench&quot;&gt;conllu-workbench&lt;/a&gt; : a set of opensource tools that we use for
    searching and editing the corpus.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In particular, the CL library needs better support for rules and
  functions for comparing different trees and help in the identification
  of common patterns of errors.&lt;/p&gt;
&lt;h2&gt;Improving the openWordnet-PT interface&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;http://wnpt.brlcloud.com/wn/&quot;&gt;OpenWordnet-PT&lt;/a&gt;, abbreviated as OpenWN-PT or simply OWN-PT) is a
  open access wordnet for Portuguese, originally developed by Valeria de
  Paiva, Alexandre Rademaker and Gerard de Melo as a syntactic
  projection of Universal WordNet (UWN) of de Melo and Weikum. Like many
  other open wordnets we believe that lexical resources need to be open
  to be useful.&lt;/p&gt;
&lt;p&gt;The OpenWN-PT is available in RDF/OWL, following and expanding, when
  necessary, the mappings from the original Princeton WordNet. Both the
  data and the RDF template settings (classes and properties) of the
  OpenWN-PT are freely available for download here. Besides being
  downloadable, the data can be retrieved via SPARQL in the endpoint and
  one can consult and compare it with other wordnets at the generic
  interface provided by the Open MultiLingual WordNet project.&lt;/p&gt;
&lt;p&gt;This project is about helping our team in the improving of the &lt;a href=&quot;https://github.com/own-pt/cl-wnbrowser&quot;&gt;web
  interface&lt;/a&gt; for our openWordnet-PT.&lt;/p&gt;
&lt;p&gt;In particular, we need to: (1) simplify the architecture; (2) improve
  the interface for votes and suggestions; (3) improve navegation and
  data visualization.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>web crawler in Racket</title>
   <link href="http://arademaker.github.com/blog/2016/10/01/web-crawler-racket.html"/>
   <updated>2016-10-01T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/10/01/web-crawler-racket</id>
   <content type="html">&lt;p&gt;I usually like to suggest projects for students as part of their
  evaluation in the &amp;#8216;programming language&amp;#8217; course. This course uses
  &lt;a href=&quot;http://racket-lang.org&quot;&gt;Racket&lt;/a&gt; language and we follow the &lt;a href=&quot;https://mitpress.mit.edu/sicp/full-text/book/book.html&quot;&gt;SICP&lt;/a&gt; book. So the question is always
  what are the good projects for the students. Getting data from
  different source and combine then in a flexible user interface is a
  very common idea. Today I decide to investigate the difficult of
  developing a simple web crawler in Racket. Since I usually code in
  Common Lisp, I was looking for something similar of CL libs like
  &lt;a href=&quot;http://www.weitz.de/drakma/&quot;&gt;drakma&lt;/a&gt; and &lt;a href=&quot;https://common-lisp.net/project/closure/closure-html/&quot;&gt;Closure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using DrRacket only for teaching is not enough for being confortable
  with the Racket ecosystem. First I had to discover how to install the
  HTML parsing lib. This was done with:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bash&quot;&gt;
raco pkg install html-parsing
&lt;/pre&gt;
&lt;p&gt;After I have decided what libs to use, I had to understand their
  interfaces. My first code in Racket for retrieve and parse a simple
  HTML page is:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;scheme&quot;&gt;
#lang racket

(require net/http-client)
(require html-parsing)

(let-values (((a b c) (http-sendrecv &amp;quot;arademaker.github.io&amp;quot;
                                     &amp;quot;/about.html&amp;quot;)))
  (html-&amp;gt;xexp c))
&lt;/pre&gt;
&lt;p&gt;Not sure if this is the most efficiente way to make it, but surelly it
  is simple enough to start with.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Project Ideas</title>
   <link href="http://arademaker.github.com/blog/2016/09/17/projects.html"/>
   <updated>2016-09-17T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/09/17/projects</id>
   <content type="html">&lt;h1&gt;Knowledge Representation via ACE&lt;/h1&gt;
&lt;p&gt;Following the article https://arxiv.org/abs/1303.4293, we can think in
  a lot of possible extensions. For Portuguese support, we would need to
  develop the Portuguese concrete syntax in GF. The idea of code the
  translation of ACE to OWL in GF would be very interesting to explore.&lt;/p&gt;
&lt;h1&gt;Learning support tools&lt;/h1&gt;
&lt;p&gt;We would like to have an environment similar to
  https://www.hackerrank.com for receiving submissions of students
  projects.&lt;/p&gt;
&lt;h1&gt;SUMO to TFF and HTF&lt;/h1&gt;
&lt;p&gt;We have an SUMO to TPTP/FOF translation in
  https://github.com/own-pt/cl-krr. We would like to extend to TFF and
  later to TFF or THF. Alternatively, we can also translate to
  http://www.ai.sri.com/~stickel/snark.html language.&lt;/p&gt;
&lt;h1&gt;Semantic Web technologies&lt;/h1&gt;
&lt;p&gt;Both systems were realised opensource, much can be done to improve
  them! We need Common Lisp libraries for Linked Data and related
  technologies:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://json-ld.org&quot;&gt;json-ld&lt;/a&gt;: some initial attempt made from &lt;a href=&quot;https://github.com/RDProjekt/cl-json-ld&quot;&gt;cl-json-ld&lt;/a&gt; and
    maybe some references from &lt;a href=&quot;http://allegrograph.com/rdf-json/&quot;&gt;Franz&amp;#8217;s code&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://owlapi.sourceforge.net&quot;&gt;owlapi&lt;/a&gt;: some references at &lt;a href=&quot;http://www.cliki.net/rdf&quot;&gt;cliki&lt;/a&gt;. Common Lisp really need an OWLAPI
    library.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ha-mo-we/Racer&quot;&gt;Racer&lt;/a&gt; and &lt;a href=&quot;http://wilbur-rdf.sourceforge.net&quot;&gt;Wilbur&lt;/a&gt; are very nice libs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I would be very happy to supervise a work (undergrad or masters) to
  develop such libraries in Common Lisp.&lt;/p&gt;
&lt;h1&gt;Lisp library for metadata extraction&lt;/h1&gt;
&lt;p&gt;Contribute to projects like http://code.google.com/p/cl-jpegmeta/ and
  http://www.xach.com/lisp/zpb-exif/ to improve both library and the
  hability to handle IIM-style IPTC fields, EXIF fields and XMP
  metadata.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Simple queries in openWordnet-PT</title>
   <link href="http://arademaker.github.com/blog/2016/07/28/openwordnet-pt.html"/>
   <updated>2016-07-28T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/07/28/openwordnet-pt</id>
   <content type="html">&lt;p&gt;Our &lt;a href=&quot;http://wnpt.brlcloud.com/wn/&quot;&gt;OpenWordnet-PT&lt;/a&gt; is freely available for download and online use
  since its beginning. Nevertheless, some people still have difficulties
  to use the data without a proper introduction to our &amp;#8216;data
  model&amp;#8217;. Although we have already presented it in many conferences and
  articles, I believe some examples of queries can help people
  understand better our data.&lt;/p&gt;
&lt;p&gt;All relations are between synsets from PWN (Princeton). Since we
  haven&amp;#8217;t created any new synset yet, all our synsets are linked to
  Princeton Synsets via &lt;code&gt;owl:sameAs&lt;/code&gt; relation. Thus, our network is a
  projection of the PWN network, we have a injective map between our
  synsets and PWN synsets. Obviously, we have new senses and new words
  and these resources are linked to our synsets.&lt;/p&gt;
&lt;p&gt;In other words, to know the hypernyms of the word &amp;#8220;cachorro&amp;#8221; one must
  &amp;#8216;use&amp;#8217; the PWN synsets and relations:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sparql&quot;&gt;
select ?sspt ?otherpt ?otherword 
{
  ?word wn30:lexicalForm &amp;quot;cachorro&amp;quot;@pt .
  ?sspt wn30:containsWordSense/wn30:word ?word .
  ?ssen owl:sameAs ?sspt .   
  ?ssen wn30:hyponymOf+ ?other .
  ?other owl:sameAs ?otherpt .
  ?otherpt wn30:containsWordSense/wn30:word/wn30:lexicalForm ?otherword .
}
&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;hyponymOf+&lt;/code&gt; is a SPARQL 1.1 construction (&lt;a href=&quot;http://www.w3.org/TR/sparql11-query/#propertypaths&quot;&gt;property
  paths&lt;/a&gt;). It means the transitive closure of the &lt;code&gt;hyponymOf&lt;/code&gt;
  relation. The idea is to first get the synset in OWN-PT which contains
  &amp;#8220;cachorro&amp;#8221;, then find the equivalent synset in PWN. With the right
  synsets in PWN, we look for the related ones in OWN-PT and return
  them. Finally, we get the words from the OWN-PT synsets that we found.&lt;/p&gt;
&lt;p&gt;Note also that not all relations are between synsets, some relations
  such as &lt;code&gt;derivationallyRelated&lt;/code&gt; are relation between senses:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sparql&quot;&gt;
select ?s ?p
{
   ?s wn30:derivationallyRelated ?p.
}
&lt;/pre&gt;
&lt;p&gt;We associate synsets via &lt;code&gt;skos:inScheme&lt;/code&gt; to two special resources
  representing the PWN and OWN-PT wordnets to facilitate queries.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sparql&quot;&gt;
select distinct ?schema
{
   ?wsa wn30:derivationallyRelated ?wsb .
   ?ss skos:inScheme ?schema ;
       wn30:containsWordSense ?wsa .
}
&lt;/pre&gt;
&lt;p&gt;Our data model is described in the &lt;a href=&quot;https://github.com/own-pt/openWordnet-PT/blob/master/wn30.ttl&quot;&gt;wn30.ttl&lt;/a&gt; file. This is our
  &amp;#8216;vocabulary&amp;#8217; in Semantic Web terms. The queries above can be tested in
  our &lt;a href=&quot;http://wnpt.brlcloud.com:10035/repositories/wn30&quot;&gt;SPARQL endpoint&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sudoku as programming and logic exercise</title>
   <link href="http://arademaker.github.com/blog/2016/07/19/sudoku.html"/>
   <updated>2016-07-19T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/07/19/sudoku</id>
   <content type="html">&lt;p&gt;Last semester, at some point in my course on Data Structures and
  Algorithms, once again I mentioned the SUDOKU problem. I am sure that
  I haven&amp;#8217;t covered all about it yet, and this post is just to remember
  me about things that I would like to came back at some point:&lt;/p&gt;
&lt;p&gt;In Common Lisp, some libraries make the problem so easy that is hard
  to explain to the students why the logic based approaches are so
  challenging. Examples are: &lt;a href=&quot;https://common-lisp.net/project/computed-class/index-old.shtml&quot;&gt;computed-class&lt;/a&gt;, &lt;a href=&quot;https://github.com/kennytilton/cells/wiki&quot;&gt;cells&lt;/a&gt; and &lt;a href=&quot;http://www.cliki.net/screamer&quot;&gt;screamer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sudoku as SAT is documented in the article [3] and we know that a
  better encoding should be possible. I would love to continue the
  experiments with &lt;a href=&quot;http://www.ai.sri.com/~stickel/snark.html&quot;&gt;SNARK&lt;/a&gt; theorem prover following ideas from the
  papers [1] and [2].&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;T. Hillenbrand, D. Topic, and C. Weidenbach, &amp;#8220;Sudokus as Logical
    Puzzles&amp;#8221;, pp. 1–11, Apr. 2016.&lt;/li&gt;
  &lt;li&gt;G. Santos-García and M. Palomino, &amp;#8220;Solving Sudoku Puzzles with
    Rewriting Rules&amp;#8221;, pp. 1–16, 2006.&lt;/li&gt;
  &lt;li&gt;I. Lynce and J. Ouaknine, &amp;#8220;Sudoku as a SAT Problem&amp;#8221;, &lt;a href=&quot;http://anytime.cs.umass.edu/aimath06/proceedings/P34.pdf&quot;&gt;online&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So many interesting things to do!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Semantic Links for Portuguese</title>
   <link href="http://arademaker.github.com/blog/2016/06/13/nominalizations.html"/>
   <updated>2016-06-13T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/06/13/nominalizations</id>
   <content type="html">&lt;p&gt;We have presented the paper &lt;a href=&quot;/bibliography/lrec-2016-morpholinks.html&quot;&gt;Semantic Links for Portuguese&lt;/a&gt; in the
  LREC 2016. As we already know, the paper is not the end of this work,
  possible the contrary of that. We already know some improvements
  needed and some related works that we sill have to analyse. This post
  is to register these information and list possible future works for
  the article.&lt;/p&gt;
&lt;p&gt;Thanks, Diana Santos for suggesting me the works:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Medeiros, José Carlos, Rui Marques &amp;amp; Diana Santos. &lt;a href=&quot;http://www.linguateca.pt/Diana/download/Medeirosetal93.pdf&quot;&gt;Português
    Quantitativo&lt;/a&gt;, Actas do 1.o Encontro de Processamento de Língua
    Portuguesa (Escrita e Falada) - EPLP&amp;#8217;93, (Lisboa, 25-26 de Fevereiro
    de 1993).&lt;/li&gt;
  &lt;li&gt;Alina Villalva, &lt;a href=&quot;https://www.uam.es/gruposinv/upstairs/upstairs2/curricula/trabajos/villalva_1995_estructuras.pdf&quot;&gt;Estruturas Morfológicas&lt;/a&gt;, thesis, Lisboa, 1994.&lt;/li&gt;
  &lt;li&gt;The thesis &amp;#8220;A complementação da forma nominalizada deverbal sufixal
    e a conceituação do complemento nominal&amp;#8221; by Rosa Marina de Brito
    Meyer, see &lt;a href=&quot;http://lattes.cnpq.br/0344501785488249&quot;&gt;thesis&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Articles in PROPOR or EBRALC or ELC from Violeta Quertal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We still need more experiments with corpora. Possible verifying the
  cases of zero-derivational words. We can use a generator of verbs
  forms (conjugator) for help with the annotation of candidates.&lt;/p&gt;
&lt;p&gt;We have cases like the verb &amp;#8216;aparecer&amp;#8217; and its nominalizations:
  &amp;#8216;aparição&amp;#8217;, &amp;#8216;aparecimento&amp;#8217; and &amp;#8216;aparência&amp;#8217;. All of them must be in the
  resource for sure. Words as &amp;#8216;coberta&amp;#8217; and &amp;#8216;cobertura&amp;#8217; should be also
  in the resource, but what about the less frequent ones?&lt;/p&gt;
&lt;p&gt;Ambiguities versus fine-grained classification. The semantic links
  &lt;i&gt;ação&lt;/i&gt; and &lt;i&gt;resultado&lt;/i&gt; do make sense in Portuguese? If the introduce
  too many classes (using classes from other languages), we are creating
  a problem, not dealing with one real problem.&lt;/p&gt;
&lt;p&gt;Valeria has also posted recently about our work with &lt;a href=&quot;http://logic-forall.blogspot.pt/2016/06/nominalizations-and-zombie-nouns.html&quot;&gt;nominalizations&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Congratuatios Guilherme Passos</title>
   <link href="http://arademaker.github.com/blog/2016/06/10/guilherme.html"/>
   <updated>2016-06-10T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/06/10/guilherme</id>
   <content type="html">&lt;p&gt;Guilherme Passos presented last week his final undergrad
  project. Congratulations Guilherme, well done. The project is an
  extension of the ideas from the book:&lt;/p&gt;
&lt;p&gt;P. Blackburn and J. Bos, Representation and inference for natural
  language. 2005&lt;/p&gt;
&lt;p&gt;Guilherme expands the code from the book to use the Princeton Wordnet
  for making a simple conversational chatbot that can understand some
  sentences (basically all type of sentences considered in the book)
  module some hyponyms and hypernyms from PWN, and provide some answers
  to the user. The links for the code and texts will be available soon
  here.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Solving the Puzzle</title>
   <link href="http://arademaker.github.com/blog/2016/04/03/puzzle.html"/>
   <updated>2016-04-03T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/04/03/puzzle</id>
   <content type="html">&lt;p&gt;Solving the &lt;a href=&quot;http://fivethirtyeight.com/features/can-you-solve-the-impossible-puzzle/&quot;&gt;Can You Solve The Impossible Puzzle?&lt;/a&gt; with Common Lisp and
  &lt;a href=&quot;http://orgmode.org&quot;&gt;Org Mode&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our search space contains 45 possible candidates of pairs of
  numbers. The tables below will have one candidate of a pair on each
  row. In the first column, we have the multiple, in the second column
  the sum and the last two columns the candidate pair. The &lt;code&gt;prune&lt;/code&gt;
  function will help us to reduce the search space on each interaction,
  filtering the elements without repetition.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(ql:quickload :group-by)

(defun prune (data func)
  (let* ((pre (group-by:group-by-repeated data :keys (list func)))
         (input (remove-if (lambda (p) (&amp;lt;= (length (cdr p)) 1)) pre)))
    (reduce (lambda (acc a) (append acc (cdr a))) input
            :initial-value &amp;#39;((m s x y)))))

(cons &amp;#39;(m s x y)
      (loop for x from 1 to 9
            append (loop for y from 9 downto x
                         collect (list (* x y) (+ x y) x y))))
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[ca79f9a22ba8cd356c32197a5ac0ad93f8159c81]: start&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;27&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;21&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;32&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;28&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;45&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;35&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;30&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;25&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;54&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;48&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;42&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;63&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;56&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;49&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;72&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;64&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;81&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In the first time that Barack asked Pete, if Pete knew the answer his
  multiple would be unique defined in the candidate list, that was not
  the case, so we must remove the multiples without repetitions.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[6db81e23c88ea3483e1865437f8de0f9cca170cd]: step-1&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;When Barack asked Susan for the first time, she already knew that Pete
  didn&amp;#8217;t know the answer either. So the candidate list in her mind is
  the list above. But she didn&amp;#8217;t know the answer of Barack&amp;#8217;s question
  either, so her sum are not unique in this list too.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[2ac1f2a3d955fbf7a89f6db99a91c8f902775483]: step-2&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In the second time that Barack asked Pete, he still didn&amp;#8217;t know. So we
  have to exclude all unique multiples again.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[d5b928145f97d2bea7471b63383a9e34d6178b5a]: step-3&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The same again for the second time Barack asked Susan:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[17b25e5fc689d147eda2bd35c388cde44f310568]: step-4&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Pete in the third time still didn&amp;#8217;t know.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[44d455fea1e59e9db1788bb012f6cdd4abcc32f1]: step-5&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Susan in the third still didn&amp;#8217;t know.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[7a0a13546e37c1bd0d1fff1059eb069b381cbd30]: step-6&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Pete once more didn&amp;#8217;t know:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[079394364b443353af7d9353a8c0a38835b09ee2]: step-7&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Susan in the fourth time didn&amp;#8217;t know either:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[e8f43474732654a4d80a659c50fe51b7ddba6a28]: step-8&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;At this moment, in the fifth time, Pete knew the answer. That is, his
  number should be 16, since this is the only multiple that unique
  defines the candidates: 2 and 8.&lt;/p&gt;
&lt;p&gt;If Pete didn&amp;#8217;t knew at this time, Barack would have asked once more to
  Susan and we would have to exclude the pair &lt;code&gt;(2,8)&lt;/code&gt; from the list of
  candidates:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[b04f949aa4cfaea9ae9f63533fc50b207336f698]: step-9&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In this candidate list, Susan would not be able to identify the
  numbers since no sum is unique.&lt;/p&gt;
&lt;h1&gt;Links&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jstatsoft.org/article/view/v046i03&quot;&gt;A Multi-Language Computing Environment for Literate Programming and
    Reproducible Research&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://orgmode.org&quot;&gt;http://orgmode.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://quicklisp.org&quot;&gt;http://quicklisp.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://common-lisp.net&quot;&gt;Common Lisp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Let over Lambda</title>
   <link href="http://arademaker.github.com/blog/2016/02/07/let-over-lambda.html"/>
   <updated>2016-02-07T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2016/02/07/let-over-lambda</id>
   <content type="html">&lt;iframe style=&quot;width:120px;height:240px;&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; src=&quot;//ws-na.amazon-adsystem.com/widgets/q?ServiceVersion=20070822&amp;OneJS=1&amp;Operation=GetAdHtml&amp;MarketPlace=US&amp;source=ac&amp;ref=tf_til&amp;ad_type=product_link&amp;tracking_id=alexanradema-20&amp;marketplace=amazon&amp;region=US&amp;placement=1435712757&amp;asins=1435712757&amp;linkId=2H7S2HFMK6B465XQ&amp;show_border=false&amp;link_opens_in_new_window=true&quot;&gt;
&lt;/iframe&gt;
&lt;p&gt;I am reading &amp;#8220;Let over Lambda&amp;#8221;. Amazing how truly is the comments and
  the words on the back cover. This book is really for Common Lisp
  developers with very solid background. I am still in Section 5.3, some
  part demands 1-2 days for real understanding. The problem isn&amp;#8217;t the
  text, actually, the book is very well-written, but the code blocks are
  hard to grasp: macros that define macros that define macros and so
  on&amp;#8230;&lt;/p&gt;
&lt;p&gt;I hope to update this post with more specific comments from my
  experiences through the book. So far, two observations:&lt;/p&gt;
&lt;p&gt;Don&amp;#8217;t try to use &lt;a href=&quot;http://sbcl.org&quot;&gt;SBCL&lt;/a&gt; for run the code. The way that SBCL encodes
  backquotes is incompatible with the code from Chapter 3. More in this
  &lt;a href=&quot;http://stackoverflow.com/questions/33724300/macros-that-write-macros-compile-error&quot;&gt;question&lt;/a&gt; of Stackoverflow.  I didn&amp;#8217;t find any information about it in
  the official &lt;a href=&quot;http://letoverlambda.com&quot;&gt;book website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I haven&amp;#8217;t investigated it further but it looks like CCL does not
  handle very well the &lt;a href=&quot;http://clhs.lisp.se/Body/02_dhq.htm&quot;&gt;read-time conditionalization&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
#+nil
(progn
  (defvar test (counter-class))
  (funcall test)
  (toogle-counter-direction)
  (funcall test))
&lt;/pre&gt;
&lt;p&gt;That is, the selection of no feature - one method to comment the
  following expression. I am still investigating this issue since it
  only appeared when I loaded the code using an ASDF system definition
  and &lt;a href=&quot;http://quicklisp.org&quot;&gt;quicklisp&lt;/a&gt;. Comments are welcome.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Building a shared world: Mapping distributional to model-theoretic semantic spaces</title>
   <link href="http://arademaker.github.com/blog/2016/01/03/building-shared-world.html"/>
   <updated>2016-01-03T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2016/01/03/building-shared-world</id>
   <content type="html">&lt;p&gt;The introduction of this papers is fascinating. The complementarity
  mentioned in the introduction has been in my mind for a while:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In recent years, the complementarity of distributional and formal
    semantics has become increasingly evident&amp;#8230; A number of proposals
    have emerged from these considerations, suggesting that an overarching
    semantics integrating both distributional and formal aspects would be
    desirable&amp;#8230;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another interesting part of the paper is its section 4.2, when the
  authors introduce their definition of &amp;#8216;model-theoretic spaces&amp;#8217;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Ontologies can be represented in various ways, but in this paper, we
    assume they are formalised in terms of sets of entities&amp;#8230; In our
    account, we do not have an a priori model of the world: we wish to
    infer it from our observation of language data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the next paragraphs, the authors precisely defined their
  &amp;#8216;model-theoretic spaces&amp;#8217;. The article is worth to reading.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bibtex&quot;&gt;
@InProceedings{herbelot-vecchi:2015:EMNLP,
    author    = {Herbelot, Aur\&amp;#39;{e}lie  and  Vecchi, Eva Maria},
    title     = {Building a shared world: mapping distributional to
                    model-theoretic semantic spaces},
    booktitle = {Proceedings of the 2015 Conference on Empirical Methods
                    in Natural Language Processing},
    month     = {September},
    year      = {2015},  address   = {Lisbon, Portugal},
    publisher = {Association for Computational Linguistics},
    pages     = {22--32},
    url       = {http://aclweb.org/anthology/D15-1003}
}
&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Merging RDF files</title>
   <link href="http://arademaker.github.com/blog/2015/08/18/combine-rdf.html"/>
   <updated>2015-08-18T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2015/08/18/combine-rdf</id>
   <content type="html">&lt;p&gt;How to merge multiple RDF files into a single RDF file? The first idea
  would be to convert each RDF file in &lt;a href=&quot;http://www.w3.org/2001/sw/RDFCore/ntriples/&quot;&gt;ntriples&lt;/a&gt; and just concatenate
  them using unix &lt;code&gt;cat&lt;/code&gt; utility, right? No, it doesn&amp;#8217;t work with blank
  nodes (or BNodes)! BNodes from different files with the same ID would
  be merged as a single resource and this is not the expected semantics,
  BNodes from different files are different resources, even if they have
  the same id.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rapper&lt;/code&gt; is a utility from the package &lt;a href=&quot;http://librdf.org&quot;&gt;Redland&lt;/a&gt;. Below I am
  presented the files and the number of triples on each one.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rapper -c -i ntriples wordnet-en-fixed.nt
rapper: Parsing returned 3517504 triples

$ rapper -c -i ntriples own-pt-fixed.nt
rapper: Parsing returned  824916 triples
&lt;/pre&gt;
&lt;p&gt;The oldest tool to support merging of RDF files is &lt;a href=&quot;http://www.w3.org/2000/10/swap/doc/cwm.html&quot;&gt;CWM&lt;/a&gt;. CWM is written
  in python and its performance is really bad. The command below hasn&amp;#8217;t
  finish after 5 minutes.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
/usr/local/cwm-1.2.1/cwm --ntriples own-pt-fixed.nt wordnet-en-fixed.nt &amp;gt; tudo-cwm.nt
&lt;/pre&gt;
&lt;p&gt;Next tool that I tried was &lt;a href=&quot;http://rdfpro.fbk.eu&quot;&gt;$RDF&lt;sub&gt;pro&lt;/sub&gt;$&lt;/a&gt;. The performance was excellent,
  only 11 seconds! But we must add a parameter &lt;code&gt;-w&lt;/code&gt; to force BNodes in
  input files to be renamed to avoid possible clashes. Actually, it
  doesn&amp;#8217;t make sense to me why this is not the default behaviour.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rdfpro @r -w own-pt-fixed.nt wordnet-en-fixed.nt @w tudo-pro.nt
14:45:53(I) 4342420 triples read (377077 tr/s avg)
14:45:53(I) 4342420 triples written (377077 tr/s avg)
14:45:53(I) Done in 11 s
&lt;/pre&gt;
&lt;p&gt;Next tool, &lt;code&gt;riot&lt;/code&gt; from the &lt;a href=&quot;https://jena.apache.org&quot;&gt;Jena&lt;/a&gt; library. The performance was not bad,
  it took twice the time of $RDF&lt;sub&gt;pro&lt;/sub&gt;$ but it finished. The only
  problem is that it complained about some IRI that no other tool
  complained.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ time riot own-pt-fixed.nt wordnet-en-fixed.nt &amp;gt; tudo-riot.nt
14:51:14 WARN riot :: [line: 282756, col: 1 ] Bad IRI: &amp;lt;https://w3id.org/own-pt/wn30-pt/instances/word-Ĳsselmeer&amp;gt; Code: 47/NOT_NFKC in PATH: The IRI is not in Unicode Normal Form KC.
14:51:14 WARN riot :: [line: 282756, col: 1 ] Bad IRI: &amp;lt;https://w3id.org/own-pt/wn30-pt/instances/word-Ĳsselmeer&amp;gt; Code: 56/COMPATIBILITY_CHARACTER in PATH: Bad character
...

real	0m27.398s
user	0m29.905s
sys	0m1.751s
&lt;/pre&gt;
&lt;p&gt;I don&amp;#8217;t like warnnings so I tried the safe path. I converted the
  ntriple file with these strange IRIs to RDF/XML and called &lt;code&gt;riot&lt;/code&gt;
  again. No warnnings this time, good!&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rapper -i ntriples -o rdfxml own-pt-fixed.nt  &amp;gt; own-pt-fixed.rdf
rapper: Serializing with serializer rdfxml
rapper: Parsing returned 824916 triples

$ riot --time own-pt-fixed.rdf wordnet-en-fixed.nt &amp;gt; tudo-riot.nt
own-pt-fixed.rdf : 14.84 sec  824,916 triples  55,602.32 TPS
wordnet-en-fixed.nt : 21.82 sec  3,517,504 triples  161,175.95 TPS
Total : 36.66 sec  4,342,420 triples  118,451.17 TPS
&lt;/pre&gt;
&lt;p&gt;But the output produced does have some errors! The IRIs are not
  encoded as the way the ntriples specification requires.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rapper -c -i ntriples tudo-riot.nt

rapper: Parsing URI tudo-riot.nt with parser ntriples
rapper: Error - URI tudo-riot.nt:117668 column 55 - Non-printable ASCII character 195 (0xC3) found.
rapper: Error - URI tudo-riot.nt:117668 column 56 - Non-printable ASCII character 162 (0xA2) found.
&lt;/pre&gt;
&lt;p&gt;By the way, for the future, I will use $RDF&lt;sub&gt;pro&lt;/sub&gt;$.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>VIVO Apps and Tools Webinar</title>
   <link href="http://arademaker.github.com/blog/2014/04/30/webinar.html"/>
   <updated>2014-04-30T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2014/04/30/webinar</id>
   <content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Yerterday I presented for the Apps and Tools working group my workflow
  to prepared data to be inserted into FGV VIVO instance. Since some
  people asked be to share the links and the file that I used to guide
  the presentation, I made this post.&lt;/p&gt;
&lt;p&gt;This page is generated from a &lt;a href=&quot;http://orgmode.org&quot;&gt;org file&lt;/a&gt; that I export to HTML and
  further processed with &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. The process of use org files with
  jekyll is outlined &lt;a href=&quot;http://orgmode.org/worg/org-tutorials/org-jekyll.html&quot;&gt;here&lt;/a&gt;. I plan to improve this workflow, but it is
  working for my personal website and for the websites of the courses
  that I teach at FGV.&lt;/p&gt;
&lt;h1&gt;The Toolset&lt;/h1&gt;
&lt;p&gt;This is the non comprehensive list of tools that I use. I am listing
  here the main tools that come up into my mind that should be
  interesting to others.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gnu.org/software/emacs/&quot;&gt;Emacs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://orgmode.org&quot;&gt;Org-Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://common-lisp.net/project/slime/&quot;&gt;Slime&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Common Lisp compilers and interpreters: &lt;a href=&quot;http://franz.com/products/allegro-common-lisp/&quot;&gt;Allegro CL&lt;/a&gt;, &lt;a href=&quot;http://www.sbcl.org&quot;&gt;SBCL&lt;/a&gt; and &lt;a href=&quot;http://abcl.org&quot;&gt;ABCL&lt;/a&gt;
    (Lisp on JVM, so I can use Java RDF libraries).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.w3.org/2000/10/swap/doc/cwm.html&quot;&gt;CWM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://franz.com/agraph/allegrograph/&quot;&gt;Allegro Graph Triplestore&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://franz.com/agraph/gruff/&quot;&gt;Gruff&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Git&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.r-project.org&quot;&gt;R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Python&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://xmlsoft.org&quot;&gt;xsltproc and xmllint&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tidy.sourceforge.net&quot;&gt;tidy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Data Sources&lt;/h1&gt;
&lt;p&gt;We have to main sources of data: (1) the FGV researchers&amp;#8217; curricula
  vitae from &lt;a href=&quot;http://lattes.cnpq.br&quot;&gt;Lattes Platform&lt;/a&gt; and; (2) the FGV digital library.&lt;/p&gt;
&lt;p&gt;During the webinar I shared my screen and presented the web interface
  that &lt;a href=&quot;http://cnpq.br&quot;&gt;CNPq&lt;/a&gt; provides for researchers update their resumes. I also shown
  &lt;a href=&quot;http://lattes.cnpq.br/0675365413696898&quot;&gt;my curriculum vitae&lt;/a&gt; and discussed why I do not consider Brazilian
  Researchers curricula vitae open data given the captcha that blocks
  crawlers to get XML files from the Lattes website in a batch mode.&lt;/p&gt;
&lt;p&gt;I forgot to mention during the webinar but one of my old dreams is to
  convice CNPq to add &lt;a href=&quot;http://www.w3.org/TR/xhtml-rdfa-primer/&quot;&gt;RDFa&lt;/a&gt; or https://schema.org microformats into the
  HTML pages of the curricula vitae. This would not only allow crawlers
  to easier process the data but could also facilitate the maintainance
  of the system. My current XSLT transformation of Lattes XML to RDF
  cloud provide the starting point to RDFa embeeding.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://bibliotecadigital.fgv.br/dspace&quot;&gt;FGV Digital Library&lt;/a&gt; runs Dspace. The publications and collections
  metadata are easly collected from Dspace using the &lt;a href=&quot;http://www.openarchives.org/&quot;&gt;OAI-PMH protocol&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Lattes XML Files&lt;/h1&gt;
&lt;p&gt;As I said in the previous section, the curricula vitae of Brazilian
  Researchers are not open data, that is, they are not public available
  in structured format. They are only public available as HTML pages in
  the Lattes website, with limited search interface. The only way to
  universities and research institutions get the curricula vitae of
  their researchers in a structured format is to sign an &lt;a href=&quot;http://www.cnpq.br/web/portal-lattes/acordos-institucionais&quot;&gt;aggrement&lt;/a&gt; with
  CNPq. FGV has signed this aggrement and we have one server authorized
  to access CNPq web server to retrive the XML files of all curriculae
  that have informed any professional activity with FGV.&lt;/p&gt;
&lt;p&gt;To transform the Lattes files to RDF, I use a XSLT transformation that
  I developed few years ago. The XSLT is freely available at github in
  the repository &lt;a href=&quot;https://github.com/arademaker/SLattes&quot;&gt;Semantic Lattes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this repo, I made also available the DTD that I try hard to keep
  up-to-date. Unfortunately, until recently, CNPq did not make public
  annoucement of chances in the structure of the XML files that they
  produce, so I had to adapt the DTD whenever I identify changes. I have
  just found in the end of &lt;a href=&quot;http://www.cnpq.br/web/portal-lattes/extracoes-de-dados&quot;&gt;this page&lt;/a&gt; that CNPq finally realized the
  importance of making available the updated DTD. Nevertheless, the DTD
  in the link of this page is outdated. At least, using this DTD to
  validate the 489 FGV&amp;#8217;s curriculae I got more than 100 erros but using
  my DTD to validate the same files I got only 2 errors. Considering
  that those 489 files were produced my CNPq, we have two options: (1)
  the DTD is outdated; or (2) the code that produces the XML files has
  bugs. The two erros that I find using my DTD occur only with two
  curriculae that were not updated in the last 2 years.&lt;/p&gt;
&lt;p&gt;After download the XML files, the general idea to process them and
  produce the RDF files is outlined in the code below:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sh&quot;&gt;
for f in $ROOT/ontos/xml/*.xml; do
    ID=$(basename $f .xml)
    echo Processing $ID
    xmllint --noout --dtdvalid $REPO/LMPLCurriculo.DTD $f 2&amp;gt;&amp;gt; error.log
    xsltproc --stringparam ID $ID $REPO/lattes.xsl $f &amp;gt; $ID.rdf  
done
&lt;/pre&gt;
&lt;p&gt;After that, I import the RDF files to Allegro Graph making each
  curriculum a separated graph so I can easly identify the provenance of
  each triple. The importation is done using the &lt;a href=&quot;http://franz.com/agraph/support/documentation/current/agload.html&quot;&gt;agload&lt;/a&gt; utility. The
  load process takes aprox. 2 minutes:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
Load finished 487 sources in 00:02:03 (123.02 seconds).  
Triples added: 1,690,538 
Average Rate: 13742.00 tps
&lt;/pre&gt;
&lt;h1&gt;Data Deduplication&lt;/h1&gt;
&lt;p&gt;I briefly commented about the deduplication of records during the
  webinar. I do have to take care of removing duplicated resources about
  the same entity. Considering a thesis defended by and student at FGV
  whom have as advisor a professor at FGV. I will have metadata
  (triples) about this thesis from three different sources: (1) the RDF
  produced from the advisor&amp;#8217;s curriculum lattes XML; (2) the RDF produce
  from the student&amp;#8217;s curriculum lattes XML; and (3) the RDF obtained
  from the FGV Digital Library.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://github.com/arademaker/vivo-code&quot;&gt;current code&lt;/a&gt; that I use to identify duplicated resources is a
  Common Lisp library that is easily used if placed inside the
  local-projects directory of a &lt;a href=&quot;http://www.quicklisp.org/&quot;&gt;Quicklisp&lt;/a&gt; instalation.&lt;/p&gt;
&lt;p&gt;I can write an entire article only about deduplication in RDF. I am
  still thinking hard about this problem and really would like to find
  better alternatives.  One can note that deduplication of nodes in a
  RDF graph should not be done type by type as I am doing now. The rules
  to identify resources as being refering the same entity could
  dependent each other. That is, the deduplication of instances of
  &lt;code&gt;foaf:Person&lt;/code&gt; can activate the rule to deduplicate instances of
  &lt;code&gt;bibo:Article&lt;/code&gt; and vice-versa. It would be better to have a kind of
  fixed point transformation in the RDF graph that could keep clustering
  nodes until nothing more can be done. As a logician, I am very
  interested in approach this problem in a more declarative and
  deductive way.&lt;/p&gt;
&lt;p&gt;I also have to note that &lt;code&gt;owl:sameAs&lt;/code&gt; semantics doesn&amp;#8217;t help here. I
  do use &lt;code&gt;owl:sameAs&lt;/code&gt; to mark the nodes that should be merged but I have
  to merge the nodes after all &lt;code&gt;owl:sameAs&lt;/code&gt; triples are produced. I do
  this with two SPARQL construct queries:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
delete { ?s1 ?p ?o . }
insert { ?s2 ?p ?o . }
where {
  ?s1 owl:sameAs ?s2 .
  ?s1 ?p ?o .
  filter( !sameTerm(?p, owl:sameAs) )
}
&lt;/pre&gt;
&lt;pre class=&quot;example&quot;&gt;
delete { ?x ?p ?o1 . }
insert { ?x ?p ?o2 . }
where {
  ?o1 owl:sameAs ?o2 .
  ?x ?p ?o1 .
  filter( !sameTerm(?p, owl:sameAs) )
}
&lt;/pre&gt;
&lt;p&gt;Note that the filters block the propagation of the &lt;code&gt;owl:sameAs&lt;/code&gt;
  triples.&lt;/p&gt;
&lt;h1&gt;Mapping Lattes RDF to VIVO RDF&lt;/h1&gt;
&lt;p&gt;To map the Lattes RDF model produced by my XSLT to the expected VIVO
  RDF model, I have to look carefully to each instance of data. This
  mapping is not completed but at this point I have already mapped most
  of the data about people, publication, research areas and
  departaments.&lt;/p&gt;
&lt;p&gt;To work on the rules and queries to transform the data, I used the
  query and data browsing tools developed by Franz: Gruff and
  AllegroGraph WebView. During the webinar I presented both systems.&lt;/p&gt;
&lt;p&gt;The mapping is developed as rules that were easly tested with CWM. One
  example of rules is&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
{ ?dept foaf:member ?person ;
        rdf:type foaf:Group . } =&amp;gt; 
{ [ vivo:relates ?dept ;
    vivo:relates ?person ;
    a vivo:FacultyPosition ;
    rdfs:label &amp;quot;Professor Adjunto&amp;quot;@pt ] . } .
&lt;/pre&gt;
&lt;p&gt;Rules like the one above are placed in an n3 file and executed by CWM
  that receives the rule file and the data file and produces the data
  output file. Unfortunately, CWM does not have good performance and I
  haven&amp;#8217;t even tried to use it with all the data. I develop the rules
  and test them with only one curriculum vitae file.&lt;/p&gt;
&lt;p&gt;Once I finish to test the rules, I rewrite them as SPARQL queries. The
  one above becomes:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
insert 
{ graph &amp;lt;http://www.fgv.br/vivo/import/&amp;gt; 
  {            
   [ vivo:relates ?dept ;
     vivo:relates ?person ;
      a vivo:FacultyPosition ;
     rdfs:label &amp;quot;Professor Adjunto&amp;quot;@pt ] . 
  }
}
where
{ ?dept foaf:member ?person ;
        rdf:type foaf:Group . 
}
&lt;/pre&gt;
&lt;p&gt;Note that: (1) the query produces blank nodes that need to be
  transformed into normal nodes before loaded into VIVO; (2) All created
  triples are placed in a separated graph; and (3) if this query is
  executed twice it will generate duplicated and dispensable
  triples. This is the most important limitation of using SPARQL for
  me. CWM will only execute a rule whenever necessary and the rules do
  not have to explicit declare any condition to avoid unnecessary
  creation of triples.&lt;/p&gt;
&lt;p&gt;It is still not clear to me if all SPARQL queries can be rewrited to
  prevent non necessary creation of triples. Moreover, I don&amp;#8217;t want to
  have too complicated SPARQL queries to maintain.&lt;/p&gt;
&lt;p&gt;More on the next post.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>We found a bug in the split command of Mac OS</title>
   <link href="http://arademaker.github.com/blog/2013/07/18/we-found-a-bug-in-macos-split.html"/>
   <updated>2013-07-18T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2013/07/18/we-found-a-bug-in-macos-split</id>
   <content type="html">&lt;p&gt;Yesterday my friend
&lt;a href=&quot;http://researcher.ibm.com/researcher/view.php?person=br-mnerys&quot;&gt;Marcelo Nery&lt;/a&gt;
and I found a bug in the &lt;code class=&quot;highlighter-rouge&quot;&gt;split&lt;/code&gt; command of Mac OS. At first, I was
surprised (we don’t expect to find bugs in core tools like grep, ls,
split, wc…, right?) and almost expected to find the same bug in the
Linux version of split. At least in the split of Ubuntu distribution,
that was not the case. The bug is presented only in the Mac OS
version.&lt;/p&gt;

&lt;h2 id=&quot;the-bug&quot;&gt;The bug&lt;/h2&gt;

&lt;p&gt;Consider the file &lt;code class=&quot;highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; created with the following Common Lisp code
(actually for the rest of the post you don’t need to understand the
code):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-common-lisp&quot; data-lang=&quot;common-lisp&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;with-open-file&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;zero.log&quot;&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:element-type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned-byte&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			      &lt;span class=&quot;ss&quot;&gt;:direction&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:output&lt;/span&gt;
			      &lt;span class=&quot;ss&quot;&gt;:if-exists&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:supersede&lt;/span&gt;
			      &lt;span class=&quot;ss&quot;&gt;:external-format&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:utf-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;across&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;AB_CDE~%F~%G~%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		     &lt;span class=&quot;nb&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;equal&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;#\_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;do&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;write-byte&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
			      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;write-byte&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;char-code&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This file content could be inspected with hexdump command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hexdump &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; zero.log
00000000  41 42 00 00 00 43 44 45  0a 46 0a 47 0a     |AB...CDE.F.G.|
0000000d&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That is, the file has three 0 bytes in the first line right after the
&lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; letter and before the &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; letter. Now we want to split this file
one line per file.&lt;/p&gt;

&lt;p&gt;The Linux version of split works as expected, it splits the file
keeping the zero bytes unchanged.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;split &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; zero.log
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;f &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;x??&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---Begin: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---End: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xaa
ABCDE
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xaa
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xab
F
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xab
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xac
G
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xac&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Moreover, the sum of bytes of the &lt;code class=&quot;highlighter-rouge&quot;&gt;x??&lt;/code&gt; files is equal the number of
bytes in the &lt;code class=&quot;highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; file, 13 bytes.&lt;/p&gt;

&lt;p&gt;Nevertheless, the Mac OS version of split produces an unexpected
output. The letter &lt;code class=&quot;highlighter-rouge&quot;&gt;F&lt;/code&gt; is merged with the begining of the first line
althouth it is in the second line of the &lt;code class=&quot;highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; file. Besides
that, the zero bytes causes the Mac OS split to ignore the rest of the
first line of &lt;code class=&quot;highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; causing a lost of data. The sum of the bytes
of the &lt;code class=&quot;highlighter-rouge&quot;&gt;x??&lt;/code&gt; files in Mac OS is only 6 bytes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;split &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; zero.log
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;f &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;x??&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---Begin: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---End: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xaa
ABF
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xaa
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xab
G
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xab&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;reporting-the-bug&quot;&gt;Reporting the bug&lt;/h2&gt;

&lt;p&gt;I reported the bug to Apple using the
&lt;a href=&quot;http://www.apple.com/feedback/macosx.html&quot;&gt;Mac OS Feedback form&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>The GeTFun Research Project</title>
   <link href="http://arademaker.github.com/blog/2013/01/17/GeTFun.html"/>
   <updated>2013-01-17T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2013/01/17/GeTFun</id>
   <content type="html">&lt;p&gt;January, 1 2013 marked the official beginning of the
&lt;a href=&quot;http://sqig.math.ist.utl.pt/GeTFun/&quot;&gt;GeTFun&lt;/a&gt; project. I am very
excited about this project and willing to contributed and share ideas
with all collaborators. The first workshop of the project will happen
during the &lt;a href=&quot;http://www.uni-log.org/&quot;&gt;4th UNILOG 2013&lt;/a&gt; in Rio de
Janeiro.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Pacote R dicionariosIBGE</title>
   <link href="http://arademaker.github.com/blog/2012/09/20/pacote-dicionariosIBGE.html"/>
   <updated>2012-09-20T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2012/09/20/pacote-dicionariosIBGE</id>
   <content type="html">&lt;p&gt;Disponibilizei hoje no github e submeti para o CRAN a versão 1.5 do
pacote &lt;a href=&quot;https://github.com/arademaker/dicionariosIBGE/&quot;&gt;dicionariosIBGE&lt;/a&gt;.
Este pacote contém os dicionários das principais pesquisas do
&lt;a href=&quot;http://www.ibge.gov.br/&quot;&gt;IBGE&lt;/a&gt;: PNAD (1983-2009), POF (1987-1996) e
PME. Também incluimos nesta versão variáveis adicionais com rótulos
para as variáveis categóricas de cada pesquisa.&lt;/p&gt;

&lt;p&gt;Nesta nova versão, incorporamos a função &lt;code class=&quot;highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; original do
pacote &lt;code class=&quot;highlighter-rouge&quot;&gt;IBGEPesq&lt;/code&gt; que foi desenvolvido no IBGE mas nunca submetido ao
CRAN. O pacote &lt;code class=&quot;highlighter-rouge&quot;&gt;IBGEPesq&lt;/code&gt; é distribuído pelo IBGE nos CDs e DVDs
vendidos na &lt;a href=&quot;http://loja.ibge.gov.br&quot;&gt;loja virtual do IBGE&lt;/a&gt; e
disponibilizo no site do IBGE. Vide link “Leitura em R” na
&lt;a href=&quot;http://www.ibge.gov.br/home/estatistica/populacao/trabalhoerendimento/pnad2009/microdados.shtm&quot;&gt;página da última PNAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Fizemos várias otimizações na função &lt;code class=&quot;highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; original e
incluímos ainda um argumento extra para leitura dos rótulos das
variáveis categóricas. A partir desta versão do dicionariosIBGE, o
pacote &lt;code class=&quot;highlighter-rouge&quot;&gt;IBGEPesq&lt;/code&gt; torna-se desnecessário para leitura dos microdados
das pesquisas do IBGE contempladas pelo dicionariosIBGE.&lt;/p&gt;

&lt;p&gt;Observo, no entanto, que a novidade de disponibilizarmos os rótulos
para as variáveis categóricas e adaptarmos a função &lt;code class=&quot;highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; para
usar os rótulos para construir factors ainda é
experimental. Pessoalmente, questiono sua utilidade em geral. Existem
variáveis categóricas cujos rótulos são realmente úteis. Por exemplo,
variáveis como a &lt;code class=&quot;highlighter-rouge&quot;&gt;UF&lt;/code&gt; da tabela de PESSOA da PNAD de 2009. Certamente
trabalharmos com um factor com rótulo “Rio de Janeiro” ao invés de
apenas o valor 33 torna o manuseio dos dados mais fácil. No entanto,
existem variáveis categóricas como a &lt;code class=&quot;highlighter-rouge&quot;&gt;V2927&lt;/code&gt; cujos rótulos são tão
grandes e verbosos que provavelmente não facilitam em nada o trabalho
com os dados. Vide:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; rotpes2009[rotpes2009$cod == &quot;V2927&quot;,]
       cod valor                                  rotulo
1070 V2927     1                     Custaria muito caro
1071 V2927     2                         Era muito longe
1072 V2927     3                     Por falta de provas
1073 V2927     4                         Demoraria muito
1074 V2927     5 Cabia as outras partes iniciarem a acao
1075 V2927     6    Por medo de outras partes envolvidas
1076 V2927     7     por meio de mediacao ou conciliacao
1077 V2927     8                 Nao acredita na justica
1078 V2927     9  Nao sabia que podia utilizar a Justica
1079 V2927    10                                  Outros
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;De qualquer modo, ao invés de usar os rótulos que distribuímos no
pacote, nada impede que os usuários do pacote de: (1) passarem para a
função &lt;code class=&quot;highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; seus próprios data.frames com rótulos, bastando
fornecer um data.frame com os rótulos no formato esperado (três
colunas: cod, valor, rotulo); ou (2) passar o valor &lt;code class=&quot;highlighter-rouge&quot;&gt;NULL&lt;/code&gt; para os
rótulos (default da função) retornando os dados como numéricos e/ou
string e não factors.&lt;/p&gt;

&lt;p&gt;O pacote foi desenvolvido e testado no MacOS mas deve funcionar sem
problemas no Linux e Windows.&lt;/p&gt;

&lt;p&gt;Finalmente, cabe destacar que desenvolvemos o pacote usando os
dicionários que encontramos nos CDs e DVDs disponíveis na FGV. Já
tivemos casos de diferentes CDs da mesma pesquisa, adquiridos em
diferentes momentos do IBGE, terem conteúdos diferentes (diferenças
nos dicionários e diferenças nos arquivos de dados). O IBGE parece
produzir os CDs e DVDs das pesquisas por demanda, o que pode explicar
as diferenças entre CDs da mesma pesquisa.&lt;/p&gt;

&lt;p&gt;A falta de padrão do IBGE na distribuição dos arquivos das pesquisas é
um grande problema para os pesquisadores e atrabalha bastante nossa
iniciativa de facilitar a leitura dos dados do IBGE em R. Para
contornar os problemas, poderíamos tentar distribuir os dados do IBGE
já em formato RData, para carga no R diretamente. No entanto, além das
possíveis questões legais (não está claro no site do IBGE qual a
licença de uso adotada pelo IBGE), certamente a credibilidade dos
dados é maior se os dados são obtidos diretamente do site ou mídia do
IBGE.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>O problema dos galões</title>
   <link href="http://arademaker.github.com/blog/2012/05/06/problema-dos-galoes.html"/>
   <updated>2012-05-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2012/05/06/problema-dos-galoes</id>
   <content type="html">&lt;p&gt;Na primeira prova do curso de &lt;a href=&quot;/CA-2012-1/&quot;&gt;estrutura de dados&lt;/a&gt; que
dei este ano, coloquei a seguinte questão para os alunos.&lt;/p&gt;

&lt;p&gt;Nós temos 3 galões de tamanho 10, 7 e 4 litros. Os galões de 7 e 4
litros começam cheios e o galão de 10 litros vazio. Só podemos
realizar um tipo de operação com os galões: derramar todo o conteúdo
de um galão em outro, parando apenas quando o galão sendo derramado
ficar vazio ou o destino ficar cheio. Queremos saber se existe uma
sequência de operações que termine com 2 litros no galão de 7 ou 4
litros. Pede-se:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Modele o problema como um problema de grafo, descrevendo
precisamente a definição do grafo envolvido e descrevendo a solução
em função do grafo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Qual algorítmo de grafo deverá ser usado?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Este é um problema clássico que pode ser encontrado em vários livros
de estrutura de dados e algorítmos (Ex 3.8 do Algorithms by Dasgupta,
Papadimitriou e Vazirani
&lt;a href=&quot;http://www.amazon.com/Algorithms-Sanjoy-Dasgupta/dp/0073523402&quot;&gt;Amazon&lt;/a&gt;,
&lt;a href=&quot;http://www.cs.berkeley.edu/~vazirani/algorithms.html&quot;&gt;site do livro&lt;/a&gt;,
&lt;a href=&quot;http://demonstrations.wolfram.com/WaterPouringProblem/&quot;&gt;Wolfram&lt;/a&gt;). Trata-se
de ver os estados do sistema (a quantidaade de líquido em cada galão)
como um nó de um grafo. As arestas representam as possíveis transições
de estado após transferência de líquido entre dois galões.&lt;/p&gt;

&lt;p&gt;Um formalismo bem interessante para implementar a solução deste
problema é lógica de reescrita. Em particular, resolvi usar
&lt;a href=&quot;http://maude.cs.uiuc.edu/&quot;&gt;Maude&lt;/a&gt;. Maude é uma implementação bastante
eficiente e conhecida de lógica de reescrita, o nome refere-se a
linguagem e ao sistema ao mesmo tempo.&lt;/p&gt;

&lt;p&gt;A implementação em Maude é muito simples:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mod GAL is
  inc INT .

  sorts Galon System .
  subsort Galon &amp;lt; System .

  op &amp;lt;_,_&amp;gt; : Int Int -&amp;gt; Galon .
  op __ : System System -&amp;gt; System [assoc comm id: null] . 
  op null : -&amp;gt; System .

  vars N1 N2 M1 M2 : Int .

  crl [transfer-1] :
     &amp;lt; N1 , M1 &amp;gt; &amp;lt; N2 , M2 &amp;gt; =&amp;gt; 
     &amp;lt; 0 , M1 &amp;gt; &amp;lt; (N1 + N2) , M2 &amp;gt; 
   if N1 &amp;lt; (M2 - N2) .

  crl [transfer-2] :
     &amp;lt; N1 , M1 &amp;gt; &amp;lt; N2 , M2 &amp;gt; =&amp;gt; 
     &amp;lt; (N1 - (M2 - N2)) , M1 &amp;gt; &amp;lt; M2 , M2 &amp;gt; 
   if N1 &amp;gt; (M2 - N2) .

  op initial : -&amp;gt; System .
  eq initial = &amp;lt; 0 , 10 &amp;gt; &amp;lt; 7 , 7 &amp;gt; &amp;lt; 4 , 4 &amp;gt; .
endm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Trata-se de uma especificação algébrica do estado do sistema como um
multiset de galões onde cada galão é uma dupla de números inteiros:
primeiro componente é a quantidade de líquido e segundo a
capacidade. A operação &lt;code class=&quot;highlighter-rouge&quot;&gt;initial&lt;/code&gt; serve apenas para criar uma constante
que representa o estado inicial do sistema.&lt;/p&gt;

&lt;p&gt;As duas regras de reescrita fazem deste módulo um módulo de sistema
ou, mais formalmente, uma etoria de reescrita. Estas regras
implementam as duas possibilidades de transferência de líquido entre
um galão e outro. Na primeira regra, todo o líquido é transferido de
um galão para outro esvaziando o galão origem. Na segunda, a
transferência é interrompida quando o galão destino torna-se
cheio. Isto poderia ainda ser simplificado para uso de apenas uma
regra, certo?!&lt;/p&gt;

&lt;p&gt;O interessante de implementar em Maude é que as regras de reescrita
podem ser entendidas, computacionalmente, como transições de estado de
um sistema. Logicamente, as regras podem ser entendidas como regras de
inferência de um sistema de reescrita.&lt;/p&gt;

&lt;p&gt;Na prática, podemos aplicar as reescritas de estado usando o comando
&lt;code class=&quot;highlighter-rouge&quot;&gt;rew&lt;/code&gt; e realizar uma busca por estados a partir de um estado inicial
usando o comando &lt;code class=&quot;highlighter-rouge&quot;&gt;search&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para testarmos a implementação, podemos aplicar algumas regras a
partir do estado initial. Por exemplo, para aplicarmos 4 regras de
reescrita a partir do estado inicial usamos:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Maude&amp;gt; rew [4] initial .
rewrite [4] in GAL : initial .
rewrites: 33 in 0ms cpu (0ms real) (131474 rewrites/second)
result System: &amp;lt; 0,4 &amp;gt; &amp;lt; 4,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note-se que não controlamos que regras são aplicadas (estratégia de
aplicação). Para encontrarmos a solução, usamos:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;search initial =&amp;gt;* &amp;lt; 2 , X:Int &amp;gt; S:System such that X:Int &amp;lt; 10 .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Este comando acima efetua uma busca (BFS, busca em largura), a partir
do estado inicial, por algum estado atingido com zero ou mais
transições (aplicações de alguma das duas regras que definimos), por
um estado onde exista algum galão com dois litros e cuja capacidade
seja menor que 10.&lt;/p&gt;

&lt;p&gt;A resposta do sistema são 4 possíveis soluções:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Solution 1 (state 16)
states: 17  rewrites: 501 in 3ms cpu (3ms real) (139244 rewrites/second)
S:System --&amp;gt; &amp;lt; 2,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
X:Int --&amp;gt; 4

Solution 2 (state 18)
states: 19  rewrites: 559 in 4ms cpu (4ms real) (127334 rewrites/second)
S:System --&amp;gt; &amp;lt; 0,7 &amp;gt; &amp;lt; 9,10 &amp;gt;
X:Int --&amp;gt; 4

Solution 3 (state 19)
states: 20  rewrites: 610 in 5ms cpu (5ms real) (119210 rewrites/second)
S:System --&amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 5,10 &amp;gt;
X:Int --&amp;gt; 7

Solution 4 (state 20)
states: 21  rewrites: 627 in 5ms cpu (5ms real) (112164 rewrites/second)
S:System --&amp;gt; &amp;lt; 0,4 &amp;gt; &amp;lt; 9,10 &amp;gt;
X:Int --&amp;gt; 7

No more solutions.
states: 21  rewrites: 721 in 6ms cpu (6ms real) (109824 rewrites/second)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para examinarmos a primeira solução, pedimos para o sistema mostrar o
caminho, isto é, as reescritas executadas.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Maude&amp;gt; show path 16 .
state 0, System: &amp;lt; 0,10 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 7,7 &amp;gt;
===[ crl ... [label transfer-1] . ]===&amp;gt;
state 1, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 4,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 4, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 1,7 &amp;gt; &amp;lt; 10,10 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 8, System: &amp;lt; 1,7 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 6,10 &amp;gt;
===[ crl ... [label transfer-1] . ]===&amp;gt;
state 12, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 5,7 &amp;gt; &amp;lt; 6,10 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 14, System: &amp;lt; 2,10 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 5,7 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 16, System: &amp;lt; 2,4 &amp;gt; &amp;lt; 2,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;O estado encontrado é onde o galão de 4 litros está com 2 litros, o
galão de 7 litros está completo e o galão de 10 litros está com 2
litros.&lt;/p&gt;

&lt;p&gt;Bem legal, não acham!?&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Lattes to BibTeX</title>
   <link href="http://arademaker.github.com/blog/2012/02/15/lattes-to-bibtex.html"/>
   <updated>2012-02-15T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2012/02/15/lattes-to-bibtex</id>
   <content type="html">&lt;p&gt;Disponibilizei hoje online no github uma transformação do &lt;a href=&quot;http://lattes.cnpq.br/&quot;&gt;Lattes&lt;/a&gt; para
&lt;a href=&quot;http://en.wikipedia.org/wiki/BibTeX&quot;&gt;BibTeX&lt;/a&gt;, vejam o repositório:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/arademaker/SLattes&quot;&gt;http://github.com/arademaker/SLattes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Eu acabei fazendo esta transformação por dois motivos. O primeiro para
meu uso pessoal, eu já estava querendo faz muito tempo conseguir gerar
um BibTex com minhas produções. O segundo foi como parte do projeto
Semantic Lattes. A idéia é que uma transformação dos dados do Lattes
para algum padrão de referências como o
&lt;a href=&quot;http://www.loc.gov/standards/mods/&quot;&gt;XML/MODS da Library of Congress&lt;/a&gt;,
ajuda a validar os dados do Lattes.&lt;/p&gt;

&lt;p&gt;Intruções de como usar o transformador estão no README do repositório,
mas como o texto lá está em inglês, segue a idéia geral. O que fiz foi
uma transformação XSLT do XML/Lattes para o XML/MODS. Este último pode
ser então facilmente convertido para BibTex usando o xml2bib, programa
do pacote &lt;a href=&quot;http://sourceforge.net/p/bibutils/&quot;&gt;Bibutils&lt;/a&gt; disponivel no
Linux e no MacOS (MacPorts). Para executar a transformação e validar
um XML em relação ao seu DTD/Schema, ainda são necessários os
programas xsltproc e xmllint.&lt;/p&gt;

&lt;h2 id=&quot;instalação-dos-programas-necessários&quot;&gt;Instalação dos programas necessários&lt;/h2&gt;

&lt;p&gt;Para quem usa o MacPorts no MacOS:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo port -v install bibtool bibutils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;O xmllint e xsltproc já estão instalados no MacOS (acho que no XCode).&lt;/p&gt;

&lt;p&gt;Para quem usa Linux/Ubuntu:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install bibtool bibutils xsltproc libxml2-utils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para quem usa Windows, consultar o site destas ferramentas, não
trivial! Esquece o Windows e instala o Ubuntu! ;-)&lt;/p&gt;

&lt;h2 id=&quot;usando-a-transformação&quot;&gt;Usando a transformação&lt;/h2&gt;

&lt;p&gt;Com as ferramentas instaladas, o primeiro passo é acesssar o sistema
&lt;a href=&quot;http://lattes.cnpq.br&quot;&gt;Lattes do CNPq&lt;/a&gt;, link &lt;strong&gt;atualizar&lt;/strong&gt;, logar-se
no sistema e escolher a opção de exportar para XML. Será iniciado o
download de um arquivo ZIP. Abra ZIP e extraia o arquivo XML dentro
dele que tem mesmo nome, seu lattes ID. Imagine então que vc renomeou
este arquivo XML para &lt;code class=&quot;highlighter-rouge&quot;&gt;LATTES.xml&lt;/code&gt; e o moveu para o mesmo diretório
onde está o arquivo com &lt;code class=&quot;highlighter-rouge&quot;&gt;lattes2mods.xsl&lt;/code&gt; que você pegou do
repositório no github.&lt;/p&gt;

&lt;p&gt;Agora basta rodar:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xsltproc lattes2mods.xsl LATTES.xml &amp;gt; LATTES.mods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E em seguida:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xml2bib -b -w LATTES.mods &amp;gt; LATTES.bib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mas o interessante é antes de rodar o xml2bib, validar o arquivo mods
gerado contra o
&lt;a href=&quot;http://www.loc.gov/standards/mods/mods-schemas.html&quot;&gt;XML Schema do MODS&lt;/a&gt;,
disponibilizado no site da Biblioteca do Congresso Americano, baixe a
versão 3.4:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xmllint --schema mods.xsd LATTTES.mods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Esta validação serve para verificar não apenas erros na estrutura do
arquivo, que seriam bugs no meu código, mas também erros nos dados, em
função de informações erradas (faltantes, em lugar errado etc) no
Lattes.&lt;/p&gt;

&lt;p&gt;Comentários são sempre bem vindos. Problemas podem ser reportados
diretamente no &lt;a href=&quot;https://github.com/arademaker/SLattes/issues&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>R Package SSOAP</title>
   <link href="http://arademaker.github.com/blog/2012/01/02/package-SSOAP.html"/>
   <updated>2012-01-02T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2012/01/02/package-SSOAP</id>
   <content type="html">&lt;p&gt;Last year, during a summer course that I gave at FGV, I taught the
students how to use SOAP protocol do retrive data from
&lt;a href=&quot;http://bcb.gov.br/&quot;&gt;Banco Central do Brasil&lt;/a&gt; using R. BCB has a
system called SGS (Sistema Gerenciador de Séries Temporais) that has a
SOAP interface.&lt;/p&gt;

&lt;p&gt;At that time, the &lt;a href=&quot;http://www.omegahat.org/SSOAP/&quot;&gt;package SSOAP&lt;/a&gt; had
a small bug that I contributed to fix. Today I found that my
contribution was incorporated in
&lt;a href=&quot;http://www.omegahat.org/SSOAP/Changes.html&quot;&gt;version 0.5-5&lt;/a&gt; of this
package whish makes my
&lt;a href=&quot;https://github.com/arademaker/SSOAP&quot;&gt;repository at github&lt;/a&gt; outdated.&lt;/p&gt;

&lt;p&gt;It took my a couple of minutes to test the new version of this
package. Since I am running the last version of R, 2.14, the general
procedure for install packages didn’t work.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;packages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;SSOAP&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Warning&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getDependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;package&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSOAP&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;’&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;available&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;version&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2.14.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I finnaly figured out how to install the last version from source
using the Omegahat repository version with the command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; install.packages(&quot;SSOAP&quot;, repos = &quot;http://www.omegahat.org/R&quot;, 
                   dependencies = TRUE, 
                   type = &quot;source&quot;)
trying URL 'http://www.omegahat.org/R/src/contrib/SSOAP_0.8-1.tar.gz'
Content type 'application/x-gzip' length 195424 bytes (190 Kb)
opened URL
==================================================
downloaded 190 Kb

* installing *source* package ‘SSOAP’ ...
** R
** inst
** preparing package for lazy loading
Creating a new generic function for ‘help’ in package ‘SSOAP’
Warning in .NonstandardGenericTest(body(fdef), name, stdGenericBody) :
  the supplied generic function definition for toSOAP does not
  seem to call 'standardGeneric'; no methods will be dispatched!
** help
*** installing help indices
** building package indices ...
** testing if installed package can be loaded

* DONE (SSOAP)

The downloaded packages are in
	‘/private/var/.../downloaded_packages’
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After that, I was prepared to actually test the package running the
code that I created during the
&lt;a href=&quot;https://github.com/arademaker/IR-2011/&quot;&gt;course&lt;/a&gt; (lesson 7 directory
aula-07). But some changes in RCurl package requires a change in how
we ask for not verify the ssl certificate. That is, I had to replace
the &lt;code class=&quot;highlighter-rouge&quot;&gt;ssl.verifypeer = FALSE&lt;/code&gt; argument by a list of options in the call
of the function &lt;code class=&quot;highlighter-rouge&quot;&gt;ff@functions$getValoresSeriesXML&lt;/code&gt;. The last version
of this script is now available as a gist here:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/1550651.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Note that the &lt;code class=&quot;highlighter-rouge&quot;&gt;ssl.verifypeer&lt;/code&gt; argument is necessary because the
certificate used in BCB website is invalid! What a shame!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2012-01-02-bcb-certificate.png&quot; alt=&quot;BCB certificate&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>GitHub Pages and Jekyll plugins</title>
   <link href="http://arademaker.github.com/blog/2011/12/01/github-pages-jekyll-plugins.html"/>
   <updated>2011-12-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/12/01/github-pages-jekyll-plugins</id>
   <content type="html">&lt;p&gt;Everyone that use &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; and wants to host the site at GitHub should know that GitHub Pages does not allow custom plugins, right? Using Jekyll for a little more than a blog site, like &lt;a href=&quot;http://emap.fgv.br&quot;&gt;EMAp/FGV&lt;/a&gt; will require plugins. In my case, avoid the use of custom plugins is not an option.&lt;/p&gt;

&lt;p&gt;The solution is trivial, one has to run Jekyll locally and post the produzed files into a master branch of a git repo, following the conventions described at &lt;a href=&quot;http://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; por person and organization pages. The problem that I faced was to choose the best way to organize and keep tracking of the source and produzed files.&lt;/p&gt;

&lt;p&gt;I read a couple of posts with possible solutions. The solution by &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;Charlie Park&lt;/a&gt; force us to have two distinct git repositories. I don’t like this approach because of that. Our website will be maintained by more than one person, having two distinct repositories since that I will not using all git features. The solution by &lt;a href=&quot;http://tech.hugr.fr/blog/2011/08/07/how-to-host-a-jekyll-app-on-github-pages-with-plugins/&quot;&gt;Jean Denis&lt;/a&gt; is a little bit better but keeps me thinking about why he need to keep the produced files under version control in the two branches, gh-pages and the master.&lt;/p&gt;

&lt;p&gt;My final solution is to keep the directory produced by jekyll out of git control. This simple thing allows me to switch the branches, from source to master, and still have access to the produced files, directory &lt;code class=&quot;highlighter-rouge&quot;&gt;_site&lt;/code&gt;. Once in the master branch, I only have to move the files under &lt;code class=&quot;highlighter-rouge&quot;&gt;_site/&lt;/code&gt; to the root directory in the master branch and update the master branch before push it to GitHub.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git checkout source
// do whatever you need
git status / git add / git commit
jekyll
checkout master
cp -r _site/* . &amp;amp;&amp;amp; rm -rf _site/ &amp;amp;&amp;amp; touch .nojekyll
git status &amp;gt; git add &amp;gt; git commit
git push -all origin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Comentários sobre ECLM 2011 e ISWC 2011 (1/2)</title>
   <link href="http://arademaker.github.com/blog/2011/11/27/ultimas-conferencias.html"/>
   <updated>2011-11-27T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/11/27/ultimas-conferencias</id>
   <content type="html">&lt;p&gt;Desde que voltei da minha última viagem para participar da
&lt;a href=&quot;http://weitz.de/eclm2011/&quot;&gt;ECLM 2011&lt;/a&gt; e
&lt;a href=&quot;http://iswc2011.semanticweb.org/&quot;&gt;ISWC 2011&lt;/a&gt;, estou pensando em
escrever sobre o assunto. Por estar escrevendo em português, acho que
tenho lá ainda alguma chance de contribuir com algo novo. Neste post,
vou falar da ISWC 2011. Vou começar listando alguns blogs que já
escreveram sobre estas conferências em inglês.&lt;/p&gt;

&lt;p&gt;Das duas conferências, certamente a ISWC é a maior e, por isso, também
foi a mais comentada. São vários os posts de pessoas que escreveram
sobre ela. Ivan Herman escreveu
&lt;a href=&quot;http://ivan-herman.name/2011/11/02/some-notes-on-iswc2011.../&quot;&gt;Some notes on ISWC2011&lt;/a&gt;.
Também sobre a ISWC-2011 vale a pena ler o post
&lt;a href=&quot;http://blog.phenoscape.org/2011/11/03/notes-from-iswc-2011/&quot;&gt;Notes from ISWC 2011&lt;/a&gt;
e a série de 5 posts de
&lt;a href=&quot;http://semanticweb.com/report-from-day-1-at-iswc_b24150&quot;&gt;Juan Sequeda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Como Ivan escreveu no post dele, diferentes pessoas certamente
relatarão diferentes experiências da ISWC 2011. Me chamou atenção os
comentários dele sobre os trabalhos apresentados relacionados à
visualização de ontologias. Embora o assunto me interesse, acho que
perdi os workshops ou seções relacionadas a este tema. Foi bom ter
lido o post dele.&lt;/p&gt;

&lt;p&gt;Da minha parte, gostei bastante da ISWC 2011. A conferência foi bem
organizada, o hotel muito bom e a cidade muito agradável. Dos
workshops, o mais interessante para mim certamente foi o
&lt;a href=&quot;http://www.om2011.ontologymatching.org/&quot;&gt;Ontology Matching&lt;/a&gt;, afinal,
é o assunto que mais me interessa e sobre o qual tenho artigos
publicados com Isabel Cafezeiro e Hermann. Assistir ao workshop me
motivou a voltar a este assunto e tentar implementar efetivamente as
idéias que formalizamos nos artigos.&lt;/p&gt;

&lt;p&gt;Das seções da conferência, gostei bastante da “Ontology Matching,
Mapping” e da “KR - Semantics”. No mais, vale dizer que os posters
também estavam ótimos e a idéia de cada poster ser apresentado em 1
minuto foi bem divertida embora apenas alguns apresentadores tenham
entendido o espírito da coisa! Das apresentações dos convidados, o que
mais fez sucesso foi Frank van Harmelen com o título
&lt;a href=&quot;http://www.cs.vu.nl/~frankh/spool/ISWC2011Keynote/&quot;&gt;10 Years of Semantic Web: does it work in theory?&lt;/a&gt;.
Para mim, sendo minha área de pesquisa exatamente lógicas e, em
particular, nos últimos anos, lógicas de descritivas, ter uma
apresentação sugerindo que a comunidade de web semântica deve voltar a
atenção para os fundamentos teóricos da área, é bastante motivador.
Infelizmente assisti apenas parte da apresentação. O painel “Semantic
Web Death Match” foi meio sem graça, embora na sala, assistindo as
discussões, tenho que dizer que foi uma experiência única participar
do painel pelo twitter. No twitter as discussões foram até mais
interessantes. Finalmente, da seção “MANCHustifications and
Provenance”, tive a idéia de revisar o texto da minha tese que será
publicado pela Springer. A idéia é que pode-se obter diretamente de
uma prova formal, usando os sistemas dedutivos que apresento para
algumas DL em minha tese, a tal “justification of an entailment”. O
termo refere-se ao conjunto mínimo de axiomas usados para justificar
uma conclusão lógica (qual seria a melhor tradução para
“entailment”?). Obrigado
&lt;a href=&quot;http://manchester.academia.edu/SamanthaBail&quot;&gt;Samantha Bail&lt;/a&gt; por me
ajudar a confirmar a impressão que tive durante a apresentação dos
trabalhos na seção.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Gráficos de séries temporais no R</title>
   <link href="http://arademaker.github.com/blog/2011/10/31/time-series-R.html"/>
   <updated>2011-10-31T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/10/31/time-series-R</id>
   <content type="html">&lt;p&gt;Pergunta de dois alunos: como postar duas séries temporáis usando o
ggplot? Resolvi responder usando este post. Vou aproveitar então para
mostrar como postar séries temporáis usando o ggplot, lattice e o plot
padrão do R.&lt;/p&gt;

&lt;h2 id=&quot;criando-os-dados&quot;&gt;Criando os dados&lt;/h2&gt;

&lt;p&gt;Para começar, criei um objeto série temporal conforme exemplo da função &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt;
do R. Este objeto na realidade é uma matriz, são três séries temporais
ou uma série temporal multivariável.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; dados &amp;lt;- ts(matrix(rnorm(300), 100, 3), start=c(1961, 1), 
              frequency=12)
&amp;gt; dados[1:4,]
       Series 1   Series 2   Series 3
[1,]  1.4165848  2.1049293  1.0155993
[2,] -0.4264193 -0.2730903  0.8754992
[3,]  0.5120809 -0.4023986  1.9757084
[4,]  0.1375277 -0.5043973 -0.7795633
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;plot-básico&quot;&gt;Plot básico&lt;/h2&gt;

&lt;p&gt;Se usarmos o comando padrão de plot do pacote basic do R, temos o
seguinte plot.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; plot(ts)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-basic.png&quot; alt=&quot;plot-basic&quot; title=&quot;plot basic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviamente, neste e nos próximos exemplos, estou mostrando o uso mais
básico das funções, sem me preocupar com nenhum ajuste de formatação,
legenda, rótulos, cores etc.&lt;/p&gt;

&lt;h2 id=&quot;no-pacote-lattice&quot;&gt;No pacote lattice&lt;/h2&gt;

&lt;p&gt;No pacote Lattice, temos a função xyplot que contém vários
parâmetros para geração do gráfico. Explorei apenas a forma de gerar
uma série por painel e todas as séries em um painel.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; xyplot(dados)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-lattice-1.png&quot; alt=&quot;plot-lattice-1&quot; title=&quot;plot lattice&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; xyplot(dados, superpose = TRUE) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-lattice-2.png&quot; alt=&quot;plot-lattice-2&quot; title=&quot;plot lattice&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A página de help da função &lt;code class=&quot;highlighter-rouge&quot;&gt;xyplot&lt;/code&gt; contém muito mais informação sobre
os demais parâmetros da função. A função &lt;code class=&quot;highlighter-rouge&quot;&gt;xyplot&lt;/code&gt; também tem um método
específico para lidar com objetos da classe Zoo do pacote
&lt;a href=&quot;http://cran.r-project.org/web/packages/zoo/index.html&quot;&gt;zoo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;no-pacote-ggplot&quot;&gt;No pacote ggplot&lt;/h2&gt;

&lt;p&gt;Finalmente, como fazer no ggplot? Este foi o mais difícil. Demorei
bastante a encontrar referências via google. As mais relevantes que
encontrei foram:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://had.co.nz/ggplot2/scale_date.html&quot;&gt;scale_date&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://learnr.wordpress.com/2009/05/05/ggplot2-two-time-series-with-different-dates/&quot;&gt;ggplot2: two time series with different dates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://goo.gl/Kr5wP&quot;&gt;Using ggplot, how to have the x-axis of time series plots set up automatically?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O último link é a dica de como transformar um objeto &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt; em um
data.frame:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;datas &amp;lt;- seq(as.Date(paste(c(start(dados),1), collapse=&quot;/&quot;)), 
             by = &quot;month&quot;, length.out = length(dados))
dados.df &amp;lt;- data.frame(date = datas, value = dados)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E finalmente consegui plotar uma série das três com:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; ggplot(data=dados.df) + geom_line(aes(date, value.Series.1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-ggplot-single.png&quot; alt=&quot;plot-ggplot&quot; title=&quot;plot ggplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para conseguir em um único gráfico as três séries temporais, tive mais
trabalho. Primeiro em transformar os dados em um data.frame que
pudesse ser entendido pelo ggplot.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; tmp &amp;lt;- stack(dados.df, select = -1)
&amp;gt; tmp$date &amp;lt;- dados.df[,1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E finalmente, o novo plot:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; ggplot(data=dados.df) + geom_line(aes(date, value.Series.1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-ggplot-multi.png&quot; alt=&quot;plot-ggplot-multi&quot; title=&quot;plot ggplot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Não existe pacote melhor ou pior, cada um é mais adequado para cada
situação. No particular problema de plotar multiplas séries temporais,
sem nenhuma dúvida, preferi a facilidade do lattice em lidar com
objetos &lt;code class=&quot;highlighter-rouge&quot;&gt;ts&lt;/code&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Verifying the ISSN's check digit in Common Lisp</title>
   <link href="http://arademaker.github.com/blog/2011/09/13/checking-issn-lisp.html"/>
   <updated>2011-09-13T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2011/09/13/checking-issn-lisp</id>
   <content type="html">&lt;p&gt;The code below is my first approach to create a lisp function that
test the ISSN &lt;a href=&quot;http://en.wikipedia.org/wiki/Check_digit&quot;&gt;check
digit&lt;/a&gt;. Unfortunately, the
code runs only in Allegro CL due the requirement of regexp2
library. Nevertheless, the regexp2 library is easly replaced by an opensource
regexp library, which makes this not a real constraint.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/1215526.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;That is it! Comments are welcome!&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Construindo tabelas verdade no R</title>
   <link href="http://arademaker.github.com/blog/2011/03/02/tabela-verdade-no-R.html"/>
   <updated>2011-03-02T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2011/03/02/tabela-verdade-no-R</id>
   <content type="html">&lt;p&gt;Durante a preparação de alguns exercícios de lógica, me deparei com a
necessidade de construir tabelas verdade. Lembrando do pacote xtable
do R, pensei como seria construir uma tabela verdade usando o R. Minha
solução em R está no github, gist abaixo, com o exemplo de como seria
a tabela da expressão&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\neg (a \lor b) \lor c&lt;/script&gt;

&lt;script src=&quot;https://gist.github.com/852194.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Observem que a saída do primeiro comando xtable é bastante bizarra,
certamente um bug do pacote xtable. Editei a saída mantendo apenas o
início da tabela gerada e incluíndo “…” no final.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;xtable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;c&quot;&gt;% latex table generated in R 2.11.1 by xtable 1.5-6 package&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Wed Mar  2 23:18:40 2011&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{table}&lt;/span&gt;[ht]
&lt;span class=&quot;nt&quot;&gt;\begin{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{tabular}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;rllll&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; a &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; b &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; ! (a &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; b) &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; c &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
1 &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
2 &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c(FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
...
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{tabular}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{table}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Para fazer com que cada célula da tabela tivesse apenas o valor lógico
correspondente, não um vetor, converti o data.frame em caracteres
antes de usar o xtable. Minha solução original convertia em números,
Bruno Lopes me lembrou de converter em caracteres diretamente.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{table}&lt;/span&gt;[ht]
&lt;span class=&quot;nt&quot;&gt;\begin{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{tabular}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;llll&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
a &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; b &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\~&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (a OR b) OR c &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
   &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{tabular}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{table}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</content>
 </entry>
 
 <entry>
   <title>Barras e Linhas no R</title>
   <link href="http://arademaker.github.com/blog/2010/11/18/barras-e-linhas.html"/>
   <updated>2010-11-18T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2010/11/18/barras-e-linhas</id>
   <content type="html">&lt;p&gt;Ontem um aluno me perguntou como produzir um gráfico de barras com
linhas. Minha primeira idéia foi recorrer a pacotes como
&lt;a href=&quot;http://cran.r-project.org/web/packages/lattice/&quot;&gt;Lattice&lt;/a&gt; ou
&lt;a href=&quot;http://had.co.nz/ggplot2/&quot;&gt;ggplot2&lt;/a&gt;, imaginando tratar-se de um
gráfico pouco usual. Depois de um pouco de pesquisa, acabei
descobrindo que o gráfico em questão não é tão usual assim e pode ser
facilmente produzido com os comandos básicos de gráficos do R.&lt;/p&gt;

&lt;p&gt;Digamos que seus dados sejam uma data.frame composto por duas
variáveis.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;O que desejamos é representar no gráfico os valores da variável &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;
como barras e os valores da variável &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt; como pontos conectados por
linhas. O comando abaixo produz o gráfico de barras e retorna um
vetor com as coordenadas &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; dos meios das barras produzidas.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Agora é fácil criar os pontos e linhas:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Image que seus dados são temporais, onde cada observação está
relacionada a uma ano. Pode-se incluir os anos como rótulos do eixo
“x” com o comando:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2009&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/2010-11-18-fig.png&quot; alt=&quot;bar-and-line&quot; title=&quot;bar and line&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Datasets no Brasil</title>
   <link href="http://arademaker.github.com/blog/2010/01/30/datasets.html"/>
   <updated>2010-01-30T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2010/01/30/datasets</id>
   <content type="html">&lt;p&gt;Estão começando a surgir no Brasil iniciativas para real
transparência no acesso aos dados do governo. A
estruturação dos dados em formatos abertos (RDF, CSV
etc), acompanhados de metadados e indexados em interfaces de busca e
navegação que facilitem o download dos arquivos
são para mim as condições necessárias para
o livre acesso à informação. Uma destas
iniciativas é o &lt;a href=&quot;http://www.lexml.gov.br/&quot;&gt;LeXML&lt;/a&gt;.  Nos EUA
existem os projetos &lt;a href=&quot;http://data.gov/&quot;&gt;data.gov&lt;/a&gt; e
&lt;a href=&quot;http://datasf.org/&quot;&gt;datasf.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Na linha oposta, estão serviços como o da ANP de
levantamento de &lt;a href=&quot;http://www.anp.gov.br/preco/&quot;&gt;preços de
combustíveis&lt;/a&gt;. Que tipo de
pesquisa pode ser feita com estes dados? Para começar,
só a construção de um datasets a partir deste
site demanda um bom trabalho de desenvolvimento de um crawler e
transformadores. Afinal, eu me pergunto, qual é o objetivo
deste site da ANP? Se for para um cidadão comum pesquisar qual
o melhor posto para abastecer seu carro, a interface deixa a
desejar. Se for para a sociedade acompanhar os preços de
combustíveis no Brasil, isto implica acessibilidade as
séries de dados, e todos meus comentários anteriores
fazem sentido, não?&lt;/p&gt;

</content>
 </entry>
 
 
</feed>
