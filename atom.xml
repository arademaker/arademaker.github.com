<?xml version="1.0" encoding="utf-8" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Alexandre Rademaker</title>
 <link href="http://arademaker.github.com/atom.xml" rel="self"/>
 <link href="http://arademaker.github.com/"/>
 <updated>2015-12-04T12:19:28-02:00</updated>
 <id>http://arademaker.github.com/</id>
 <author>
   <name>Alexandre Rademaker</name>
   <email>arademaker@gmail.com</email>
 </author>

 
 <entry>
   <title>Merging RDF files</title>
   <link href="http://arademaker.github.com/blog/2015/08/18/combine-rdf.html"/>
   <updated>2015-08-18T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2015/08/18/combine-rdf</id>
   <content type="html">&lt;p&gt;
How to merge multiple RDF files into a single RDF file? The first idea
would be to convert each RDF file in &lt;a href=&quot;http://www.w3.org/2001/sw/RDFCore/ntriples/&quot;&gt;ntriples&lt;/a&gt; and just concatenate
them using unix &lt;code&gt;cat&lt;/code&gt; utility, right? No, it doesn&#39;t work with blank
nodes (or BNodes)! BNodes from different files with the same ID would
be merged as a single resource and this is not the expected semantics,
BNodes from different files are different resources, even if they have
the same id.
&lt;/p&gt;

&lt;p&gt;
The &lt;code&gt;rapper&lt;/code&gt; is a utility from the package &lt;a href=&quot;http://librdf.org&quot;&gt;Redland&lt;/a&gt;. Below I am
presented the files and the number of triples on each one.
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
$ rapper -c -i ntriples wordnet-en-fixed.nt
rapper: Parsing returned 3517504 triples

$ rapper -c -i ntriples own-pt-fixed.nt
rapper: Parsing returned  824916 triples
&lt;/pre&gt;

&lt;p&gt;
The oldest tool to support merging of RDF files is &lt;a href=&quot;http://www.w3.org/2000/10/swap/doc/cwm.html&quot;&gt;CWM&lt;/a&gt;. CWM is written
in python and its performance is really bad. The command below hasn&#39;t
finish after 5 minutes. 
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
/usr/local/cwm-1.2.1/cwm --ntriples own-pt-fixed.nt wordnet-en-fixed.nt &amp;gt; tudo-cwm.nt
&lt;/pre&gt;

&lt;p&gt;
Next tool that I tried was &lt;a href=&quot;http://rdfpro.fbk.eu&quot;&gt;\(RDF_{pro}\)&lt;/a&gt;. The performance was excellent,
only 11 seconds! But we must add a parameter &lt;code&gt;-w&lt;/code&gt; to force BNodes in
input files to be renamed to avoid possible clashes. Actually, it
doesn&#39;t make sense to me why this is not the default behaviour.
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
$ rdfpro @r -w own-pt-fixed.nt wordnet-en-fixed.nt @w tudo-pro.nt
14:45:53(I) 4342420 triples read (377077 tr/s avg)
14:45:53(I) 4342420 triples written (377077 tr/s avg)
14:45:53(I) Done in 11 s
&lt;/pre&gt;

&lt;p&gt;
Next tool, &lt;code&gt;riot&lt;/code&gt; from the &lt;a href=&quot;https://jena.apache.org&quot;&gt;Jena&lt;/a&gt; library. The performance was not bad,
it took twice the time of \(RDF_{pro}\) but it finished. The only
problem is that it complained about some IRI that no other tool
complained.
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
$ time riot own-pt-fixed.nt wordnet-en-fixed.nt &amp;gt; tudo-riot.nt
14:51:14 WARN riot :: [line: 282756, col: 1 ] Bad IRI: &amp;lt;https://w3id.org/own-pt/wn30-pt/instances/word-Ĳsselmeer&amp;gt; Code: 47/NOT_NFKC in PATH: The IRI is not in Unicode Normal Form KC.
14:51:14 WARN riot :: [line: 282756, col: 1 ] Bad IRI: &amp;lt;https://w3id.org/own-pt/wn30-pt/instances/word-Ĳsselmeer&amp;gt; Code: 56/COMPATIBILITY_CHARACTER in PATH: Bad character
...

real	0m27.398s
user	0m29.905s
sys	0m1.751s
&lt;/pre&gt;

&lt;p&gt;
I don&#39;t like warnnings so I tried the safe path. I converted the
ntriple file with these strange IRIs to RDF/XML and called &lt;code&gt;riot&lt;/code&gt;
again. No warnnings this time, good!
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
$ rapper -i ntriples -o rdfxml own-pt-fixed.nt  &amp;gt; own-pt-fixed.rdf
rapper: Serializing with serializer rdfxml
rapper: Parsing returned 824916 triples

$ riot --time own-pt-fixed.rdf wordnet-en-fixed.nt &amp;gt; tudo-riot.nt
own-pt-fixed.rdf : 14.84 sec  824,916 triples  55,602.32 TPS
wordnet-en-fixed.nt : 21.82 sec  3,517,504 triples  161,175.95 TPS
Total : 36.66 sec  4,342,420 triples  118,451.17 TPS
&lt;/pre&gt;

&lt;p&gt;
But the output produced does have some errors! The IRIs are not
encoded as the way the ntriples specification requires.
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
$ rapper -c -i ntriples tudo-riot.nt

rapper: Parsing URI tudo-riot.nt with parser ntriples
rapper: Error - URI tudo-riot.nt:117668 column 55 - Non-printable ASCII character 195 (0xC3) found.
rapper: Error - URI tudo-riot.nt:117668 column 56 - Non-printable ASCII character 162 (0xA2) found.
&lt;/pre&gt;

&lt;p&gt;
By the way, for the future, I will use \(RDF_pro\).
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>VIVO Apps and Tools Webinar</title>
   <link href="http://arademaker.github.com/blog/2014/04/30/webinar.html"/>
   <updated>2014-04-30T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2014/04/30/webinar</id>
   <content type="html">&lt;div id=&quot;outline-container-sec-1&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-1&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;1&lt;/span&gt; Introduction&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-1&quot;&gt;
&lt;p&gt;
Yerterday I presented for the Apps and Tools working group my workflow
to prepared data to be inserted into FGV VIVO instance. Since some
people asked be to share the links and the file that I used to guide
my presentation, I made this post. 
&lt;/p&gt;

&lt;p&gt;
This page is generated from a &lt;a href=&quot;http://orgmode.org&quot;&gt;org file&lt;/a&gt; that I export to HTML and
further processed with &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. The process of use org files with
jekyll is outlined &lt;a href=&quot;http://orgmode.org/worg/org-tutorials/org-jekyll.html&quot;&gt;here&lt;/a&gt;. I am stil not very confortable with this
workflow but I am using it for my personal website and for the
websites of the courses that I teach at FGV.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-2&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-2&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;2&lt;/span&gt; The Toolset&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-2&quot;&gt;
&lt;p&gt;
This is the non comprehensive list of tools that I use. I am listing
here the main tools that come up into my mind that should be
interesting to others.
&lt;/p&gt;

&lt;ul class=&quot;org-ul&quot;&gt;
&lt;li&gt;&lt;a href=&quot;http://www.gnu.org/software/emacs/&quot;&gt;Emacs&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://orgmode.org&quot;&gt;Org-Mode&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://common-lisp.net/project/slime/&quot;&gt;Slime&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Common Lisp compilers and interpreters: &lt;a href=&quot;http://franz.com/products/allegro-common-lisp/&quot;&gt;Allegro CL&lt;/a&gt;, &lt;a href=&quot;http://www.sbcl.org&quot;&gt;SBCL&lt;/a&gt; and &lt;a href=&quot;http://abcl.org&quot;&gt;ABCL&lt;/a&gt;
  (Lisp on JVM, so I can use Java RDF libraries).
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.w3.org/2000/10/swap/doc/cwm.html&quot;&gt;CWM&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://franz.com/agraph/allegrograph/&quot;&gt;Allegro Graph Triplestore&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://franz.com/agraph/gruff/&quot;&gt;Gruff&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Git
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.r-project.org&quot;&gt;R&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;Python
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://xmlsoft.org&quot;&gt;xsltproc and xmllint&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://tidy.sourceforge.net&quot;&gt;tidy&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;3&lt;/span&gt; Data Sources&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;
&lt;p&gt;
We have to main sources of data: (1) the FGV researchers&#39; curricula
vitae from &lt;a href=&quot;http://lattes.cnpq.br&quot;&gt;Lattes Platform&lt;/a&gt; and; (2) the FGV digital library.
&lt;/p&gt;

&lt;p&gt;
During the webinar I shared my screen and presented the web interface
that &lt;a href=&quot;http://cnpq.br&quot;&gt;CNPq&lt;/a&gt; provides for researchers update their resumes. I also shown
&lt;a href=&quot;http://lattes.cnpq.br/0675365413696898&quot;&gt;my curriculum vitae&lt;/a&gt; and discussed why I do not consider Brazilian
Researchers curricula vitae open data given the captcha that blocks
crawlers to get XML files from the Lattes website in a batch mode.
&lt;/p&gt;

&lt;p&gt;
I forgot to mention during the webinar but one of my old dreams is to
convice CNPq to add &lt;a href=&quot;http://www.w3.org/TR/xhtml-rdfa-primer/&quot;&gt;RDFa&lt;/a&gt; or &lt;a href=&quot;https://schema.org&quot;&gt;https://schema.org&lt;/a&gt; microformats into the
HTML pages of the curricula vitae. This would not only allow crawlers
to easier process the data but could also facilitate the maintainance
of the system. My current XSLT transformation of Lattes XML to RDF
cloud provide the starting point to RDFa embeeding.
&lt;/p&gt;

&lt;p&gt;
The &lt;a href=&quot;http://bibliotecadigital.fgv.br/dspace&quot;&gt;FGV Digital Library&lt;/a&gt; runs Dspace. The publications and collections
metadata are easly collected from Dspace using the &lt;a href=&quot;http://www.openarchives.org/&quot;&gt;OAI-PMH protocol&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-4&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;4&lt;/span&gt; Lattes XML Files&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-4&quot;&gt;
&lt;p&gt;
As I said in the previous section, the curricula vitae of Brazilian
Researchers are not open data, that is, they are not public available
in structured format. They are only public available as HTML pages in
the Lattes website, with limited search interface. The only way to
universities and research institutions get the curricula vitae of
their researchers in a structured format is to sign an &lt;a href=&quot;http://www.cnpq.br/web/portal-lattes/acordos-institucionais&quot;&gt;aggrement&lt;/a&gt; with
CNPq. FGV has signed this aggrement and we have one server authorized
to access CNPq web server to retrive the XML files of all curriculae
that have informed any professional activity with FGV.
&lt;/p&gt;

&lt;p&gt;
To transform the Lattes files to RDF, I use a XSLT transformation that
I developed few years ago. The XSLT is freely available at github in
the repository &lt;a href=&quot;https://github.com/arademaker/SLattes&quot;&gt;Semantic Lattes&lt;/a&gt;. 
&lt;/p&gt;

&lt;p&gt;
In this repo, I made also available the DTD that I try hard to keep
up-to-date. Unfortunately, until recently, CNPq did not make public
annoucement of chances in the structure of the XML files that they
produce, so I had to adapt the DTD whenever I identify changes. I have
just found in the end of &lt;a href=&quot;http://www.cnpq.br/web/portal-lattes/extracoes-de-dados&quot;&gt;this page&lt;/a&gt; that CNPq finally realized the
importance of making available the updated DTD. Nevertheless, the DTD
in the link of this page is outdated. At least, using this DTD to
validate the 489 FGV&#39;s curriculae I got more than 100 erros but using
my DTD to validate the same files I got only 2 errors. Considering
that those 489 files were produced my CNPq, we have two options: (1)
the DTD is outdated; or (2) the code that produces the XML files has
bugs. The two erros that I find using my DTD occur only with two
curriculae that were not updated in the last 2 years.
&lt;/p&gt;

&lt;p&gt;
After download the XML files, the general idea to process them and
produce the RDF files is outlined in the code below:
&lt;/p&gt;

&lt;div class=&quot;org-src-container&quot;&gt;

&lt;pre class=&quot;src src-sh&quot;&gt;&lt;span style=&quot;color: #a020f0;&quot;&gt;for&lt;/span&gt; f&lt;span style=&quot;color: #a020f0;&quot;&gt; in&lt;/span&gt; $&lt;span style=&quot;color: #a0522d;&quot;&gt;ROOT&lt;/span&gt;/ontos/xml/*.xml; &lt;span style=&quot;color: #a020f0;&quot;&gt;do&lt;/span&gt;
    &lt;span style=&quot;color: #a0522d;&quot;&gt;ID&lt;/span&gt;=$(&lt;span style=&quot;color: #ff00ff;&quot;&gt;basename&lt;/span&gt; $&lt;span style=&quot;color: #a0522d;&quot;&gt;f&lt;/span&gt; .xml)
    &lt;span style=&quot;color: #483d8b;&quot;&gt;echo&lt;/span&gt; Processing $&lt;span style=&quot;color: #a0522d;&quot;&gt;ID&lt;/span&gt;
    xmllint --noout --dtdvalid $&lt;span style=&quot;color: #a0522d;&quot;&gt;REPO&lt;/span&gt;/LMPLCurriculo.DTD $&lt;span style=&quot;color: #a0522d;&quot;&gt;f&lt;/span&gt; 2&amp;gt;&amp;gt; error.log
    xsltproc --stringparam ID $&lt;span style=&quot;color: #a0522d;&quot;&gt;ID&lt;/span&gt; $&lt;span style=&quot;color: #a0522d;&quot;&gt;REPO&lt;/span&gt;/lattes.xsl $&lt;span style=&quot;color: #a0522d;&quot;&gt;f&lt;/span&gt; &amp;gt; $&lt;span style=&quot;color: #a0522d;&quot;&gt;ID&lt;/span&gt;.rdf  
&lt;span style=&quot;color: #a020f0;&quot;&gt;done&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
After that, I import the RDF files to Allegro Graph making each
curriculum a separated graph so I can easly identify the provenance of
each triple. The importation is done using the &lt;a href=&quot;http://franz.com/agraph/support/documentation/current/agload.html&quot;&gt;agload&lt;/a&gt; utility. The
load process takes aprox. 2 minutes:
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
Load finished 487 sources in 00:02:03 (123.02 seconds).  
Triples added: 1,690,538 
Average Rate: 13742.00 tps
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-5&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-5&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;5&lt;/span&gt; Data Deduplication&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-5&quot;&gt;
&lt;p&gt;
I briefly commented about the deduplication of records during the
webinar. I do have to take care of removing duplicated resources about
the same entity. Considering a thesis defended by and student at FGV
whom have as advisor a professor at FGV. I will have metadata
(triples) about this thesis from three different sources: (1) the RDF
produced from the advisor&#39;s curriculum lattes XML; (2) the RDF produce
from the student&#39;s curriculum lattes XML; and (3) the RDF obtained
from the FGV Digital Library. 
&lt;/p&gt;

&lt;p&gt;
The &lt;a href=&quot;http://github.com/arademaker/vivo-code&quot;&gt;current code&lt;/a&gt; that I use to identify duplicated resources is a
Common Lisp library that is easily used if placed inside the
local-projects directory of a &lt;a href=&quot;http://www.quicklisp.org/&quot;&gt;Quicklisp&lt;/a&gt; instalation.
&lt;/p&gt;

&lt;p&gt;
I can write an entire article only about deduplication in RDF. I am
still thinking hard about this problem and really would like to find
better alternatives.  One can note that deduplication of nodes in a
RDF graph should not be done type by type as I am doing now. The rules
to identify resources as being refering the same entity could
dependent each other. That is, the deduplication of instances of
&lt;code&gt;foaf:Person&lt;/code&gt; can activate the rule to deduplicate instances of
&lt;code&gt;bibo:Article&lt;/code&gt; and vice-versa. It would be better to have a kind of
fixed point transformation in the RDF graph that could keep clustering
nodes until nothing more can be done. As a logician, I am very
interested in approach this problem in a more declarative and
deductive way.
&lt;/p&gt;

&lt;p&gt;
I also have to note that &lt;code&gt;owl:sameAs&lt;/code&gt; semantics doesn&#39;t help here. I
do use &lt;code&gt;owl:sameAs&lt;/code&gt; to mark the nodes that should be merged but I have
to merge the nodes after all &lt;code&gt;owl:sameAs&lt;/code&gt; triples are produced. I do
this with two SPARQL construct queries:
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
delete { ?s1 ?p ?o . }
insert { ?s2 ?p ?o . }
where {
  ?s1 owl:sameAs ?s2 .
  ?s1 ?p ?o .
  filter( !sameTerm(?p, owl:sameAs) )
}
&lt;/pre&gt;

&lt;pre class=&quot;example&quot;&gt;
delete { ?x ?p ?o1 . }
insert { ?x ?p ?o2 . }
where {
  ?o1 owl:sameAs ?o2 .
  ?x ?p ?o1 .
  filter( !sameTerm(?p, owl:sameAs) )
}
&lt;/pre&gt;

&lt;p&gt;
Note that the filters block the propagation of the &lt;code&gt;owl:sameAs&lt;/code&gt;
triples. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-6&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-6&quot;&gt;&lt;span class=&quot;section-number-2&quot;&gt;6&lt;/span&gt; Mapping Lattes RDF to VIVO RDF&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-6&quot;&gt;
&lt;p&gt;
To map the Lattes RDF model produced by my XSLT to the expected VIVO
RDF model, I have to look carefully to each instance of data. This
mapping is not completed but at this point I have already mapped most
of the data about people, publication, research areas and
departaments.
&lt;/p&gt;

&lt;p&gt;
To work on the rules and queries to transform the data, I used the
query and data browsing tools developed by Franz: Gruff and
AllegroGraph WebView. During the webinar I presented both systems.
&lt;/p&gt;

&lt;p&gt;
The mapping is developed as rules that were easly tested with CWM. One
example of rules is
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
{ ?dept foaf:member ?person ;
        rdf:type foaf:Group . } =&amp;gt; 
{ [ vivo:relates ?dept ;
    vivo:relates ?person ;
    a vivo:FacultyPosition ;
    rdfs:label &quot;Professor Adjunto&quot;@pt ] . } .
&lt;/pre&gt;

&lt;p&gt;
Rules like the one above are placed in an n3 file and executed by CWM
that receives the rule file and the data file and produces the data
output file. Unfortunately, CWM does not have good performance and I
haven&#39;t even tried to use it with all the data. I develop the rules
and test them with only one curriculum vitae file.
&lt;/p&gt;

&lt;p&gt;
Once I finish to test the rules, I rewrite them as SPARQL queries. The
one above becomes:
&lt;/p&gt;

&lt;pre class=&quot;example&quot;&gt;
insert 
{ graph &amp;lt;http://www.fgv.br/vivo/import/&amp;gt; 
  {            
   [ vivo:relates ?dept ;
     vivo:relates ?person ;
      a vivo:FacultyPosition ;
     rdfs:label &quot;Professor Adjunto&quot;@pt ] . 
  }
}
where
{ ?dept foaf:member ?person ;
        rdf:type foaf:Group . 
}
&lt;/pre&gt;

&lt;p&gt;
Note that: (1) the query produces blank nodes that need to be
transformed into normal nodes before loaded into VIVO; (2) All created
triples are placed in a separated graph; and (3) if this query is
executed twice it will generate duplicated and dispensable
triples. This is the most important limitation of using SPARQL for
me. CWM will only execute a rule whenever necessary and the rules do
not have to explicit declare any condition to avoid unnecessary
creation of triples.
&lt;/p&gt;

&lt;p&gt;
It is still not clear to me if all SPARQL queries can be rewrited to
prevent non necessary creation of triples. Moreover, I don&#39;t want to
have too complicated SPARQL queries to maintain.
&lt;/p&gt;

&lt;p&gt;
More on the next post.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>We found a bug in the split command of Mac OS</title>
   <link href="http://arademaker.github.com/blog/2013/07/18/we-found-a-bug-in-macos-split.html"/>
   <updated>2013-07-18T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2013/07/18/we-found-a-bug-in-macos-split</id>
   <content type="html">&lt;p&gt;Yesterday my friend
&lt;a href=&quot;http://researcher.ibm.com/researcher/view.php?person=br-mnerys&quot;&gt;Marcelo Nery&lt;/a&gt;
and I found a bug in the &lt;code&gt;split&lt;/code&gt; command of Mac OS. At first, I was
surprised (we don&#39;t expect to find bugs in core tools like grep, ls,
split, wc..., right?) and almost expected to find the same bug in the
Linux version of split. At least in the split of Ubuntu distribution,
that was not the case. The bug is presented only in the Mac OS
version.&lt;/p&gt;

&lt;h2&gt;The bug&lt;/h2&gt;

&lt;p&gt;Consider the file &lt;code&gt;zero.log&lt;/code&gt; created with the following Common Lisp code
(actually for the rest of the post you don&#39;t need to understand the
code):&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;common-lisp&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;with-open-file&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;zero.log&amp;quot;&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:element-type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned-byte&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;ss&quot;&gt;:direction&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:output&lt;/span&gt;
                  &lt;span class=&quot;ss&quot;&gt;:if-exists&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:supersede&lt;/span&gt;
                  &lt;span class=&quot;ss&quot;&gt;:external-format&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:utf-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;across&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;AB_CDE~%F~%G~%&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
             &lt;span class=&quot;nb&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;equal&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;#\_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;do&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;write-byte&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;write-byte&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;char-code&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This file content could be inspected with hexdump command:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hexdump -C zero.log
00000000  41 42 00 00 00 43 44 45  0a 46 0a 47 0a     |AB...CDE.F.G.|
0000000d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That is, the file has three 0 bytes in the first line right after the
&lt;code&gt;B&lt;/code&gt; letter and before the &lt;code&gt;C&lt;/code&gt; letter. Now we want to split this file
one line per file.&lt;/p&gt;

&lt;p&gt;The Linux version of split works as expected, it splits the file
keeping the zero bytes unchanged.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;split -1 zero.log
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;f in x??; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;---Begin: $f&amp;quot;&lt;/span&gt;; cat &lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;---End: $f&amp;quot;&lt;/span&gt;; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
---Begin: xaa
ABCDE
---End: xaa
---Begin: xab
F
---End: xab
---Begin: xac
G
---End: xac
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Moreover, the sum of bytes of the &lt;code&gt;x??&lt;/code&gt; files is equal the number of
bytes in the &lt;code&gt;zero.log&lt;/code&gt; file, 13 bytes.&lt;/p&gt;

&lt;p&gt;Nevertheless, the Mac OS version of split produces an unexpected
output. The letter &lt;code&gt;F&lt;/code&gt; is merged with the begining of the first line
althouth it is in the second line of the &lt;code&gt;zero.log&lt;/code&gt; file. Besides
that, the zero bytes causes the Mac OS split to ignore the rest of the
first line of &lt;code&gt;zero.log&lt;/code&gt; causing a lost of data. The sum of the bytes
of the &lt;code&gt;x??&lt;/code&gt; files in Mac OS is only 6 bytes.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;split -1 zero.log
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;f in x??; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;---Begin: $f&amp;quot;&lt;/span&gt;; cat &lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;---End: $f&amp;quot;&lt;/span&gt;; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
---Begin: xaa
ABF
---End: xaa
---Begin: xab
G
---End: xab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Reporting the bug&lt;/h2&gt;

&lt;p&gt;I reported the bug to Apple using the
&lt;a href=&quot;http://www.apple.com/feedback/macosx.html&quot;&gt;Mac OS Feedback form&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The GeTFun Research Project</title>
   <link href="http://arademaker.github.com/blog/2013/01/17/GeTFun.html"/>
   <updated>2013-01-17T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2013/01/17/GeTFun</id>
   <content type="html">&lt;p&gt;January, 1 2013 marked the official beginning of the
&lt;a href=&quot;http://sqig.math.ist.utl.pt/GeTFun/&quot;&gt;GeTFun&lt;/a&gt; project. I am very
excited about this project and willing to contributed and share ideas
with all collaborators. The first workshop of the project will happen
during the &lt;a href=&quot;http://www.uni-log.org/&quot;&gt;4th UNILOG 2013&lt;/a&gt; in Rio de
Janeiro.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pacote R dicionariosIBGE</title>
   <link href="http://arademaker.github.com/blog/2012/09/20/pacote-dicionariosIBGE.html"/>
   <updated>2012-09-20T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2012/09/20/pacote-dicionariosIBGE</id>
   <content type="html">&lt;p&gt;Disponibilizei hoje no github e submeti para o CRAN a versão 1.5 do
pacote &lt;a href=&quot;https://github.com/arademaker/dicionariosIBGE/&quot;&gt;dicionariosIBGE&lt;/a&gt;.
Este pacote contém os dicionários das principais pesquisas do
&lt;a href=&quot;http://www.ibge.gov.br/&quot;&gt;IBGE&lt;/a&gt;: PNAD (1983-2009), POF (1987-1996) e
PME. Também incluimos nesta versão variáveis adicionais com rótulos
para as variáveis categóricas de cada pesquisa.&lt;/p&gt;

&lt;p&gt;Nesta nova versão, incorporamos a função &lt;code&gt;le.pesquisa&lt;/code&gt; original do
pacote &lt;code&gt;IBGEPesq&lt;/code&gt; que foi desenvolvido no IBGE mas nunca submetido ao
CRAN. O pacote &lt;code&gt;IBGEPesq&lt;/code&gt; é distribuído pelo IBGE nos CDs e DVDs
vendidos na &lt;a href=&quot;http://loja.ibge.gov.br&quot;&gt;loja virtual do IBGE&lt;/a&gt; e
disponibilizo no site do IBGE. Vide link &quot;Leitura em R&quot; na
&lt;a href=&quot;http://www.ibge.gov.br/home/estatistica/populacao/trabalhoerendimento/pnad2009/microdados.shtm&quot;&gt;página da última PNAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Fizemos várias otimizações na função &lt;code&gt;le.pesquisa&lt;/code&gt; original e
incluímos ainda um argumento extra para leitura dos rótulos das
variáveis categóricas. A partir desta versão do dicionariosIBGE, o
pacote &lt;code&gt;IBGEPesq&lt;/code&gt; torna-se desnecessário para leitura dos microdados
das pesquisas do IBGE contempladas pelo dicionariosIBGE.&lt;/p&gt;

&lt;p&gt;Observo, no entanto, que a novidade de disponibilizarmos os rótulos
para as variáveis categóricas e adaptarmos a função &lt;code&gt;le.pesquisa&lt;/code&gt; para
usar os rótulos para construir factors ainda é
experimental. Pessoalmente, questiono sua utilidade em geral. Existem
variáveis categóricas cujos rótulos são realmente úteis. Por exemplo,
variáveis como a &lt;code&gt;UF&lt;/code&gt; da tabela de PESSOA da PNAD de 2009. Certamente
trabalharmos com um factor com rótulo &quot;Rio de Janeiro&quot; ao invés de
apenas o valor 33 torna o manuseio dos dados mais fácil. No entanto,
existem variáveis categóricas como a &lt;code&gt;V2927&lt;/code&gt; cujos rótulos são tão
grandes e verbosos que provavelmente não facilitam em nada o trabalho
com os dados. Vide:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; rotpes2009[rotpes2009$cod == &quot;V2927&quot;,]
       cod valor                                  rotulo
1070 V2927     1                     Custaria muito caro
1071 V2927     2                         Era muito longe
1072 V2927     3                     Por falta de provas
1073 V2927     4                         Demoraria muito
1074 V2927     5 Cabia as outras partes iniciarem a acao
1075 V2927     6    Por medo de outras partes envolvidas
1076 V2927     7     por meio de mediacao ou conciliacao
1077 V2927     8                 Nao acredita na justica
1078 V2927     9  Nao sabia que podia utilizar a Justica
1079 V2927    10                                  Outros
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;De qualquer modo, ao invés de usar os rótulos que distribuímos no
pacote, nada impede que os usuários do pacote de: (1) passarem para a
função &lt;code&gt;le.pesquisa&lt;/code&gt; seus próprios data.frames com rótulos, bastando
fornecer um data.frame com os rótulos no formato esperado (três
colunas: cod, valor, rotulo); ou (2) passar o valor &lt;code&gt;NULL&lt;/code&gt; para os
rótulos (default da função) retornando os dados como numéricos e/ou
string e não factors.&lt;/p&gt;

&lt;p&gt;O pacote foi desenvolvido e testado no MacOS mas deve funcionar sem
problemas no Linux e Windows.&lt;/p&gt;

&lt;p&gt;Finalmente, cabe destacar que desenvolvemos o pacote usando os
dicionários que encontramos nos CDs e DVDs disponíveis na FGV. Já
tivemos casos de diferentes CDs da mesma pesquisa, adquiridos em
diferentes momentos do IBGE, terem conteúdos diferentes (diferenças
nos dicionários e diferenças nos arquivos de dados). O IBGE parece
produzir os CDs e DVDs das pesquisas por demanda, o que pode explicar
as diferenças entre CDs da mesma pesquisa.&lt;/p&gt;

&lt;p&gt;A falta de padrão do IBGE na distribuição dos arquivos das pesquisas é
um grande problema para os pesquisadores e atrabalha bastante nossa
iniciativa de facilitar a leitura dos dados do IBGE em R. Para
contornar os problemas, poderíamos tentar distribuir os dados do IBGE
já em formato RData, para carga no R diretamente. No entanto, além das
possíveis questões legais (não está claro no site do IBGE qual a
licença de uso adotada pelo IBGE), certamente a credibilidade dos
dados é maior se os dados são obtidos diretamente do site ou mídia do
IBGE.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>O problema dos galões</title>
   <link href="http://arademaker.github.com/blog/2012/05/06/problema-dos-galoes.html"/>
   <updated>2012-05-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2012/05/06/problema-dos-galoes</id>
   <content type="html">&lt;p&gt;Na primeira prova do curso de &lt;a href=&quot;/CA-2012-1/&quot;&gt;estrutura de dados&lt;/a&gt; que
dei este ano, coloquei a seguinte questão para os alunos.&lt;/p&gt;

&lt;p&gt;Nós temos 3 galões de tamanho 10, 7 e 4 litros. Os galões de 7 e 4
litros começam cheios e o galão de 10 litros vazio. Só podemos
realizar um tipo de operação com os galões: derramar todo o conteúdo
de um galão em outro, parando apenas quando o galão sendo derramado
ficar vazio ou o destino ficar cheio. Queremos saber se existe uma
sequência de operações que termine com 2 litros no galão de 7 ou 4
litros. Pede-se:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Modele o problema como um problema de grafo, descrevendo
precisamente a definição do grafo envolvido e descrevendo a solução
em função do grafo.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Qual algorítmo de grafo deverá ser usado?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Este é um problema clássico que pode ser encontrado em vários livros
de estrutura de dados e algorítmos (Ex 3.8 do Algorithms by Dasgupta,
Papadimitriou e Vazirani
&lt;a href=&quot;http://www.amazon.com/Algorithms-Sanjoy-Dasgupta/dp/0073523402&quot;&gt;Amazon&lt;/a&gt;,
&lt;a href=&quot;http://www.cs.berkeley.edu/~vazirani/algorithms.html&quot;&gt;site do livro&lt;/a&gt;,
&lt;a href=&quot;http://demonstrations.wolfram.com/WaterPouringProblem/&quot;&gt;Wolfram&lt;/a&gt;). Trata-se
de ver os estados do sistema (a quantidaade de líquido em cada galão)
como um nó de um grafo. As arestas representam as possíveis transições
de estado após transferência de líquido entre dois galões.&lt;/p&gt;

&lt;p&gt;Um formalismo bem interessante para implementar a solução deste
problema é lógica de reescrita. Em particular, resolvi usar
&lt;a href=&quot;http://maude.cs.uiuc.edu/&quot;&gt;Maude&lt;/a&gt;. Maude é uma implementação bastante
eficiente e conhecida de lógica de reescrita, o nome refere-se a
linguagem e ao sistema ao mesmo tempo.&lt;/p&gt;

&lt;p&gt;A implementação em Maude é muito simples:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mod GAL is
  inc INT .

  sorts Galon System .
  subsort Galon &amp;lt; System .

  op &amp;lt;_,_&amp;gt; : Int Int -&amp;gt; Galon .
  op __ : System System -&amp;gt; System [assoc comm id: null] . 
  op null : -&amp;gt; System .

  vars N1 N2 M1 M2 : Int .

  crl [transfer-1] :
     &amp;lt; N1 , M1 &amp;gt; &amp;lt; N2 , M2 &amp;gt; =&amp;gt; 
     &amp;lt; 0 , M1 &amp;gt; &amp;lt; (N1 + N2) , M2 &amp;gt; 
   if N1 &amp;lt; (M2 - N2) .

  crl [transfer-2] :
     &amp;lt; N1 , M1 &amp;gt; &amp;lt; N2 , M2 &amp;gt; =&amp;gt; 
     &amp;lt; (N1 - (M2 - N2)) , M1 &amp;gt; &amp;lt; M2 , M2 &amp;gt; 
   if N1 &amp;gt; (M2 - N2) .

  op initial : -&amp;gt; System .
  eq initial = &amp;lt; 0 , 10 &amp;gt; &amp;lt; 7 , 7 &amp;gt; &amp;lt; 4 , 4 &amp;gt; .
endm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Trata-se de uma especificação algébrica do estado do sistema como um
multiset de galões onde cada galão é uma dupla de números inteiros:
primeiro componente é a quantidade de líquido e segundo a
capacidade. A operação &lt;code&gt;initial&lt;/code&gt; serve apenas para criar uma constante
que representa o estado inicial do sistema.&lt;/p&gt;

&lt;p&gt;As duas regras de reescrita fazem deste módulo um módulo de sistema
ou, mais formalmente, uma etoria de reescrita. Estas regras
implementam as duas possibilidades de transferência de líquido entre
um galão e outro. Na primeira regra, todo o líquido é transferido de
um galão para outro esvaziando o galão origem. Na segunda, a
transferência é interrompida quando o galão destino torna-se
cheio. Isto poderia ainda ser simplificado para uso de apenas uma
regra, certo?!&lt;/p&gt;

&lt;p&gt;O interessante de implementar em Maude é que as regras de reescrita
podem ser entendidas, computacionalmente, como transições de estado de
um sistema. Logicamente, as regras podem ser entendidas como regras de
inferência de um sistema de reescrita.&lt;/p&gt;

&lt;p&gt;Na prática, podemos aplicar as reescritas de estado usando o comando
&lt;code&gt;rew&lt;/code&gt; e realizar uma busca por estados a partir de um estado inicial
usando o comando &lt;code&gt;search&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para testarmos a implementação, podemos aplicar algumas regras a
partir do estado initial. Por exemplo, para aplicarmos 4 regras de
reescrita a partir do estado inicial usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Maude&amp;gt; rew [4] initial .
rewrite [4] in GAL : initial .
rewrites: 33 in 0ms cpu (0ms real) (131474 rewrites/second)
result System: &amp;lt; 0,4 &amp;gt; &amp;lt; 4,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note-se que não controlamos que regras são aplicadas (estratégia de
aplicação). Para encontrarmos a solução, usamos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;search initial =&amp;gt;* &amp;lt; 2 , X:Int &amp;gt; S:System such that X:Int &amp;lt; 10 .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Este comando acima efetua uma busca (BFS, busca em largura), a partir
do estado inicial, por algum estado atingido com zero ou mais
transições (aplicações de alguma das duas regras que definimos), por
um estado onde exista algum galão com dois litros e cuja capacidade
seja menor que 10.&lt;/p&gt;

&lt;p&gt;A resposta do sistema são 4 possíveis soluções:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Solution 1 (state 16)
states: 17  rewrites: 501 in 3ms cpu (3ms real) (139244 rewrites/second)
S:System --&amp;gt; &amp;lt; 2,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
X:Int --&amp;gt; 4

Solution 2 (state 18)
states: 19  rewrites: 559 in 4ms cpu (4ms real) (127334 rewrites/second)
S:System --&amp;gt; &amp;lt; 0,7 &amp;gt; &amp;lt; 9,10 &amp;gt;
X:Int --&amp;gt; 4

Solution 3 (state 19)
states: 20  rewrites: 610 in 5ms cpu (5ms real) (119210 rewrites/second)
S:System --&amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 5,10 &amp;gt;
X:Int --&amp;gt; 7

Solution 4 (state 20)
states: 21  rewrites: 627 in 5ms cpu (5ms real) (112164 rewrites/second)
S:System --&amp;gt; &amp;lt; 0,4 &amp;gt; &amp;lt; 9,10 &amp;gt;
X:Int --&amp;gt; 7

No more solutions.
states: 21  rewrites: 721 in 6ms cpu (6ms real) (109824 rewrites/second)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para examinarmos a primeira solução, pedimos para o sistema mostrar o
caminho, isto é, as reescritas executadas.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Maude&amp;gt; show path 16 .
state 0, System: &amp;lt; 0,10 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 7,7 &amp;gt;
===[ crl ... [label transfer-1] . ]===&amp;gt;
state 1, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 4,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 4, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 1,7 &amp;gt; &amp;lt; 10,10 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 8, System: &amp;lt; 1,7 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 6,10 &amp;gt;
===[ crl ... [label transfer-1] . ]===&amp;gt;
state 12, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 5,7 &amp;gt; &amp;lt; 6,10 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 14, System: &amp;lt; 2,10 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 5,7 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 16, System: &amp;lt; 2,4 &amp;gt; &amp;lt; 2,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O estado encontrado é onde o galão de 4 litros está com 2 litros, o
galão de 7 litros está completo e o galão de 10 litros está com 2
litros.&lt;/p&gt;

&lt;p&gt;Bem legal, não acham!?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Lattes to BibTeX</title>
   <link href="http://arademaker.github.com/blog/2012/02/15/lattes-to-bibtex.html"/>
   <updated>2012-02-15T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2012/02/15/lattes-to-bibtex</id>
   <content type="html">&lt;p&gt;Disponibilizei hoje online no github uma transformação do &lt;a href=&quot;http://lattes.cnpq.br/&quot;&gt;Lattes&lt;/a&gt; para
&lt;a href=&quot;http://en.wikipedia.org/wiki/BibTeX&quot;&gt;BibTeX&lt;/a&gt;, vejam o repositório:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/arademaker/SLattes&quot;&gt;http://github.com/arademaker/SLattes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Eu acabei fazendo esta transformação por dois motivos. O primeiro para
meu uso pessoal, eu já estava querendo faz muito tempo conseguir gerar
um BibTex com minhas produções. O segundo foi como parte do projeto
Semantic Lattes. A idéia é que uma transformação dos dados do Lattes
para algum padrão de referências como o
&lt;a href=&quot;http://www.loc.gov/standards/mods/&quot;&gt;XML/MODS da Library of Congress&lt;/a&gt;,
ajuda a validar os dados do Lattes.&lt;/p&gt;

&lt;p&gt;Intruções de como usar o transformador estão no README do repositório,
mas como o texto lá está em inglês, segue a idéia geral. O que fiz foi
uma transformação XSLT do XML/Lattes para o XML/MODS. Este último pode
ser então facilmente convertido para BibTex usando o xml2bib, programa
do pacote &lt;a href=&quot;http://sourceforge.net/p/bibutils/&quot;&gt;Bibutils&lt;/a&gt; disponivel no
Linux e no MacOS (MacPorts). Para executar a transformação e validar
um XML em relação ao seu DTD/Schema, ainda são necessários os
programas xsltproc e xmllint.&lt;/p&gt;

&lt;h2&gt;Instalação dos programas necessários&lt;/h2&gt;

&lt;p&gt;Para quem usa o MacPorts no MacOS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo port -v install bibtool bibutils
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;O xmllint e xsltproc já estão instalados no MacOS (acho que no XCode).&lt;/p&gt;

&lt;p&gt;Para quem usa Linux/Ubuntu:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install bibtool bibutils xsltproc libxml2-utils
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Para quem usa Windows, consultar o site destas ferramentas, não
trivial! Esquece o Windows e instala o Ubuntu! ;-)&lt;/p&gt;

&lt;h2&gt;Usando a transformação&lt;/h2&gt;

&lt;p&gt;Com as ferramentas instaladas, o primeiro passo é acesssar o sistema
&lt;a href=&quot;http://lattes.cnpq.br&quot;&gt;Lattes do CNPq&lt;/a&gt;, link &lt;strong&gt;atualizar&lt;/strong&gt;, logar-se
no sistema e escolher a opção de exportar para XML. Será iniciado o
download de um arquivo ZIP. Abra ZIP e extraia o arquivo XML dentro
dele que tem mesmo nome, seu lattes ID. Imagine então que vc renomeou
este arquivo XML para &lt;code&gt;LATTES.xml&lt;/code&gt; e o moveu para o mesmo diretório
onde está o arquivo com &lt;code&gt;lattes2mods.xsl&lt;/code&gt; que você pegou do
repositório no github.&lt;/p&gt;

&lt;p&gt;Agora basta rodar:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xsltproc lattes2mods.xsl LATTES.xml &amp;gt; LATTES.mods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;E em seguida:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xml2bib -b -w LATTES.mods &amp;gt; LATTES.bib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Mas o interessante é antes de rodar o xml2bib, validar o arquivo mods
gerado contra o
&lt;a href=&quot;http://www.loc.gov/standards/mods/mods-schemas.html&quot;&gt;XML Schema do MODS&lt;/a&gt;,
disponibilizado no site da Biblioteca do Congresso Americano, baixe a
versão 3.4:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xmllint --schema mods.xsd LATTTES.mods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Esta validação serve para verificar não apenas erros na estrutura do
arquivo, que seriam bugs no meu código, mas também erros nos dados, em
função de informações erradas (faltantes, em lugar errado etc) no
Lattes.&lt;/p&gt;

&lt;p&gt;Comentários são sempre bem vindos. Problemas podem ser reportados
diretamente no &lt;a href=&quot;https://github.com/arademaker/SLattes/issues&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>R Package SSOAP</title>
   <link href="http://arademaker.github.com/blog/2012/01/02/package-SSOAP.html"/>
   <updated>2012-01-02T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2012/01/02/package-SSOAP</id>
   <content type="html">&lt;p&gt;Last year, during a summer course that I gave at FGV, I taught the
students how to use SOAP protocol do retrive data from
&lt;a href=&quot;http://bcb.gov.br/&quot;&gt;Banco Central do Brasil&lt;/a&gt; using R. BCB has a
system called SGS (Sistema Gerenciador de Séries Temporais) that has a
SOAP interface.&lt;/p&gt;

&lt;p&gt;At that time, the &lt;a href=&quot;http://www.omegahat.org/SSOAP/&quot;&gt;package SSOAP&lt;/a&gt; had
a small bug that I contributed to fix. Today I found that my
contribution was incorporated in
&lt;a href=&quot;http://www.omegahat.org/SSOAP/Changes.html&quot;&gt;version 0.5-5&lt;/a&gt; of this
package whish makes my
&lt;a href=&quot;https://github.com/arademaker/SSOAP&quot;&gt;repository at github&lt;/a&gt; outdated.&lt;/p&gt;

&lt;p&gt;It took my a couple of minutes to test the new version of this
package. Since I am running the last version of R, 2.14, the general
procedure for install packages didn&#39;t work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; install.packages(&quot;SSOAP&quot;)
Warning message:
In getDependencies(pkgs, dependencies, available, lib) :
  package ‘SSOAP’ is not available (for R version 2.14.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I finnaly figured out how to install the last version from source
using the Omegahat repository version with the command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; install.packages(&quot;SSOAP&quot;, repos = &quot;http://www.omegahat.org/R&quot;, 
                   dependencies = TRUE, 
                   type = &quot;source&quot;)
trying URL &#39;http://www.omegahat.org/R/src/contrib/SSOAP_0.8-1.tar.gz&#39;
Content type &#39;application/x-gzip&#39; length 195424 bytes (190 Kb)
opened URL
==================================================
downloaded 190 Kb

* installing *source* package ‘SSOAP’ ...
** R
** inst
** preparing package for lazy loading
Creating a new generic function for ‘help’ in package ‘SSOAP’
Warning in .NonstandardGenericTest(body(fdef), name, stdGenericBody) :
  the supplied generic function definition for toSOAP does not
  seem to call &#39;standardGeneric&#39;; no methods will be dispatched!
** help
*** installing help indices
** building package indices ...
** testing if installed package can be loaded

* DONE (SSOAP)

The downloaded packages are in
    ‘/private/var/.../downloaded_packages’
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, I was prepared to actually test the package running the
code that I created during the
&lt;a href=&quot;https://github.com/arademaker/IR-2011/&quot;&gt;course&lt;/a&gt; (lesson 7 directory
aula-07). But some changes in RCurl package requires a change in how
we ask for not verify the ssl certificate. That is, I had to replace
the &lt;code&gt;ssl.verifypeer = FALSE&lt;/code&gt; argument by a list of options in the call
of the function &lt;code&gt;ff@functions$getValoresSeriesXML&lt;/code&gt;. The last version
of this script is now available as a gist here:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/1550651.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;Note that the &lt;code&gt;ssl.verifypeer&lt;/code&gt; argument is necessary because the
certificate used in BCB website is invalid! What a shame!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2012-01-02-bcb-certificate.png&quot; alt=&quot;BCB certificate&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>GitHub Pages and Jekyll plugins</title>
   <link href="http://arademaker.github.com/blog/2011/12/01/github-pages-jekyll-plugins.html"/>
   <updated>2011-12-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/12/01/github-pages-jekyll-plugins</id>
   <content type="html">&lt;p&gt;Everyone that use &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; and wants to host the site at GitHub should know that GitHub Pages does not allow custom plugins, right? Using Jekyll for a little more than a blog site, like &lt;a href=&quot;http://emap.fgv.br&quot;&gt;EMAp/FGV&lt;/a&gt; will require plugins. In my case, avoid the use of custom plugins is not an option.&lt;/p&gt;

&lt;p&gt;The solution is trivial, one has to run Jekyll locally and post the produzed files into a master branch of a git repo, following the conventions described at &lt;a href=&quot;http://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; por person and organization pages. The problem that I faced was to choose the best way to organize and keep tracking of the source and produzed files.&lt;/p&gt;

&lt;p&gt;I read a couple of posts with possible solutions. The solution by &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;Charlie Park&lt;/a&gt; force us to have two distinct git repositories. I don&#39;t like this approach because of that. Our website will be maintained by more than one person, having two distinct repositories since that I will not using all git features. The solution by &lt;a href=&quot;http://tech.hugr.fr/blog/2011/08/07/how-to-host-a-jekyll-app-on-github-pages-with-plugins/&quot;&gt;Jean Denis&lt;/a&gt; is a little bit better but keeps me thinking about why he need to keep the produced files under version control in the two branches, gh-pages and the master.&lt;/p&gt;

&lt;p&gt;My final solution is to keep the directory produced by jekyll out of git control. This simple thing allows me to switch the branches, from source to master, and still have access to the produced files, directory &lt;code&gt;_site&lt;/code&gt;. Once in the master branch, I only have to move the files under &lt;code&gt;_site/&lt;/code&gt; to the root directory in the master branch and update the master branch before push it to GitHub.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git checkout source
// do whatever you need
git status / git add / git commit
jekyll
checkout master
cp -r _site/* . &amp;amp;&amp;amp; rm -rf _site/ &amp;amp;&amp;amp; touch .nojekyll
git status &amp;gt; git add &amp;gt; git commit
git push -all origin
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Comentários sobre ECLM 2011 e ISWC 2011 (1/2)</title>
   <link href="http://arademaker.github.com/blog/2011/11/27/ultimas-conferencias.html"/>
   <updated>2011-11-27T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/11/27/ultimas-conferencias</id>
   <content type="html">&lt;p&gt;Desde que voltei da minha última viagem para participar da
&lt;a href=&quot;http://weitz.de/eclm2011/&quot;&gt;ECLM 2011&lt;/a&gt; e
&lt;a href=&quot;http://iswc2011.semanticweb.org/&quot;&gt;ISWC 2011&lt;/a&gt;, estou pensando em
escrever sobre o assunto. Por estar escrevendo em português, acho que
tenho lá ainda alguma chance de contribuir com algo novo. Neste post,
vou falar da ISWC 2011. Vou começar listando alguns blogs que já
escreveram sobre estas conferências em inglês.&lt;/p&gt;

&lt;p&gt;Das duas conferências, certamente a ISWC é a maior e, por isso, também
foi a mais comentada. São vários os posts de pessoas que escreveram
sobre ela. Ivan Herman escreveu
&lt;a href=&quot;http://ivan-herman.name/2011/11/02/some-notes-on-iswc2011.../&quot;&gt;Some notes on ISWC2011&lt;/a&gt;.
Também sobre a ISWC-2011 vale a pena ler o post
&lt;a href=&quot;http://blog.phenoscape.org/2011/11/03/notes-from-iswc-2011/&quot;&gt;Notes from ISWC 2011&lt;/a&gt;
e a série de 5 posts de
&lt;a href=&quot;http://semanticweb.com/report-from-day-1-at-iswc_b24150&quot;&gt;Juan Sequeda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Como Ivan escreveu no post dele, diferentes pessoas certamente
relatarão diferentes experiências da ISWC 2011. Me chamou atenção os
comentários dele sobre os trabalhos apresentados relacionados à
visualização de ontologias. Embora o assunto me interesse, acho que
perdi os workshops ou seções relacionadas a este tema. Foi bom ter
lido o post dele.&lt;/p&gt;

&lt;p&gt;Da minha parte, gostei bastante da ISWC 2011. A conferência foi bem
organizada, o hotel muito bom e a cidade muito agradável. Dos
workshops, o mais interessante para mim certamente foi o
&lt;a href=&quot;http://www.om2011.ontologymatching.org/&quot;&gt;Ontology Matching&lt;/a&gt;, afinal,
é o assunto que mais me interessa e sobre o qual tenho artigos
publicados com Isabel Cafezeiro e Hermann. Assistir ao workshop me
motivou a voltar a este assunto e tentar implementar efetivamente as
idéias que formalizamos nos artigos.&lt;/p&gt;

&lt;p&gt;Das seções da conferência, gostei bastante da &quot;Ontology Matching,
Mapping&quot; e da &quot;KR - Semantics&quot;. No mais, vale dizer que os posters
também estavam ótimos e a idéia de cada poster ser apresentado em 1
minuto foi bem divertida embora apenas alguns apresentadores tenham
entendido o espírito da coisa! Das apresentações dos convidados, o que
mais fez sucesso foi Frank van Harmelen com o título
&lt;a href=&quot;http://www.cs.vu.nl/~frankh/spool/ISWC2011Keynote/&quot;&gt;10 Years of Semantic Web: does it work in theory?&lt;/a&gt;.
Para mim, sendo minha área de pesquisa exatamente lógicas e, em
particular, nos últimos anos, lógicas de descritivas, ter uma
apresentação sugerindo que a comunidade de web semântica deve voltar a
atenção para os fundamentos teóricos da área, é bastante motivador.
Infelizmente assisti apenas parte da apresentação. O painel &quot;Semantic
Web Death Match&quot; foi meio sem graça, embora na sala, assistindo as
discussões, tenho que dizer que foi uma experiência única participar
do painel pelo twitter. No twitter as discussões foram até mais
interessantes. Finalmente, da seção &quot;MANCHustifications and
Provenance&quot;, tive a idéia de revisar o texto da minha tese que será
publicado pela Springer. A idéia é que pode-se obter diretamente de
uma prova formal, usando os sistemas dedutivos que apresento para
algumas DL em minha tese, a tal &quot;justification of an entailment&quot;. O
termo refere-se ao conjunto mínimo de axiomas usados para justificar
uma conclusão lógica (qual seria a melhor tradução para
&quot;entailment&quot;?). Obrigado
&lt;a href=&quot;http://manchester.academia.edu/SamanthaBail&quot;&gt;Samantha Bail&lt;/a&gt; por me
ajudar a confirmar a impressão que tive durante a apresentação dos
trabalhos na seção.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Gráficos de séries temporais no R</title>
   <link href="http://arademaker.github.com/blog/2011/10/31/time-series-R.html"/>
   <updated>2011-10-31T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/10/31/time-series-R</id>
   <content type="html">&lt;p&gt;Pergunta de dois alunos: como postar duas séries temporáis usando o
ggplot? Resolvi responder usando este post. Vou aproveitar então para
mostrar como postar séries temporáis usando o ggplot, lattice e o plot
padrão do R.&lt;/p&gt;

&lt;h2&gt;Criando os dados&lt;/h2&gt;

&lt;p&gt;Para começar, criei um objeto série temporal conforme exemplo da função &lt;code&gt;ts&lt;/code&gt;
do R. Este objeto na realidade é uma matriz, são três séries temporais
ou uma série temporal multivariável.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; dados &amp;lt;- ts(matrix(rnorm(300), 100, 3), start=c(1961, 1), 
              frequency=12)
&amp;gt; dados[1:4,]
       Series 1   Series 2   Series 3
[1,]  1.4165848  2.1049293  1.0155993
[2,] -0.4264193 -0.2730903  0.8754992
[3,]  0.5120809 -0.4023986  1.9757084
[4,]  0.1375277 -0.5043973 -0.7795633
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Plot básico&lt;/h2&gt;

&lt;p&gt;Se usarmos o comando padrão de plot do pacote basic do R, temos o
seguinte plot.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; plot(ts)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-basic.png&quot; title=&quot;plot basic&quot; alt=&quot;plot-basic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviamente, neste e nos próximos exemplos, estou mostrando o uso mais
básico das funções, sem me preocupar com nenhum ajuste de formatação,
legenda, rótulos, cores etc.&lt;/p&gt;

&lt;h2&gt;No pacote lattice&lt;/h2&gt;

&lt;p&gt;No pacote Lattice, temos a função xyplot que contém vários
parâmetros para geração do gráfico. Explorei apenas a forma de gerar
uma série por painel e todas as séries em um painel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; xyplot(dados)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-lattice-1.png&quot; title=&quot;plot lattice&quot; alt=&quot;plot-lattice-1&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; xyplot(dados, superpose = TRUE) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-lattice-2.png&quot; title=&quot;plot lattice&quot; alt=&quot;plot-lattice-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A página de help da função &lt;code&gt;xyplot&lt;/code&gt; contém muito mais informação sobre
os demais parâmetros da função. A função &lt;code&gt;xyplot&lt;/code&gt; também tem um método
específico para lidar com objetos da classe Zoo do pacote
&lt;a href=&quot;http://cran.r-project.org/web/packages/zoo/index.html&quot;&gt;zoo&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;No pacote ggplot&lt;/h2&gt;

&lt;p&gt;Finalmente, como fazer no ggplot? Este foi o mais difícil. Demorei
bastante a encontrar referências via google. As mais relevantes que
encontrei foram:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://had.co.nz/ggplot2/scale_date.html&quot;&gt;scale_date&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://learnr.wordpress.com/2009/05/05/ggplot2-two-time-series-with-different-dates/&quot;&gt;ggplot2: two time series with different dates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://goo.gl/Kr5wP&quot;&gt;Using ggplot, how to have the x-axis of time series plots set up automatically?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;O último link é a dica de como transformar um objeto &lt;code&gt;ts&lt;/code&gt; em um
data.frame:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;datas &amp;lt;- seq(as.Date(paste(c(start(dados),1), collapse=&quot;/&quot;)), 
             by = &quot;month&quot;, length.out = length(dados))
dados.df &amp;lt;- data.frame(date = datas, value = dados)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;E finalmente consegui plotar uma série das três com:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ggplot(data=dados.df) + geom_line(aes(date, value.Series.1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-ggplot-single.png&quot; title=&quot;plot ggplot&quot; alt=&quot;plot-ggplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para conseguir em um único gráfico as três séries temporais, tive mais
trabalho. Primeiro em transformar os dados em um data.frame que
pudesse ser entendido pelo ggplot.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; tmp &amp;lt;- stack(dados.df, select = -1)
&amp;gt; tmp$date &amp;lt;- dados.df[,1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;E finalmente, o novo plot:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ggplot(data=dados.df) + geom_line(aes(date, value.Series.1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-ggplot-multi.png&quot; title=&quot;plot ggplot&quot; alt=&quot;plot-ggplot-multi&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Não existe pacote melhor ou pior, cada um é mais adequado para cada
situação. No particular problema de plotar multiplas séries temporais,
sem nenhuma dúvida, preferi a facilidade do lattice em lidar com
objetos &lt;code&gt;ts&lt;/code&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Verifying the ISSN's check digit in Common Lisp</title>
   <link href="http://arademaker.github.com/blog/2011/09/13/checking-issn-lisp.html"/>
   <updated>2011-09-13T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2011/09/13/checking-issn-lisp</id>
   <content type="html">&lt;p&gt;The code below is my first approach to create a lisp function that
test the ISSN &lt;a href=&quot;http://en.wikipedia.org/wiki/Check_digit&quot;&gt;check
digit&lt;/a&gt;. Unfortunately, the
code runs only in Allegro CL due the requirement of regexp2
library. Nevertheless, the regexp2 library is easly replaced by an opensource
regexp library, which makes this not a real constraint.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/1215526.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;That is it! Comments are welcome!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Portal Semântico do CPDOC</title>
   <link href="http://arademaker.github.com/publications/2011/05/06/portal-semantico-cpdoc.html"/>
   <updated>2011-05-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2011/05/06/portal-semantico-cpdoc</id>
   <content type="html">&lt;h1&gt;Portal Semântico do CPDOC&lt;/h1&gt;

&lt;p&gt;Este artigo descreve o projeto de criação do portal semântico do CPDOC
– FGV, juntamente a todas as iniciativas que estão sendo engendradas
para que este seja possível. Dentre estas, destacam-se a criação de
ontologias de domínio para história contemporânea e descrição de
acervos, para o adequado provisionamento de metadados para os
documentos pertencentes aos acervos em questão.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Construindo tabelas verdade no R</title>
   <link href="http://arademaker.github.com/blog/2011/03/02/tabela-verdade-no-R.html"/>
   <updated>2011-03-02T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2011/03/02/tabela-verdade-no-R</id>
   <content type="html">&lt;p&gt;Durante a preparação de alguns exercícios de lógica, me deparei com a
necessidade de construir tabelas verdade. Lembrando do pacote xtable
do R, pensei como seria construir uma tabela verdade usando o R. Minha
solução em R está no github, gist abaixo, com o exemplo de como seria
a tabela da expressão&lt;/p&gt;

&lt;p&gt;$$\neg (a \lor b) \lor c$$&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/852194.js&quot;&gt; &lt;/script&gt;


&lt;p&gt;Observem que a saída do primeiro comando xtable é bastante bizarra,
certamente um bug do pacote xtable. Editei a saída mantendo apenas o
início da tabela gerada e incluíndo &quot;...&quot; no final.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;xtable&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;tt&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;




&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;latex&quot;&gt;&lt;span class=&quot;c&quot;&gt;% latex table generated in R 2.11.1 by xtable 1.5-6 package&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Wed Mar  2 23:18:40 2011&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;table&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;[ht]
&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;center&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;tabular&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;rllll&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
 &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; a &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; b &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; c &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; ! (a &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&lt;/span&gt; b) &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$&lt;/span&gt; c &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
1 &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
2 &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; c(FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE) &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; 
...
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;tabular&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;center&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;table&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Para fazer com que cada célula da tabela tivesse apenas o valor lógico
correspondente, não um vetor, converti o data.frame em caracteres
antes de usar o xtable. Minha solução original convertia em números,
Bruno Lopes me lembrou de converter em caracteres diretamente.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;latex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;table&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;[ht]
&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;center&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;tabular&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;llll&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
a &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; b &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; c &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\~&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{}&lt;/span&gt; (a OR b) OR c &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;nb&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
   &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;tabular&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;center&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;table&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



</content>
 </entry>
 
 <entry>
   <title>Using Intuitionistic Logic as a Basis for Legal Ontologies</title>
   <link href="http://arademaker.github.com/publications/2011/01/01/using-intuitionistic-logic-basis-legal-ontologies.html"/>
   <updated>2011-01-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2011/01/01/using-intuitionistic-logic-basis-legal-ontologies</id>
   <content type="html">&lt;h1&gt;Using Intuitionistic Logic as a Basis for Legal Ontologies&lt;/h1&gt;

&lt;p&gt;Classical Description Logic has been widely used as a basis for
ontology creation and reasoning in many knowledge specific
domains. These specific domains naturally include Legal AI. As in any
other domain, consistency is an important issue for legal ontologies.
However, due to its inherently normative feature, coherence
(consistency) in legal ontologies is more subtle than in most other
domains. Negation and subsumption play a central role in ontology
coherence. An adequate intuitionistic semantics for negation in a
legal domain comes to the fore when we take legally valid individual
statements as the inhabitants of our legal ontology. This allows us to
elegantly deal with particular situations of legal coherence, such as
conflict of laws, as those solved by Private International Law
analysis. This paper: (1) Briefly presents our version of
Intuitionistic Description Logic, called IALC for Intuitionistic ALC
(ALC being the canonical classical description logic system)(2)
Discuss the jurisprudence foundation of our system, and (3) Shows how
we can perform a coherence analysis of “Conflict of Laws in Space” by
means of IALC. This paper reports work-in-progress on using this
alternative definition of logical negation for building and testing
legal ontologies and reasoning in AI.&lt;/p&gt;

&lt;p&gt;Keywords: Description logic, intuitionistic Logic, legal ontologies, constructive negation&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Intuitionistic Logic and Legal Ontologies</title>
   <link href="http://arademaker.github.com/publications/2010/12/16/intuitionistic-logic-legal-ontologies.html"/>
   <updated>2010-12-16T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2010/12/16/intuitionistic-logic-legal-ontologies</id>
   <content type="html">&lt;h1&gt;Intuitionistic Logic and Legal Ontologies&lt;/h1&gt;

&lt;p&gt;Classical Description Logic has been widely used as a basis for
ontology creation and reasoning in many knowledge specific
domains. These specific domains naturally include Legal AI. As in any
other domain, consistency is an important issue for legal ontologies.
However, due to its inherently normative feature, coherence
(consistency) in legal ontologies is more subtle than in most other
domains. Negation and subsumption play a central role in ontology
coherence. An adequate intuitionistic semantics for negation in a
legal domain comes to the fore when we take legally valid individual
statements as the inhabitants of our legal ontology.&lt;/p&gt;

&lt;p&gt;Keywords: Description logic, intuitionistic Logic, legal ontologies, constructive negation&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Barras e Linhas no R</title>
   <link href="http://arademaker.github.com/blog/2010/11/18/barras-e-linhas.html"/>
   <updated>2010-11-18T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2010/11/18/barras-e-linhas</id>
   <content type="html">&lt;p&gt;Ontem um aluno me perguntou como produzir um gráfico de barras com
linhas. Minha primeira idéia foi recorrer a pacotes como
&lt;a href=&quot;http://cran.r-project.org/web/packages/lattice/&quot;&gt;Lattice&lt;/a&gt; ou
&lt;a href=&quot;http://had.co.nz/ggplot2/&quot;&gt;ggplot2&lt;/a&gt;, imaginando tratar-se de um
gráfico pouco usual. Depois de um pouco de pesquisa, acabei
descobrindo que o gráfico em questão não é tão usual assim e pode ser
facilmente produzido com os comandos básicos de gráficos do R.&lt;/p&gt;

&lt;p&gt;Digamos que seus dados sejam uma data.frame composto por duas
variáveis.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;dados &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; data.frame&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;a &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; sample&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; b &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; sample&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;O que desejamos é representar no gráfico os valores da variável &lt;code&gt;a&lt;/code&gt;
como barras e os valores da variável &lt;code&gt;b&lt;/code&gt; como pontos conectados por
linhas. O comando abaixo produz o gráfico de barras e retorna um
vetor com as coordenadas &lt;code&gt;x&lt;/code&gt; dos meios das barras produzidas.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;a &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; barplot&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;dados&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;a&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; ylim &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Agora é fácil criar os pontos e linhas:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;points&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; dados&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;b&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
lines&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; dados&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;b&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; ylim&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; c&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Image que seus dados são temporais, onde cada observação está
relacionada a uma ano. Pode-se incluir os anos como rótulos do eixo
&quot;x&quot; com o comando:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;axis&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; at &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; a&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; labels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2009&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;img src=&quot;/images/2010-11-18-fig.png&quot; title=&quot;bar and line&quot; alt=&quot;bar-and-line&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Providing a Proof-Theoretical Basis for Explanation. A Case Study on UML and ALCQI Reasoning</title>
   <link href="http://arademaker.github.com/publications/2010/11/01/providing-proof-theoretical-basis-explanation.html"/>
   <updated>2010-11-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2010/11/01/providing-proof-theoretical-basis-explanation</id>
   <content type="html">&lt;h1&gt;Providing a Proof-Theoretical Basis for Explanation. A Case Study on UML and ALCQI Reasoning&lt;/h1&gt;

&lt;p&gt;In this article we argue in favour of Natural Deduction Systems as a
basis for formal proof explanations. We illustrate our choice
presenting a Natural Deduction for ALCQI and use it to help explain
UML reasoning.&lt;/p&gt;

&lt;p&gt;Key Words: ALC, Description Logics, UML, ALCQI, Proof Theory, Sequent
Calcu- lus, Natural Deduction&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Constructive Description Logics Hybrid-Style</title>
   <link href="http://arademaker.github.com/publications/2010/07/10/constructive-description-logics-hybrid-style.html"/>
   <updated>2010-07-10T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2010/07/10/constructive-description-logics-hybrid-style</id>
   <content type="html">&lt;h1&gt;Constructive Description Logics Hybrid-Style&lt;/h1&gt;

&lt;p&gt;Constructive modal logics come in several different flavours and
constructive description logics, not surprisingly, do the same. We
introduce an intuitionistic description logic, which we call iALC (for
intuitionistic ALC, since ALC is the name of the canonical description
logic system) and provide axioms, a Natural Deduction formulation and
a sequent calculus for it. The system iALC is related to Simpsonʼs
constructive modal logic IK the same way Mendler and Scheeleʼs cALC is
related to constructive CK and in the same way classical multimodal K
is related to ALC. In the system iALC, as well as in cALC, the
classical principles of the excluded middle C⊔¬C=T, double negation
¬¬C=C and the definitions of the modalities ∃R.C=¬∀R.¬C and
∀R.C=¬∃R.¬C are no longer validities, but simply non-trivial TBox
statements used to axiomatize specific application
scenarios. Meanwhile in iALC, like in classical ALC, we have that the
distribution of existential roles over disjunction
i.e. ∃R.(C⊔D)=∃R.C⊔∃R.D and (the nullary case) ∃R.⊥=⊥ hold, which is
not true for cALC. We intend to use iALC for modelling juridical
Artificial Intelligence (AI) systems and we describe briefly how.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Using Intuitionistic Logic as a basis for Legal Ontologies</title>
   <link href="http://arademaker.github.com/publications/2010/07/07/using-intuitionistic-logic-basis-legal-ontologies.html"/>
   <updated>2010-07-07T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2010/07/07/using-intuitionistic-logic-basis-legal-ontologies</id>
   <content type="html">&lt;h1&gt;Using Intuitionistic Logic as a basis for Legal Ontologies&lt;/h1&gt;

&lt;p&gt;Classical Description Logic has been widely used as a basis for
ontology creation and reasoning in many knowledge specific
domains. These specific domains naturally include Legal AI. As in any
other domain, consistency is an important issue for legal ontologies.
However, due to its inherently normative feature, coherence
(consistency) in legal ontologies is more subtle than in most other
domains. Negation and subsumption play a central role in ontology
coherence. An adequate intuitionistic semantics for negation in a
legal domain comes to the fore when we take legally valid individual
statements as the inhabitants of our legal ontology. This allows us to
elegantly deal with particular situations of legal coherence, such as
conflict of laws, as those solved by Private International Law
analysis. This paper: (1) Briefly presents our version of
Intuitionistic Description Logic, called IALC for Intuitionistic ALC
(ALC being the canonical classical description logic system)(2)
Discuss the jurisprudence foundation of our system, and (3) Shows how
we can perform a coherence analysis of “Conflict of Laws in Space” by
means of IALC. This paper reports work-in-progress on using this
alternative definition of logical negation for building and testing
legal ontologies and reasoning in AI.&lt;/p&gt;

&lt;p&gt;Keywords: Description logic, intuitionistic Logic, legal ontologies, constructive negation&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Datasets no Brasil</title>
   <link href="http://arademaker.github.com/blog/2010/01/30/datasets.html"/>
   <updated>2010-01-30T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2010/01/30/datasets</id>
   <content type="html">&lt;p&gt;Est&amp;atilde;o come&amp;ccedil;ando a surgir no Brasil iniciativas para real
transpar&amp;ecirc;ncia no acesso aos dados do governo. A
estrutura&amp;ccedil;&amp;atilde;o dos dados em formatos abertos (RDF, CSV
etc), acompanhados de metadados e indexados em interfaces de busca e
navega&amp;ccedil;&amp;atilde;o que facilitem o download dos arquivos
s&amp;atilde;o para mim as condi&amp;ccedil;&amp;otilde;es necess&amp;aacute;rias para
o livre acesso &amp;agrave; informa&amp;ccedil;&amp;atilde;o. Uma destas
iniciativas &amp;eacute; o &lt;a href=&quot;http://www.lexml.gov.br/&quot;&gt;LeXML&lt;/a&gt;.  Nos EUA
existem os projetos &lt;a href=&quot;http://data.gov/&quot;&gt;data.gov&lt;/a&gt; e
&lt;a href=&quot;http://datasf.org/&quot;&gt;datasf.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Na linha oposta, est&amp;atilde;o servi&amp;ccedil;os como o da ANP de
levantamento de &lt;a href=&quot;http://www.anp.gov.br/preco/&quot;&gt;pre&amp;ccedil;os de
combust&amp;iacute;veis&lt;/a&gt;. Que tipo de
pesquisa pode ser feita com estes dados? Para come&amp;ccedil;ar,
s&amp;oacute; a constru&amp;ccedil;&amp;atilde;o de um datasets a partir deste
site demanda um bom trabalho de desenvolvimento de um crawler e
transformadores. Afinal, eu me pergunto, qual &amp;eacute; o objetivo
deste site da ANP? Se for para um cidad&amp;atilde;o comum pesquisar qual
o melhor posto para abastecer seu carro, a interface deixa a
desejar. Se for para a sociedade acompanhar os pre&amp;ccedil;os de
combust&amp;iacute;veis no Brasil, isto implica acessibilidade as
s&amp;eacute;ries de dados, e todos meus coment&amp;aacute;rios anteriores
fazem sentido, n&amp;atilde;o?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Using Intuitionistic Logic as a Basis for Legal Ontologies</title>
   <link href="http://arademaker.github.com/publications/2010/01/01/using-intuitionistic-logic-basis-legal-ontologies.html"/>
   <updated>2010-01-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2010/01/01/using-intuitionistic-logic-basis-legal-ontologies</id>
   <content type="html">&lt;h1&gt;Using Intuitionistic Logic as a Basis for Legal Ontologies&lt;/h1&gt;

&lt;p&gt;Classical Description Logic has been widely used as a basis for
ontology creation and reasoning in many knowledge specific
domains. These specific domains naturally include Legal AI. As in any
other domain, consistency is an important issue for legal ontologies.
However, due to its inherently normative feature, coherence
(consistency) in legal ontologies is more subtle than in most other
domains. Negation and subsumption play a central role in ontology
coherence. An adequate intuitionistic semantics for negation in a
legal domain comes to the fore when we take legally valid individual
statements as the inhabitants of our legal ontology. This allows us to
elegantly deal with particular situations of legal coherence, such as
conflict of laws, as those solved by Private International Law
analysis. This paper: (1) Briefly presents our version of
Intuitionistic Description Logic, called IALC for Intuitionistic ALC
(ALC being the canonical classical description logic system)(2)
Discuss the jurisprudence foundation of our system, and (3) Shows how
we can perform a coherence analysis of “Conflict of Laws in Space” by
means of IALC. This paper reports work-in-progress on using this
alternative definition of logical negation for building and testing
legal ontologies and reasoning in AI.&lt;/p&gt;

&lt;p&gt;Keywords: Description logic, intuitionistic Logic, legal ontologies, constructive negation&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Is it important to explain a theorem? A case study on UML and ALCQI</title>
   <link href="http://arademaker.github.com/publications/2009/11/09/important-explain-theorem-case-study-uml-alcqi.html"/>
   <updated>2009-11-09T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2009/11/09/important-explain-theorem-case-study-uml-alcqi</id>
   <content type="html">&lt;h1&gt;Is it important to explain a theorem? A case study on UML and ALCQI&lt;/h1&gt;

&lt;p&gt;Description Logics (DL) are quite well-established as underlying
logics for KR. is a basic description logic. ER and UML are among the
most used semiformal artifacts in computer science. The DL-community
has shown that one needs to go a bit further to reason on ER and UML
models. is able to express most of the features involved in an ER and
UML modeling. DL-Lite would also be taken for doing this, although it
might be more verbose.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On the Proof Theory of ALC</title>
   <link href="http://arademaker.github.com/publications/2009/10/12/proof-theory-alc.html"/>
   <updated>2009-10-12T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2009/10/12/proof-theory-alc</id>
   <content type="html">&lt;h1&gt;On the Proof Theory of ALC&lt;/h1&gt;

&lt;p&gt;Description Logics is a family of formalisms used to represent
knowledge of a domain. In con- trast with others knowledge
representation systems, Description Logics are equipped with a formal,
logic-based semantics. Knowledge representation systems based on
description logics provide various inference capabilities that deduce
implicit knowledge from the explicitly represented knowledge.  In &quot;A
sequent calculus for alc&quot; we present a sequent calculus for ALC , a
basic Description Logic, called SALC . The first motivation for
developing such system is the extraction of computational content of
ALC proofs. The present calculus is an intermediate step towards a
Natural Deduction System for ALC . In this paper we present the proof
of cut elimination for SALC .&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Designing ubiquitous applications, proposal of a specification environment</title>
   <link href="http://arademaker.github.com/publications/2009/06/15/designing-ubiquitous-applications.html"/>
   <updated>2009-06-15T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2009/06/15/designing-ubiquitous-applications</id>
   <content type="html">&lt;h1&gt;Designing ubiquitous applications, proposal of a specification environment&lt;/h1&gt;

&lt;p&gt;In ubiquitous applications, where the meaning of an entity, such as a
user or service, depends on environment-specific constraints and
dynamic changes in the environment have to be considered in all stages
of development, the separation between the system&#39;s behaviour and its
context representation (a.k.a. context model) is essential for
facilitating the development of such inherently complex systems. At
the same time, because of its well-known benefits, a formal
specifications should be considered not only for describing the
system&#39;s behaviour, but also the corresponding context
model. Considering this, we propose in this paper an environment to
support context modelling through formal specification. For this sake,
we adopt the algebra of contextualized entities proposed and define
levels of abstractions over its diagrams, enabling a stepwise
construction of modular specifications. The overall goal is to reduce
the gap between the formal description of an ubiquitous application
and its implementation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Is it important to explain a theorem? A case study on UML and ALCQI</title>
   <link href="http://arademaker.github.com/publications/2009/01/01/important-explain-theorem-case-study-uml-alcqi.html"/>
   <updated>2009-01-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2009/01/01/important-explain-theorem-case-study-uml-alcqi</id>
   <content type="html">&lt;h1&gt;Is it important to explain a theorem? A case study on UML and ALCQI&lt;/h1&gt;

&lt;p&gt;Description Logics (DL) are quite well-established as underlying
logics for KR. is a basic description logic. ER and UML are among the
most used semiformal artifacts in computer science. The DL-community
has shown that one needs to go a bit further to reason on ER and UML
models. is able to express most of the features involved in an ER and
UML modeling. DL-Lite would also be taken for doing this, although it
might be more verbose.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Toward Short and Structural ALC-Reasoning Explanations, A Sequent Calculus Approach</title>
   <link href="http://arademaker.github.com/publications/2008/10/26/toward-short-structural-alc-reasoning-explanations.html"/>
   <updated>2008-10-26T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2008/10/26/toward-short-structural-alc-reasoning-explanations</id>
   <content type="html">&lt;h1&gt;Toward Short and Structural ALC-Reasoning Explanations, A Sequent Calculus Approach&lt;/h1&gt;

&lt;p&gt;Description Logics (DL) are quite well-established as underlying
logics for KR. is a basic description logic. ER and UML are among the
most used semiformal artifacts in computer science. The DL-community
has shown that one needs to go a bit further to reason on ER and UML
models. is able to express most of the features involved in an ER and
UML modeling. DL-Lite would also be taken for doing this, although it
might be more verbose.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Formal Framework for Modeling Context-Aware Behavior in Ubiquitous Computing</title>
   <link href="http://arademaker.github.com/publications/2008/10/13/formal-framework-modeling-context-aware-behavior-ubiquitous-computing.html"/>
   <updated>2008-10-13T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2008/10/13/formal-framework-modeling-context-aware-behavior-ubiquitous-computing</id>
   <content type="html">&lt;h1&gt;A Formal Framework for Modeling Context-Aware Behavior in Ubiquitous Computing&lt;/h1&gt;

&lt;p&gt;A formal framework to contextualize ontologies, proposed in [3],
provides several ways of composing ontologies, contexts or both. The
proposed algebra can be used to model applications in which the
meaning of an entity depends on environment constraints or where
dynamic changes in the environment have to be considered. In this
article we use this algebra to formalize the problem of interpreting
context information in ubiquitous systems, based on a concrete
scenario. The main goal is to verify, on one hand, how the formal
approach can contribute with a better understanding of the fundamental
concepts of ubiquitous computing and, on the other hand, if this
formal framework is flexible and rich enough to adequately express
specific characteristics of the concrete application domain and
scenario.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On The Proof Theory of ALC</title>
   <link href="http://arademaker.github.com/publications/2008/05/11/proof-theory-alc.html"/>
   <updated>2008-05-11T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2008/05/11/proof-theory-alc</id>
   <content type="html">&lt;h1&gt;On The Proof Theory of ALC&lt;/h1&gt;

&lt;p&gt;Description Logics is a family of formalisms used to represent
knowledge of a domain. In con- trast with others knowledge
representation systems, Description Logics are equipped with a formal,
logic-based semantics. Knowledge representation systems based on
description logics provide various inference capabilities that deduce
implicit knowledge from the explicitly represented knowledge.  In &quot;A
sequent calculus for alc&quot; we present a sequent calculus for ALC , a
basic Description Logic, called SALC . The first motivation for
developing such system is the extraction of computational content of
ALC proofs. The present calculus is an intermediate step towards a
Natural Deduction System for ALC . In this paper we present the proof
of cut elimination for SALC .&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ontology and Context</title>
   <link href="http://arademaker.github.com/publications/2008/03/17/ontology-context.html"/>
   <updated>2008-03-17T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2008/03/17/ontology-context</id>
   <content type="html">&lt;h1&gt;Ontology and Context&lt;/h1&gt;

&lt;p&gt;This paper considers a formal framework to contextualize ontologies
and presents a formal algebra to manipulate these entities providing
several ways of composing ontologies, contexts or both. The algebra
gives the flexibility that is required to model applications where the
meaning of an entity depends on environment constraints or where
dynamic changes in the environment must be considered.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On The Proof Theory of ALC</title>
   <link href="http://arademaker.github.com/publications/2008/01/15/proof-theory-alc.html"/>
   <updated>2008-01-15T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2008/01/15/proof-theory-alc</id>
   <content type="html">&lt;h1&gt;On The Proof Theory of ALC&lt;/h1&gt;

&lt;p&gt;Description Logics is a family of formalisms used to represent
knowledge of a domain. In con- trast with others knowledge
representation systems, Description Logics are equipped with a formal,
logic-based semantics. Knowledge representation systems based on
description logics provide various inference capabilities that deduce
implicit knowledge from the explicitly represented knowledge.  In &quot;A
sequent calculus for alc&quot; we present a sequent calculus for ALC , a
basic Description Logic, called SALC . The first motivation for
developing such system is the extraction of computational content of
ALC proofs. The present calculus is an intermediate step towards a
Natural Deduction System for ALC . In this paper we present the proof
of cut elimination for SALC .&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dealing with the Formal Analysis of Information Security Policies through Ontologies. A Case Study</title>
   <link href="http://arademaker.github.com/publications/2007/12/02/dealing-formal-analysis-information-security-policies-through-ontologies.html"/>
   <updated>2007-12-02T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2007/12/02/dealing-formal-analysis-information-security-policies-through-ontologies</id>
   <content type="html">&lt;h1&gt;Dealing with the Formal Analysis of Information Security Policies through Ontologies. A Case Study&lt;/h1&gt;

&lt;p&gt;We present the structure of an ontology for Information Security (IS),
applied to the extraction of knowledge from Natural Language texts (IS
standards, security policies and security control descriptions). This
ontology is com- posed of the vocabulary for the IS Domain, and a
partic- ular kind of ontology description, logical forms to deter-
mine the structure of the DL formulas associated with the texts. We
also discuss the relationship between the struc- ture of the formulas
and the efficiency of the reasoner.&lt;/p&gt;

&lt;p&gt;Keywords: Information Security Policies, Description Logic, Ontologies&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>An Ontology-based Approach to the Formalization of Information Security Policies</title>
   <link href="http://arademaker.github.com/publications/2006/10/16/ontology-based-approach-formalization-information-security-policies.html"/>
   <updated>2006-10-16T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/publications/2006/10/16/ontology-based-approach-formalization-information-security-policies</id>
   <content type="html">&lt;h1&gt;An Ontology-based Approach to the Formalization of Information Security Policies&lt;/h1&gt;

&lt;p&gt;We present the structure of an ontology for Information Security (IS)
and discuss a paradigm whereby it can be used to extract knowledge
from natural language texts such as IS standards, security policies
and security control descriptions. Besides providing a vocabulary for
the IS domain, the proposed ontology stores logical forms
corresponding to statements in the text, as well as a set of axioms
used for inference in description logic (DL). We also describe a tool
to provide automatic support for the formalization process.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>A Rewriting Semantics for a Software Architecture Description Language</title>
   <link href="http://arademaker.github.com/publications/2004/11/23/rewriting-semantics-software-architecture-description-language.html"/>
   <updated>2004-11-23T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/publications/2004/11/23/rewriting-semantics-software-architecture-description-language</id>
   <content type="html">&lt;h1&gt;A Rewriting Semantics for a Software Architecture Description Language&lt;/h1&gt;

&lt;p&gt;Distributed and concurrent application invariably have coordination
requirements. The design of those applications, composed by several
(possibly distributed) components, has to consider coordination
requirements comprising inter-component interaction styles, and
intra-component concurrency and synchronization aspects. In our
approach coordination aspects are treated in the software architecture
level and can be specified in high-level contracts in CBabel ADL. A
rewriting logic semantics for the software architecture description
language CBabel is given, revisiting and extending previous work by
some of the authors, which now includes a revision of the previous
semantics and the addition of new features covering all the
language. The CBabel tool is also presented. The CBabel tool is a
prototype executable environment for CBabel, that implements the given
CBabel&#39;s rewriting logic semantics and allows the execution and
verification of CBabel descriptions in the Maude system, an
implementation of rewriting logic. In this way, software architectures
describing complex applications can be formally verified regarding
properties such as deadlock and synchronization consistency in the
software architecture design phase of its life cycle.&lt;/p&gt;

&lt;p&gt;Keywords: software architecture description languages; rewriting
logic; Maude; contracts&lt;/p&gt;
</content>
 </entry>
 
 
</feed>
