<?xml version="1.0" encoding="utf-8" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Alexandre Rademaker</title>
 <link href="http://arademaker.github.com/atom.xml" rel="self"/>
 <link href="http://arademaker.github.com/"/>
 <updated>2021-12-03T12:45:05-03:00</updated>
 <id>http://arademaker.github.com/</id>
 <author>
   <name>Alexandre Rademaker</name>
   <email>arademaker@gmail.com</email>
 </author>

 
 <entry>
   <title>Idéias de projetos</title>
   <link href="http://arademaker.github.com/blog/2021/04/19/projetos.html"/>
   <updated>2021-04-19T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2021/04/19/projetos</id>
   <content type="html">&lt;p&gt;Segue uma relação atualizada de idéias de projetos para alunos.&lt;/p&gt;
&lt;h2&gt;implicação textual&lt;/h2&gt;
&lt;p&gt;O problema de detecção de implicação textual (text entailment, TE) é
  identificar quando duas sentenças (ou fragmentos de texto) estão
  relacionados de tal forma que a verdade de um fragmento de texto segue
  da verdade do outro. Dizemos que existe uma consequência lógica entre
  h e t, se de um texto denominado h podemos concluir um texto
  t. Determinar se essa relação se mantém é uma tarefa relacionada à
  semântica formal de sentenças; adicionalmente, a implicação textual
  inclui a semântica lexical. Muitas aplicações de processamento de
  linguagem natural, como resposta a perguntas e extração de informações
  de textos, podem se beneficiar de algorítmos de conhecimento de
  implicação textual. O objetivo do projeto é investigar as técnicas de
  conhecimento de implicação textual existentes.&lt;/p&gt;
&lt;h2&gt;A OpenWordnet-PT&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&quot;http://wnpt.brlcloud.com/wn/&quot;&gt;OpenWordnet-PT&lt;/a&gt;, ou simplesmente OWN-PT) é a WordNet de acesso aberto
  para o português. A OpenWN-PT está disponível em RDF/OWL e vem sendo
  expandida, melhorada e utilizada em projetos de processamento de
  linguagem nos últimos 10 anos.&lt;/p&gt;
&lt;p&gt;Este projeto visa ajudar nossa equipe na melhoria do &lt;a href=&quot;https://github.com/own-pt/cl-wnbrowser&quot;&gt;web interface&lt;/a&gt;
  para nosso openWordnet-PT.&lt;/p&gt;
&lt;p&gt;Em particular, precisamos: (1) simplificar a arquitetura; (2) melhorar
  a interface para votos e sugestões; (3) melhorar a navegação e
  Visualização de dados.&lt;/p&gt;
&lt;h2&gt;CoNLL-U Library&lt;/h2&gt;
&lt;p&gt;A biblioteca &lt;a href=&quot;https://hackage.haskell.org/package/hs-conllu&quot;&gt;hs-conllu&lt;/a&gt; desenvolvida pelo Bruno Cuconato foi proposta
  como uma evolução da biblioteca Common Lisp &lt;a href=&quot;https://github.com/own-pt/cl-conllu&quot;&gt;cl-conllu&lt;/a&gt; desenvolvida
  inicialmente por mim em colaboração com vários alunos. A biblitoeca
  haskell ainda é bastante básica e faltam recursos diversos presentes
  em outras ferramentas para manipulação deste formato de arquivo usado
  pelo projeto &lt;a href=&quot;https://universaldependencies.org/format.html&quot;&gt;Universal Dependencies&lt;/a&gt;. O projeto seria implementar as
  diversas funcionalidades presentes na biblioteca Lisp ainda não
  portadas para haskell, implementar novas funcionalidade como, por
  exemplo, a leitura do formato conllup que será cada vez mais usado
  pela comunidade.&lt;/p&gt;
&lt;h2&gt;Delph-In suporte em Haskell&lt;/h2&gt;
&lt;p&gt;O Consórcio DELPH-IN é uma colaboração entre lingüistas computacionais
  de locais de pesquisa em todo o mundo que trabalham no processamento
  lingüístico &amp;#8220;profundo&amp;#8221; da linguagem humana. O objetivo é a combinação
  de métodos linguísticos e estatísticos de processamento para chegar ao
  significado de textos e enunciados. Os parceiros adotaram o
  Head-Driven Phrase Structure Grammar (HPSG) e a Minimal Recursion
  Semantics (MRS), dois modelos avançados de análise linguística
  formal. Eles também se comprometeram com um formato compartilhado para
  a representação gramatical e com um esquema rígido de avaliação, bem
  como com o uso geral do licenciamento de código aberto e da
  transparência. Neste contexto, queremos contribuir com ferramentas
  para este consórcio, em particular:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;uma implementação haskell alternativa a atual
    &lt;a href=&quot;https://pydelphin.readthedocs.io/&quot;&gt;https://pydelphin.readthedocs.io/&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Implementação da linguagem de query e interface para buscas em
    representações semânticas.&lt;/li&gt;
  &lt;li&gt;Outras aplicações para HPSG e MRS.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;SUO-KIF translator to TPTP&lt;/h2&gt;
&lt;p&gt;Em projetos anteriores, escrevemos uma &lt;a href=&quot;https://github.com/own-pt/cl-krr&quot;&gt;tradução&lt;/a&gt; do formato &lt;a href=&quot;http://www.adampease.org/OP/&quot;&gt;SUO-KIF&lt;/a&gt;
  para o formato &lt;a href=&quot;http://www.cs.miami.edu/~tptp/&quot;&gt;TPTP&lt;/a&gt;. Esta tradução permite usarmos a ontologia &lt;a href=&quot;http://www.ontologyportal.org&quot;&gt;SUMO&lt;/a&gt; em
  provadores automáticos de teoremas. Neste projeto, gostariamos de
  expandir esta transformação resolvendo bugs presentes na transformação
  atual e expandindo seu suporte para o formato TF0/TPTP. Como etapa
  seguinte, gostaríamos de usar SUMO em projetos de processamento de
  linguagem e contribuir com SUMO. A transformação inicial foi escrita
  em Lisp, mas esperasse a migração do código para Haskell. Outra
  direção possível de pesquisa é o reuso e possível reescrita do
  provador &lt;a href=&quot;http://www.ai.sri.com/~stickel/snark.html&quot;&gt;SNARK&lt;/a&gt; e seu suporte a procedural Attachments.&lt;/p&gt;
&lt;h2&gt;Formalizing ALC SC/ND em Lean&lt;/h2&gt;
&lt;p&gt;Neste projeto, gostariamos de concluir a formalização dos sistemas
  dedutivos desenvolvidos para algumas lógicas de descrição em &lt;a href=&quot;http://arademaker.github.io/bibliography/phdthesis-4.html&quot;&gt;tese&lt;/a&gt; e
  provar as propriedades básicas destes sistemas. Código atual em
  &lt;a href=&quot;https://github.com/arademaker/alc-lean&quot;&gt;https://github.com/arademaker/alc-lean&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Tutorial sobre gramáticas computacionais no formalismo HPSG utilizando a Grammar Matrix</title>
   <link href="http://arademaker.github.com/blog/2021/04/05/grammar-matrix.html"/>
   <updated>2021-04-05T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2021/04/05/grammar-matrix</id>
   <content type="html">&lt;h2&gt;Pré-Inscrição&lt;/h2&gt;
&lt;p&gt;Os interessados devem se inscrever no formulário&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://forms.gle/8EGJNa8oWjbLjqe47&quot;&gt;https://forms.gle/8EGJNa8oWjbLjqe47&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Inscrição Zoom&lt;/h2&gt;
&lt;p&gt;Os interessados devem ainda se inscrever no Zoom, ambiente que será
  usado para o curso:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://fgv-br.zoom.us/meeting/register/tJMrcu-hpjojGN2fy_gru7zDcVIp5xkIAXo4&quot;&gt;https://fgv-br.zoom.us/meeting/register/tJMrcu-hpjojGN2fy_gru7zDcVIp5xkIAXo4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Depois do registro, um email com a confirmação e link para os
  encontros será enviado para seu email.&lt;/p&gt;
&lt;h2&gt;Resumo&lt;/h2&gt;
&lt;p&gt;Gramáticas computacionais elaboradas manualmente com base em
  princípios linguísticos têm tido a sua eficácia comprovada em diversas
  aplicações de nível industrial que exigem compreensão textual, em
  tarefas como tradução automática, resolução de perguntas e extração de
  informações. Esse tipo de gramática é um complemento às abordagens
  estatísticas baseadas em corpora sintaticamente anotados, os chamados
  treebanks. A anotação de um corpus por meio de um gramática
  computacional assegura a profundidade e a consistência das análises,
  permitindo que o conhecimento de especialistas seja automaticamente
  aplicado na anotação de um grande volume de sentenças. Uma das teorias
  gramaticais formais mais utilizadas para a elaboração de gramáticas
  desse tipo é a HPSG. As principais gramáticas de ampla cobertura
  implementadas nesse formalismo são a English Resource Grammar (ERG), a
  JACY do japonês e a gramática alemã do DFKI (Centro de Pesquisa Alemão
  de Inteligência Artificial), resultado de um esforço de mais de uma
  década de indivíduos ou pequenos grupos. A modelagem computacional dos
  fenômenos gramaticais de uma língua nesse formalismo pressupõe o
  domínio da linguagem de descrição TDL (Type Description Language),
  constituindo uma tarefa de programação complexa, objeto da engenharia
  da gramática. A Grammar Matrix, que vem sendo desenvolvida desde os
  anos 2000 na University of Washington por Emily M. Bender e colegas,
  possibilita a reutilização de soluções de implementação das gramáticas
  referidas para a construção de novas gramáticas, dispensando
  conhecimento da linguagem TDL. O sistema possui uma interface sob a
  forma de um questionário baseado em extensa pesquisa tipológica, que
  cobre alguns dos principais fenômenos gramaticais das línguas do
  mundo. Para construção de uma gramática computacional de uma
  determinada língua, o usuário só precisa especificar as
  particularidades da língua em relação a uma série de parâmetros
  gramaticais, como ordem de palavras, tipos de categorias
  morfossintáticas etc. bem como descrever as propriedades dos itens
  lexicais. Essa gramática inicial pode ser ampliada depois
  manualmente. Neste tutorial, apresentamos os conceitos linguísticos
  fundamentais necessários à compreensão e utilização do questionário
  bem como noções mínimas da teoria da HPSG. As noções serão
  exemplificadas por meio da construção de minigramáticas do inglês e do
  latim, línguas que diferem estruturalmente de modo bastante
  significativo. O tutorial será concluído com a apresentação de
  aplicações e ferramentas para utilização da ERG. Para acompanhamento
  do tutorial, recomendamos a prévia instalação do parser LKB-Fos
  integrado ao editor Emacs e aquisição de uma familiaridade mínima com
  os dois sistemas, embora isso não seja estritamente necessário.&lt;/p&gt;
&lt;h2&gt;Instrutores&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Leonel Figueiredo de Alencar — Professor Titular da Universidade
    Federal do Ceará e Professor Visitante da Escola de Matemática
    Aplicada da Fundação Getúlio Vargas&lt;/li&gt;
  &lt;li&gt;Alexandre Rademaker — Professor da Escola de Matemática Aplicada da
    Fundação Getúlio Vargas e Pesquisador do IBM Research&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Programação&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;b&gt;12.04.2021 15:00 – 16:30&lt;/b&gt; (L. F. de Alencar): Conceitos linguísticos
    fundamentais: estrutura de constituintes, teoria X-barra, gramática
    universal, relações gramaticais, categorias morfossintáticas,
    controle, raising etc. Noções elementares de HPSG: estrutura de
    traços tipada, hierarquia de tipos, unificação etc. Minigramática
    English 1.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;19.04.2021 15:00 – 16:30&lt;/b&gt; (L. F. de Alencar): Minigramática
    English 2. Minigramática Latin 1.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;26.04.2021 15:00 – 16:30&lt;/b&gt; (L. F. de Alencar): Minigramática
    English 3. Minigramática Latin 2.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;03.05.2021 15:00 – 16:30&lt;/b&gt; (L. F. de Alencar): Minigramática
    English 4. Minigramática Latin 3. Limitações da Grammar Matrix e
    como contorná-las. Exemplos concretos de modificações manuais do
    código TDL.&lt;/li&gt;
  &lt;li&gt;&lt;b&gt;10.05.2021 15:00 – 16:30&lt;/b&gt; (A. Rademaker): English Resource Grammar,
    aplicações e ferramentas de utilização.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Referências&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;BENDER, Emily; FLICKINGER, Dan M.; OEPEN, Stephan. The Grammar
    Matrix: An Open-Source Starter-Kit for the Rapid Development of
    Cross-Linguistically Consistent Broad-Coverage Precision
    Grammars. CARROLL, John; OOSTDIJK, Nelleke; SUTCLIFFE, Richard
    (Org.). Proceedings of the Workshop on Grammar Engineering and
    Evaluation at the 19th International Conference on Computational
    Linguistics. Taipei, Taiwan, 2002. p. 8-14.&lt;/li&gt;
  &lt;li&gt;BENDER, Emily M.; DRELLISHAK, Scott; FOKKENS, Antske; POULSON,
    Laurie; SALEEM, Safiyyah. Grammar Customization. Research on
    Language and Computation, vol. 8, n. 1, p. 23-72. 2010.&lt;/li&gt;
  &lt;li&gt;COPESTAKE, Ann. Implementing Typed Feature Structure
    Grammars. Stanford: CSLI, 2002.&lt;/li&gt;
  &lt;li&gt;COPESTAKE, Ann. &lt;a href=&quot;https://web.stanford.edu/group/cslipublications/cslipublications/pdf/1575862603usersmanual.pdf&quot;&gt;LKB User Manual&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LinGO Grammar Matrix. &lt;a href=&quot;https://matrix.ling.washington.edu&quot;&gt;https://matrix.ling.washington.edu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;LKB-FOS. &lt;a href=&quot;http://moin.delph-in.net/wiki/LkbFos&quot;&gt;http://moin.delph-in.net/wiki/LkbFos&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;SAG, Ivan A.; WASOW, Thomas; BENDER, Emily. Syntactic theory: a
    formal introduction. 2. ed. Stanford: CSLI Publications, 2003.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Material&lt;/h2&gt;
&lt;p&gt;arquivos disponibilizados em &lt;a href=&quot;https://github.com/LR-POR/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>reading list</title>
   <link href="http://arademaker.github.com/blog/2020/09/23/mapping-resources.html"/>
   <updated>2020-09-23T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2020/09/23/mapping-resources</id>
   <content type="html">&lt;p&gt;https://www.aclweb.org/anthology/P08-2048.pdf&lt;/p&gt;
&lt;p&gt;the authors suggest a basian network to map ERG predicates to propbank
  and verbnet predicates.&lt;/p&gt;
&lt;p&gt;http://www.mt-archive.info/MTS-2005-Flickinger.pdf&lt;/p&gt;
&lt;p&gt;The description of the LOGON transfer system for machine translation
  and the SEM-I (semantic interface) for a grammar.&lt;/p&gt;
&lt;p&gt;https://www.d.umn.edu/~tpederse/Pubs/AAAI04PedersenT.pdf&lt;/p&gt;
&lt;p&gt;WordNet::Similarity is a freely available software package that makes
  it possible to measure the semantic similarity or relatedness between
  a pair of concepts.&lt;/p&gt;
&lt;p&gt;https://www.aclweb.org/anthology/E09-1001.pdf&lt;/p&gt;
&lt;p&gt;Links for articles about RMRS and MRS. Description of the RMRS to DMRS
  conversion. Link to the article about EDS.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Argument labelling is thus quite different from PropBank (Palmer et
    al., 2005) role labelling despite the unfortunate similarity of the
    PropBank naming scheme.&lt;/li&gt;
  &lt;li&gt;In principle, at least, we could (and should) systematically link
    the ERG to FrameNet, but this would be a form of semantic
    enrichment mediated via the SEM-I (cf Roa et al. (2008)), and not
    an alternative technique for argument indexation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;http://www.lrec-conf.org/proceedings/lrec2006/pdf/364_pdf.pdf&lt;/p&gt;
&lt;p&gt;Reference to syntatic `discriminants` Carter 1997. The paper presents
  the MRS to EDS transformation and discussions about the
  discriminant-based treebanking.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Handle constraints of the form hi =q hj (‘equal modulo quantifier
    insertion’) in an MRS express that either the two are equal or that
    hi outscopes hj , i.e. formally that the formula depicted by hj is
    a subformula of the formula depicted by hi.&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Extração de relações familiares do DHBB</title>
   <link href="http://arademaker.github.com/blog/2020/06/06/dhbb.html"/>
   <updated>2020-06-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2020/06/06/dhbb</id>
   <content type="html">&lt;p&gt;Continuando o post anterior, como o leitor deve ter observado, nosso
  objetivo não é fazer perguntas específicas, mas preparar o corpus e
  desenvolver ferramentas para que várias diferentes perguntas possam
  ser respondidas. Mas vamos à uma demanda por extração de informação
  que nos parece óbvia: relações familiares. Imagine tentarmos responder
  questões como o número de famílias que se perpetuam na política
  brasileira nas últimas décadas.&lt;/p&gt;
&lt;p&gt;O que então precisamos responder é o que esperamos de saída ao
  executarmos ferramentas de NLP nos verbetes do DHBB. No que segue,
  assumo que desejamos construir um grafo onde nós são pessoas e arestas
  representem relações familiares entre as pessoas, rotuladas pelas
  relações. A idéia seria construir este grafo a partir dos
  verbetes. Diversas consultas sobre este grafo seriam possíveis e,
  direta ou indiretamente, as respostas serviriam de dados para sua
  reportagem. Alguém consegue pensar em algo mais específico ou mais
  simples?&lt;/p&gt;
&lt;p&gt;Construir o grafo que idealizei acima envolve: 1) identificação de
  nomes próprios de pessoas; 2) identificação de substantivos ou
  adjetivos relacionados à relações familiares; 3) identificação de
  padrões sintáticos que identifiquem relações familiares explicitas ou
  implícitas mencionadas nos verbetes.&lt;/p&gt;
&lt;p&gt;Existem algumas tarefas que precisamos realizar: NER (named e entities
  recognition), NEC (named entities classification: pessoa ou
  instituição ou lugar etc) e desambiguação de nomes. Para identificar
  as entidades nomeadas o problema parece simples mas sempre temos casos
  mais complicados. Por exemplo, podemos marcar uma entidade ou duas na
  sentença 1, levando as anotação de 2 ou 3:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Universidade Federal do Rio de Janeiro&lt;/li&gt;
  &lt;li&gt;[Universidade Federal do Rio de Janeiro]&lt;/li&gt;
  &lt;li&gt;[Universidade Federal] do [Rio de Janeiro]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note que identificar, classificar e disambiguar não necessariamente
  são tarefas separadas. Tipicamente a desambiguação significa mapear
  uma string no texto à uma entrada em uma base de dados. Obviamente a
  existência de uma lista de nomes de políticos (obtida de sites do
  governo), pode ajudar, mas em geral usamos recursos mais abrangentes
  como DBPedia, Wkidata, Yago etc. Tomando a decisão por 2, uma ligação
  possível seria com a &lt;a href=&quot;https://pt.wikipedia.org/wiki/Universidade_Federal_do_Rio_de_Janeiro&quot;&gt;página&lt;/a&gt;. A existência da entrada na Wikipedia para
  `Universidade Federal do Rio de Janeiro&amp;#8217; é uma evidência de que a
  anotação 2 seria mais adequada. E obviamente se desambiguamos uma
  entrada X contra um DB que já nos diz o tipo de X também estamos
  também fazendo a classificação, os problemas se misturam.&lt;/p&gt;
&lt;p&gt;A identificação dos nomes pode ser feita diretamente, marcando no
  texto de entrada segmentos que correspondem a nomes ou como uma camada
  de anotação acima das árvores sintáticas. Penso que combinar as
  abordagens sempre ajuda muito a identificarmos errors de
  processamento. Usando o &lt;a href=&quot;https://www.ibm.com/watson/services/knowledge-studio/&quot;&gt;WKS&lt;/a&gt; já temos uma primeira versão de &lt;a href=&quot;https://github.com/cpdoc/dhbb-json&quot;&gt;anotações&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Uma das coisas que está na nossa &lt;a href=&quot;https://github.com/cpdoc/dhbb-nlp/issues/31&quot;&gt;lista de tarefas&lt;/a&gt; é verificar as
  anotações sintáticas a partir de listas de nomes de
  entidades. Sequencias de tokens partes de um nome deveriam estar todos
  em uma subárvore da arvore sintática. Esta verificação nos ajudaria a
  melhorar as &lt;a href=&quot;https://github.com/cpdoc/dhbb-nlp/tree/master/udp&quot;&gt;análises sintáticas&lt;/a&gt; que já temos. Como disse acima, listas
  de nomes de pessoas, instituições ou lugares podem ser obtidas de
  sites do governo, das bases que mencionei acima ou usar as anotações
  que já temos.&lt;/p&gt;
&lt;p&gt;Feita a identificação de nomes, podemos partir para os termos de
  parentesco. Um bom ponto de partida seria a nossa &lt;a href=&quot;http://openwordnet-pt.org&quot;&gt;OpenWordnet-PT&lt;/a&gt;. Uma
  revisão da cobertura dela para estes termos seria excelente. Descobrir o
  que temos e o que está faltando. Assim poderíamos usar a estrutura da
  WN para generalizar as buscas nos textos por termos relacionados à
  relações familiares, ao invés de enumerar todos os termos, poderíamos
  usar as relações semânticas da WN para perguntar por termos e seus
  hipônimos, por exemplo. Entender a modelagem destes termos na OWN-PT
  seria o ponto de partida e este assunto é bem interessante por si só:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Kinship_terminology&quot;&gt;https://en.wikipedia.org/wiki/Kinship_terminology&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Dyadic_kinship_term&quot;&gt;https://en.wikipedia.org/wiki/Dyadic_kinship_term&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Coefficient_of_relationship&quot;&gt;https://en.wikipedia.org/wiki/Coefficient_of_relationship&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Category:Kinship_terminology&quot;&gt;https://en.wikipedia.org/wiki/Category:Kinship_terminology&lt;/a&gt; não tem
    links para Português, talvez Wikipedia precise de ajuda.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Uma busca no DHBB pelos termos mais &amp;#8216;óbvios&amp;#8217; que me ocorreram ajuda a
  termos uma idéia do que devemos encontrar no DHBB. Esta é uma &lt;a href=&quot;https://gist.github.com/arademaker/e10d43992287008ef044630d5ab12e9c&quot;&gt;busca
  preliminar&lt;/a&gt;, não está cobrindo o DHBB inteiro e claramente o analisador
  sintático ainda comete errors e não faz análises consistentes. Também
  podemos ter uma idéias da quantidade de sentenças que mencionam estes
  termos:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bash&quot;&gt;
% awk &amp;#39;$0 ~ /text =/ {sent = $0} $3 ~ /^(irmã|irmão|pai|mãe|tio|tia|bisavô|bisavó|primo|prima|avô|avó|sobrinho|sobrinha|cunhado|cunhada|parente)$/ {print sent}&amp;#39; *.conllu  | wc -l
    5812
&lt;/pre&gt;
&lt;p&gt;Em seguida, temos a identificação das possíveis relações de parentesco
  entre pessoas que possam ser identificadas nos textos. Obviamente,
  muitas vezes os autores podem apenas mencionar a existência de um
  parente mas não nomea-lo. Nos exemplos abaixo, note que o nome do
  verbetado (pessoa que o verbete descreve) não aparece explicitamente
  nas sentenças.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bash&quot;&gt;
% awk &amp;#39;$0 ~ /text =/ {sent = $0} $3 ~ /^(irmã|irmão|pai|mãe|tio|tia|bisavô|bisavó|primo|prima|avô|avó|sobrinho|sobrinha|cunhado|cunhada|parente)$/ {print sent}&amp;#39; *.conllu  | head
&lt;/pre&gt;
&lt;ol&gt;
  &lt;li&gt;Em reportagem do dia 18 de março de 2000, do jornal «Folha de
    S. Paulo», Armando Abílio foi acusado de ser, ao lado do deputado
    Raimundo Santos (PFL-PA), o campeão do nepotismo na Câmara dos
    Deputados, tendo contratado sete parentes para seu gabinete.&lt;/li&gt;
  &lt;li&gt;Casou-se com Rosimere Bronzeado Vieira - sobrinha de Luís
    Bronzeado, que foi deputado federal pela Paraíba de 1959 a 1967 -,
    com quem teve cinco filhos.&lt;/li&gt;
  &lt;li&gt;Ainda em 1998, foi nomeada para o cargo vitalício de conselheira do
    Tribunal de Contas do Estado do Amapá (TCE-AP) por João Capiberibe,
    seu irmão e então governador do estado.&lt;/li&gt;
  &lt;li&gt;Como advogado, Pedro Aleixo participou de rumorosos julgamentos,
    entre eles o das irmãs Poni (31/3/1964), que conseguiu inocentar da
    acusação de assassinato, e o de Roberto Lobato (abril de 1973),
    igualmente absolvido da mesma imputação, nesse caso contra o
    posicionamento de Pedro Aleixo, que atuou como advogado de
    acusação.&lt;/li&gt;
  &lt;li&gt;Era casado com Maria Stuart Brandi Aleixo, com quem teve quatro
    filhos, um dos quais, Maurício Brandi Aleixo, após a morte do pai,
    empenhou-se em dar prosseguimento à organização do PDR.&lt;/li&gt;
  &lt;li&gt;Seu avô, Miguel Arraes, de quem era considerado sucessor político,
    foi por três vezes governador de Pernambuco (1963-1964, 1987-1990 e
    1995-1998) e deputado federal durante dois mandatos (1983-1987 e
    1991-1995).&lt;/li&gt;
  &lt;li&gt;Sua mãe Ana Arraes exerceu mandato como deputada federal
    (2007-2010) e desde 2011 é ministra do Tribunal de Contas da União
    (TCU).&lt;/li&gt;
  &lt;li&gt;Em 2005, foi eleito para o cargo de presidente do PSB, após a morte
    de seu avô e então presidente do Partido, Miguel Arraes.&lt;/li&gt;
  &lt;li&gt;Durante sua gestão destacam-se a criação de programas sociais como
    o “Pacto pela Vida”, promovido pela Secretaria Estadual de
    Segurança com vistas à redução dos índices de violência, que
    alcançou queda de 39% no índice de homicídios; e o programa “Mãe
    Coruja Pernambucana”, criado para diminuir a taxa de mortalidade
    infantil, mais tarde condecorado pela Organização das Nações Unidas
    (ONU) e posteriormente agraciado com o Prêmio Interamericano da
    Inovação para a Gestão Pública Efetiva.&lt;/li&gt;
  &lt;li&gt;Francisco Campos aprendeu as primeiras letras com sua mãe e depois
    passou dois anos como interno no Instituto de Ciências e Letras de
    São Paulo, regressando em seguida a Dores do Indaiá para estudar
    português e francês.&lt;/li&gt;
  &lt;li&gt;Há entretanto quem julgue, como Alexandre Barbosa Lima Sobrinho,
    que suas realizações foram motivadas pelo afã de projetar
    nacionalmente o próprio nome de Antônio Carlos, tendo em vista a
    sucessão de Washington Luís, que assumira a presidência da
    República em 15 de novembro de 1926.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Quando olhamos para os tokens que são termos de relações familiares e
  sua relação com os demais tokens das sentenças, temos 299 casos de
  &lt;a href=&quot;https://universaldependencies.org/u/dep/index.html&quot;&gt;ligações sintáticas&lt;/a&gt; diferentes, comprovando o que vemos nos exemplos
  acima, uma possível grande diversidade de padrões sintáticos usados.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bash&quot;&gt;
% awk &amp;#39;$3 ~ /^(irmã|irmão|pai|mãe|tio|tia|bisavô|bisavó|primo|prima|avô|avó|sobrinho|sobrinha|cunhado|cunhada|parente)$/ {print $2,$4,$8}&amp;#39; *.conllu | sort | uniq -c | sort -nr | wc -l
     299
&lt;/pre&gt;
&lt;p&gt;Os 10 casos mais frequentes são os abaixo. O simbolo &lt;code&gt;nsubj&lt;/code&gt; indica
  sujeito da sentença, &lt;code&gt;nmod&lt;/code&gt; indica que a palavra ‘pai’ está
  modificando outro substantivo, &lt;code&gt;flat:name&lt;/code&gt; indica que o termo é parte
  de um nome etc.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bash&quot;&gt;
% awk &amp;#39;$3 ~ /^(irmã|irmão|pai|mãe|tio|tia|bisavô|bisavó|primo|prima|avô|avó|sobrinho|sobrinha|cunhado|cunhada|parente)$/ {print $2,$4,$8}&amp;#39; *.conllu | sort | uniq -c | sort -nr | head
 649 pai NOUN nsubj
 394 pai NOUN nmod
 370 irmão NOUN nsubj
 309 Sobrinho PROPN flat:name
 254 pai NOUN nsubj:pass
 211 irmão NOUN appos
 177 irmão NOUN nmod
 170 irmão NOUN nsubj:pass
 163 avô NOUN nsubj
 144 tio NOUN nsubj
&lt;/pre&gt;
&lt;p&gt;Isto indica que embora possamos escrever padrões para extração de
  informações das árvores ou diretamente do texto das sentenças, o
  trabalho poderá ser bem grande. Uma idéia é construir abstrações
  nestas estruturas gerando representação mais semânticas para serem
  analisadas. Outra idéia seria usar técnicas de aprendizado de máquina
  tanto na identificação de termos para relações familiares quando na
  extração de relações familiares. Existem vários artigos explorando
  estas técnicas.&lt;/p&gt;
&lt;p&gt;Iniciantes na área de PLN costumam encontrar bibliotecas como NLTK, e
  acreditar que usando a biblioteca poderão processar textos
  facilmente. De fato, esta particular biblioteca é bem documentada e
  descreve no &lt;a href=&quot;http://www.nltk.org/book/ch07.html&quot;&gt;capítulo 7&lt;/a&gt; algumas abordagens para extração de
  informações. Mas muito dos passos mencionados no texto assumem modelos
  já treinados, no inglês. E mesmo que existam modelos para Português,
  nossos experimentos mostram que no DHBB os resultados contém muitos
  erros em todas as etapa: segmentação de sentenças, pos tagging,
  identificação de nomes etc. Por isso nosso projeto.&lt;/p&gt;
&lt;p&gt;Para interessados na área de processamento de texto, sugiro a leitura
  do livro ainda em edição &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;https://web.stanford.edu/~jurafsky/slp3/&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Processando o DHBB -- o que temos até agora?</title>
   <link href="http://arademaker.github.com/blog/2020/06/04/dhbb.html"/>
   <updated>2020-06-04T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2020/06/04/dhbb</id>
   <content type="html">&lt;p&gt;Hoje recebi uma mensagem bem interessante perguntando sobre o que
  tenho feito em relação ao processamento do &lt;a href=&quot;http://cpdoc.fgv.br/acervo/dhbb&quot;&gt;DHBB&lt;/a&gt;. Resolvi aproveitar
  para escrever este post.&lt;/p&gt;
&lt;p&gt;Na lista das minhas &lt;a href=&quot;http://arademaker.github.io/publications.html&quot;&gt;publicações&lt;/a&gt;, interessados podem consultar todos os
  artigos que escrevemos sobre trabalhos com o DHBB. Ao longo dos anos,
  fizemos diferentes experimentos ‘exploratórios’ no DHBB, alguns com
  objetivo, não de processar o DHBB, mas de usa-lo para expandir
  recursos que precisávamos criar para o processamento de textos em PT,
  além mesmo do DHBB.&lt;/p&gt;
&lt;p&gt;Recentemente, agora sim ja tendo recursos para tal, pudemos começar a
  pensar no processamento do DHBB propriamente dito. Mas os recursos
  necessários ainda estão longe de completos ou com tamanho adequado
  para garantir boa qualidade, logo penso no processamento do DHBB como
  um projeto de longo prazo que passará por refinamentos
  sucessivos. Quando digo recursos, falo de coisas como:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Nossa &lt;a href=&quot;http://openwordnet-pt.org&quot;&gt;WordNet do português&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;O corpus &lt;a href=&quot;http://github.com/universaldependencies/UD_Portuguese-Bosque&quot;&gt;Bosque&lt;/a&gt; e os outros de PT que também mantemos como parte do
    projeto &lt;a href=&quot;http://universaldependencies.org&quot;&gt;Universal Dependencies&lt;/a&gt;. Estes dados usados para parser das
    sentenças.&lt;/li&gt;
  &lt;li&gt;O &lt;a href=&quot;https://github.com/LFG-PTBR/MorphoBr&quot;&gt;dicionário morfológico&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;O &lt;a href=&quot;https://github.com/System-T/UniversalPropositions/tree/master/UP_Portuguese-Bosque&quot;&gt;corpus com anotações de papéis semânticos&lt;/a&gt; para treino de SRL.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;No &lt;a href=&quot;http://arademaker.github.io/bibliography/propor-2020.html&quot;&gt;artigo do último PROPOR&lt;/a&gt;, mostramos que mesmo a segmentação de
  sentenças não é trivial e que certamente vários erros de análise
  sintática (usamos o UDPipe) existem dado o reduzido tamanho do corpus
  Bosque e diferenças entre o estilo textual do corpus Bosque
  (jornalístico) para o DHBB (enciclopédico). O que estamos fazendo é
  criar um workflow que permita o trabalho de longo prazo onde revisões
  humanas sejam integradas em ciclos de treino e avaliação dos
  diferentes componentes que estamos usando. Eu poderia falar muito mais
  sobre as dificuldades reais na segmentação de sentenças, mas vou
  deixar isso para outros posts.&lt;/p&gt;
&lt;p&gt;Para a segmentação de sentenças, temos tido melhores resultados com o
  &lt;a href=&quot;https://opennlp.apache.org&quot;&gt;OpenNLP&lt;/a&gt;, que comparamos com outras ferramentas no paper mencionado
  acima, dentre elas o &lt;a href=&quot;https://github.com/TALP-UPC/FreeLing&quot;&gt;Freeling&lt;/a&gt;, que encontramos muitas limitações de
  configuração (abreviações no final de sentenças são um problema sem
  solução para o Freeling). Mas aprendemos que o modelo de segmentação
  &lt;a href=&quot;http://opennlp.sourceforge.net/models-1.5/&quot;&gt;disponibilizado&lt;/a&gt; em também erra várias sentenças do DHBB. Com isso,
  retreinamos o OpenNLP com um subconjunto de sentenças do DHBB
  segmentadas manualmente. Ainda estamos investigando alternativa ao
  OpenNLP, talvez o próprio UDPipe ou NLTK, este &lt;a href=&quot;https://github.com/cpdoc/dhbb-nlp/issues/39&quot;&gt;issue&lt;/a&gt; trata deste
  assunto. Não sou muito fã de Python, mas começamos a testar o NLTK
  principalmente por causa do modulo de segmentação de sentenças que
  implementa o algoritmo Punkt, especialmente util para lidar com
  abreviações.&lt;/p&gt;
&lt;p&gt;Falando da análise sintática. Treinamos o &lt;a href=&quot;http://lindat.mff.cuni.cz/services/udpipe/&quot;&gt;UDPipe&lt;/a&gt; com o corpus
  &lt;a href=&quot;https://github.com/universaldependencies/UD_Portuguese-Bosque&quot;&gt;UD_Portuguese-Bosque&lt;/a&gt;, que estamos ainda constantemente revisando,
  expandido com as sentenças do DHBB analisadas que já revisamos. Com
  este modelo, aplicamos no restante do DHBB e repetimos o ciclo. Nas
  últimas semanas, focamos nas sentenças dos primeiros parágrafos dos
  verbetes, temos atualmente umas 200-300 sentenças revisadas. Estas
  sentenças, quando adicionadas ao Bosque, conseguiram ‘ensinar’ o
  UDPipe a análise dos primeiros parágrafos dos verbetes (os parágrafos
  que falam sobre relações familiares).&lt;/p&gt;
&lt;p&gt;Em paralelo ao nosso trabalho, Diana e Suemi incluiram o DHBB no
  acervo da &lt;a href=&quot;https://www.linguateca.pt&quot;&gt;Linguateca&lt;/a&gt;, seguindo os métodos de processamento dos textos
  que a Linguateca adota. Eu não estou mais participando diretamente
  deste esforço embora tenha colaborado em um &lt;a href=&quot;http://arademaker.github.io/bibliography/dhn-2019.html&quot;&gt;artigo do ano
  passado&lt;/a&gt;. Para mim, a principal limitação da abordagem da Linguateca
  são as ferramentas adotadas. Algumas proprietárias, como o parser
  &lt;a href=&quot;http://visl.sdu.dk/visl/pt/parsing/automatic/dependency.php&quot;&gt;PALAVRAS&lt;/a&gt; que produzem analises pouco ’standard’ e consequentemente de
  difícil integração com outras ferramentas. Por isso prefiro outras
  abordagens.&lt;/p&gt;
&lt;p&gt;Quanto a colaboração com interessados. Uma coisa que precisamos muito
  é UI. Nosso ciclo de revisão e treino com human-in-the-loop precisa
  evoluir para que os usuários possam fazer revisões em interfaces mais
  visuais sem lidar com arquivos. Isto permitiria maior agilidade em
  cada ciclo de revisão. Na parte mais de NLP, ter a análise sintática é
  apenas uma etapa para extração de informações. Precisamos de muitas
  outras ‘camadas’ de anotação como SRL, NER, expressões temporais
  etc. E talvez continuar combinando técnicas. Por exemplo, para
  reconhecimento de entidades nomeadas, temos um modelo treinado no &lt;a href=&quot;https://www.ibm.com/cloud/watson-knowledge-studio&quot;&gt;IBM
  Watson Knowledge Studio&lt;/a&gt; (WKS). Nosso &lt;a href=&quot;http://dhbb.mybluemix.net/dhbb/home&quot;&gt;demo&lt;/a&gt; ainda está super básico e
  também precisa evoluir. Mas o modelo treinado no WKS ainda precisa ser
  melhorado e ainda precisamos pensar como integrar as anotações do WKS
  com estes processamentos mais ‘linguisticos’ que estamos fazendo
  agora. Enfim, muita coisa para fazer.&lt;/p&gt;
&lt;p&gt;Com sorte, o que está sendo feito pode ser útil para processamento de
  outros textos no futuro, além do DHBB, como wikipedia.&lt;/p&gt;
&lt;p&gt;São vários colaboradores participando deste projeto, em diferentes
  momentos e de diferentes formas. Em especial, não posso deixar de
  mencionar a Valeria de Paiva, amiga que me introduziu na área de
  linguistica computacional e me apresentou tantas pessoas ao longo
  destes anos.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>PARSEME corpus</title>
   <link href="http://arademaker.github.com/blog/2019/05/20/parseme.html"/>
   <updated>2019-05-20T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2019/05/20/parseme</id>
   <content type="html">&lt;p&gt;During the evaluation of a paper, I read about the PARSEME annotation
  guidelines of verbal multiword expression.&lt;/p&gt;
&lt;p&gt;The question that came to my mind was, what would happen if we process
  PARSEME data with ERG grammar? Would it be possible to associate to
  each type of verbal construction a pattern in the MRS?&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;http://parsemefr.lif.univ-mrs.fr/parseme-st-guidelines/1.1/?page=home&lt;/li&gt;
  &lt;li&gt;http://erg.delph-in.net&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>RDF/OWL support for Common Lisp</title>
   <link href="http://arademaker.github.com/blog/2018/01/17/projects.html"/>
   <updated>2018-01-17T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2018/01/17/projects</id>
   <content type="html">&lt;p&gt;As I wrote in the comments on &lt;a href=&quot;https://github.com/ha-mo-we/Racer/issues/4&quot;&gt;this issue&lt;/a&gt;, it would be a nice project
  to improve Wilbur and Racer2 making them more modular and robust. It
  is a shame the current support for RDF/OWL in Lisp, it is almost
  nothing compare with Python and Java libraries available such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://rdflib.readthedocs.io/en/stable/&lt;/li&gt;
  &lt;li&gt;https://jena.apache.org&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The only robust library available for RDF/OWL in Lisp is the Allegro
  Graph system from Franz, but it is a commercial and since its 3.X
  version, it can&amp;#8217;t be used as a library for simples RDF/OWL
  manipulations.&lt;/p&gt;
&lt;p&gt;I found https://github.com/SeijiKoide/SWCLOS but I haven&amp;#8217;t tried it
  yet. The ideas didn&amp;#8217;t not evolved so much since the paper
  &lt;a href=&quot;http://www.iiia.csic.es/~puyol/TAPIA2001/use-of-lisp-on-sw.pdf&quot;&gt;The Use of Lisp in Semantic Web Applications&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>anotações sintáticas</title>
   <link href="http://arademaker.github.com/blog/2017/06/06/dependencias.html"/>
   <updated>2017-06-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2017/06/06/dependencias</id>
   <content type="html">&lt;p&gt;Dada a sentença &amp;#8220;O garoto, que mora na rua 12, estava correndo atrás
  da bola.&amp;#8221;, são várias as análises sintáticas possíveis dependendo do
  formalismo adotado. Estes formalismos dividem-se em duas grandes
  classes: `phrase-structure` ou `dependencies` mas existem variações
  entre teorias dentro destas classes. Vejamos dois formalismos de
  dependências.&lt;/p&gt;
&lt;p&gt;O sistema &lt;a href=&quot;http://visl.sdu.dk/visl/pt/parsing/automatic/dependency.php&quot;&gt;PALAVRAS&lt;/a&gt; tem como saída padrão para a análise por
  dependências da sentença acima, o seguinte trecho:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
O [o] &amp;lt;*&amp;gt; &amp;lt;artd&amp;gt; DET M S @&amp;gt;N #1-&amp;gt;2
garoto [garoto] &amp;lt;Hbio&amp;gt; N M S @SUBJ&amp;gt; #2-&amp;gt;11
, #3-&amp;gt;0
que [que] &amp;lt;clb&amp;gt; &amp;lt;clb-fs&amp;gt; &amp;lt;rel&amp;gt; SPEC M S @SUBJ&amp;gt; #4-&amp;gt;5
mora [morar] &amp;lt;vK&amp;gt; &amp;lt;mv&amp;gt; &amp;lt;np-close&amp;gt; V PR 3S IND VFIN @FS-N&amp;lt; #5-&amp;gt;2
em [em] &amp;lt;sam-&amp;gt; PRP @&amp;lt;SA #6-&amp;gt;5
a [o] &amp;lt;-sam&amp;gt; &amp;lt;artd&amp;gt; DET F S @&amp;gt;N #7-&amp;gt;8
rua [rua] N F S @P&amp;lt; #8-&amp;gt;6
12 [12] &amp;lt;card&amp;gt; NUM M/F P @&amp;lt;SC #9-&amp;gt;5
, #10-&amp;gt;0
estava [estar] &amp;lt;fmc&amp;gt; &amp;lt;aux&amp;gt; V IMPF 3S IND VFIN @FS-STA #11-&amp;gt;0
correndo [correr] &amp;lt;clb&amp;gt; &amp;lt;mv&amp;gt; V GER @ICL-AUX&amp;lt; #12-&amp;gt;11
atrás de [atrás=de] &amp;lt;sam-&amp;gt; PRP @&amp;lt;ADVL #13-&amp;gt;12
a [o] &amp;lt;artd&amp;gt; &amp;lt;-sam&amp;gt; DET F S @&amp;gt;N #14-&amp;gt;15
bola [bola] &amp;lt;cc&amp;gt; &amp;lt;tool&amp;gt; &amp;lt;food-c-h&amp;gt; &amp;lt;act&amp;gt; N F S @P&amp;lt; #15-&amp;gt;13
. #16-&amp;gt;0
&lt;/pre&gt;
&lt;p&gt;Um parser de dependências treinado com um corpus anotado seguindo o
  modelo de dependências &lt;a href=&quot;http://universaldependencies.org&quot;&gt;Universal Dependencies&lt;/a&gt;, neste caso o corpus
  UD_Portuguese (Bosque em UD produzido pelo nosso grupo), produz a
  seguinte saída para a mesma sentença:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
1	O	_	DET	DET	_	2	det	_	_
2	garoto	_	NOUN	NOUN	_	12	nsubj	_	SpaceAfter=No
3	,	_	PUNCT	.	_	5	punct	_	_
4	que	_	PRON	PRON	_	5	nsubj	_	_
5	mora	_	VERB	VERB	_	2	acl:relcl	_	_
6-7	na	_	_	_	_	_	_	_	_
6	en	en	ADP	ADP	_	8	case	_	_
7	a	o	DET	DET	Definite=Def|Gender=Fem|Number=Sing|PronType=Art	8	det	_	_
8	rua	_	NOUN	NOUN	_	5	nmod	_	_
9	12	_	NUM	NUM	NumType=Card	8	appos	_	SpaceAfter=No
10	,	_	PUNCT	.	_	5	punct	_	_
11	estava	_	AUX	AUX	_	12	aux	_	_
12	correndo	_	VERB	VERB	_	0	root	_	_
13	atrás	_	ADV	ADV	_	12	advmod	_	_
14-15	da	_	_	_	_	_	_	_	_
14	de	de	ADP	ADP	_	16	case	_	_
15	a	o	DET	DET	Definite=Def|Gender=Fem|Number=Sing|PronType=Art	16	det	_	_
16	bola	_	NOUN	NOUN	_	13	nmod	_	SpaceAfter=No
17	.	_	PUNCT	.	_	12	punct	_	_
&lt;/pre&gt;
&lt;p&gt;Existem diferenças teóricas e técnicas entre estes dois modelos de
  dependências. As diferenças teórias são as que caracterizam cada
  modelo ou formalismo. Por exemplo, em UD, o `root` da sentença, o nó
  raiz da árvore sintática, é o verbo `correr`. Para o PALAVRAS o root é
  o verbo `estar` que, alias, não é exatamente o root, porque o root
  para o PALAVRAS é um nó 0, que não tem nenhum token
  associado. PALAVRAS identificou `atrás de` como uma MWE funcionando
  como preposição enquanto UD não fez o mesmo agrupamento. Para o
  PALAVRAS, todas as pontuações apontam para o root da sentença. Para
  UD, o tratamento das pontuações não é tão simples. Vale perceber que o
  parser &lt;a href=&quot;http://lindat.mff.cuni.cz/services/udpipe/run.php&quot;&gt;UDPipe&lt;/a&gt;, que usei para produzir a saída acima, errou no
  desmembramento da contração `na`.&lt;/p&gt;
&lt;p&gt;Mas para revisão do corpus, estou agora mais interessado nas
  diferenças técnicas. Ambas as saídas codificam de forma diferente
  várias informações: os links de dependência, as POS tags, as features,
  lemas etc. Ambos representam um token por linha, mas o PALAVRAS
  apresenta as informações de cada token de uma forma mais `flat`, uma
  &lt;i&gt;sopa&lt;/i&gt; de símbolos, apelando para alguns caracteres especiais que
  identificam os tipos de cada símbolo. Lema entre conchetes, tags e
  features em maiusculas, relações sintáticas começam com o símbolo `@`
  e outras tags sintáticas e semânticas entre `&amp;lt;&amp;#8230;&amp;gt;`. Em contrapartida,
  o formato CoNLL-U adotado por UD propõe que cada informação esteja em
  uma coluna de um formato tabular. Em UD, as features são
  explicitamente definidas, por exemplo, `Gender=Fem`. No PALAVRAS o
  símbolo `F` codifica esta mesma informação.&lt;/p&gt;
&lt;p&gt;Pensando na tarefa de revisão de anotações sintáticas, qual formato
  seria mais adequado para edições? Quais outros formatos possíveis
  existem? Esta discussÃo é certamente menos relevante se adotarmos uma
  postura de revisão de corpora centrada no suporte de alguma ferramenta
  de anotação, como &lt;a href=&quot;http://brat.nlplab.org&quot;&gt;Brat&lt;/a&gt; ou &lt;a href=&quot;https://webanno.github.io/webanno/&quot;&gt;Webanno&lt;/a&gt;. Mas a verdade é que nenhuma destas
  ferramentas é tão flexível como a edição direta de arquivos texto com
  suporte de alguma interface de visualização e `debug` da anotação.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ideas of Projects for 2017</title>
   <link href="http://arademaker.github.com/blog/2017/02/22/projects.html"/>
   <updated>2017-02-22T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2017/02/22/projects</id>
   <content type="html">&lt;p&gt;Some possible ideas for students looking for projects: &amp;#8216;iniciação
  científica&amp;#8217;.&lt;/p&gt;
&lt;h2&gt;Text Entailment&lt;/h2&gt;
&lt;p&gt;We propose a project to evaluate different techniques for &lt;a href=&quot;https://en.wikipedia.org/wiki/Textual_entailment&quot;&gt;text
  entailment&lt;/a&gt; using deep parsing. We were particularly interested in
  ‘deep’ linguistic processing of sentences. The goal is the combination
  of linguistic and statistical processing methods for getting at the
  meaning of texts and utterances.  For the experiments, we propose the
  use of the &lt;a href=&quot;http://clic.cimec.unitn.it/composes/sick.html&quot;&gt;SICK&lt;/a&gt; corpus and it was the corpus used in the &lt;a href=&quot;http://alt.qcri.org/semeval2014/task1/&quot;&gt;SemEval 2014&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some tools/ideas under consideration are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.03196&quot;&gt;Universal DepLambda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/percyliang/sempre&quot;&gt;Sempre&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nlp.stanford.edu/pubs/schuster2016enhanced.pdf&quot;&gt;Enhanced Dependencies&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/uwnlp/EasySRL&quot;&gt;EasySRL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://amr.isi.edu/&quot;&gt;AMR&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Dependency Parser for Portuguese in FreeLing&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://nlp.lsi.upc.edu/freeling/&quot;&gt;Freeling&lt;/a&gt; is a developer-oriented library providing language analysis
  services.  Freeling has already a good support for Portuguese in all
  its base modules (tokenizer, sentence splitter, POS tagger, WSD,
  etc.). We want to extend that support with a dependency parser for
  Portuguese.  This project is about to understand how to train a parser
  in FreeLing and make it for Portuguese, evaluating the result.&lt;/p&gt;
&lt;p&gt;For the training we can use the recently released UD_Portuguese data
  under the &lt;a href=&quot;http://universaldependencies.org/&quot;&gt;Universal Dependencies&lt;/a&gt; project.&lt;/p&gt;
&lt;h2&gt;SUO-KIF translator to TPTP&lt;/h2&gt;
&lt;p&gt;We have rewrote the &lt;a href=&quot;https://github.com/own-pt/cl-krr&quot;&gt;translation&lt;/a&gt; from &lt;a href=&quot;http://www.adampease.org/OP/&quot;&gt;SUO-KIF&lt;/a&gt; logic language to &lt;a href=&quot;http://www.cs.miami.edu/~tptp/&quot;&gt;TPTP&lt;/a&gt;
  language. In this project we want to expand the translation of
  high-order construction to TPTP/THF. In the sequence, we want to make
  the output readable to &lt;a href=&quot;http://www.ai.sri.com/~stickel/snark.html&quot;&gt;SNARK&lt;/a&gt; prover to explore its support to
  Procedural Attachments.&lt;/p&gt;
&lt;p&gt;Ideally, the translator should be written in logic or functional
  programming style using: Prolog, Haskell or Common Lisp, etc.&lt;/p&gt;
&lt;h2&gt;CoNLL-U and Universal Dependencies toolset&lt;/h2&gt;
&lt;p&gt;The creation of an annotated corpus with dependencies is a hard task
  and very time-consuming. We are collaborating with the &lt;a href=&quot;http://universaldependencies.org/&quot;&gt;Universal
  Dependencies&lt;/a&gt; Project, with a Portuguese Corpus (&lt;a href=&quot;http://github.com/universaldependencies/ud_portuguese&quot;&gt;UD_Portuguese&lt;/a&gt;). After
  release 2.0, we are now preparing for the next version expanding and
  solving errors in the current 2.0 corpus.&lt;/p&gt;
&lt;p&gt;In this project, we are interested in improving the necessary tools
  that we use:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/own-pt/cl-conllu&quot;&gt;CL-CONLLU&lt;/a&gt; : a Common Lisp library for work with CoNLL-U files&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/own-pt/conll-workbench&quot;&gt;conllu-workbench&lt;/a&gt; : a set of opensource tools that we use for
    searching and editing the corpus.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In particular, the CL library needs better support for rules and
  functions for comparing different trees and help in the identification
  of common patterns of errors.&lt;/p&gt;
&lt;h2&gt;Improving the openWordnet-PT interface&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;http://wnpt.brlcloud.com/wn/&quot;&gt;OpenWordnet-PT&lt;/a&gt;, abbreviated as OpenWN-PT or simply OWN-PT) is a
  open access wordnet for Portuguese, originally developed by Valeria de
  Paiva, Alexandre Rademaker and Gerard de Melo as a syntactic
  projection of Universal WordNet (UWN) of de Melo and Weikum. Like many
  other open wordnets we believe that lexical resources need to be open
  to be useful.&lt;/p&gt;
&lt;p&gt;The OpenWN-PT is available in RDF/OWL, following and expanding, when
  necessary, the mappings from the original Princeton WordNet. Both the
  data and the RDF template settings (classes and properties) of the
  OpenWN-PT are freely available for download here. Besides being
  downloadable, the data can be retrieved via SPARQL in the endpoint and
  one can consult and compare it with other wordnets at the generic
  interface provided by the Open MultiLingual WordNet project.&lt;/p&gt;
&lt;p&gt;This project is about helping our team in the improving of the &lt;a href=&quot;https://github.com/own-pt/cl-wnbrowser&quot;&gt;web
  interface&lt;/a&gt; for our openWordnet-PT.&lt;/p&gt;
&lt;p&gt;In particular, we need to: (1) simplify the architecture; (2) improve
  the interface for votes and suggestions; (3) improve navegation and
  data visualization.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>web crawler in Racket</title>
   <link href="http://arademaker.github.com/blog/2016/10/01/web-crawler-racket.html"/>
   <updated>2016-10-01T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/10/01/web-crawler-racket</id>
   <content type="html">&lt;p&gt;I usually like to suggest projects for students as part of their
  evaluation in the &amp;#8216;programming language&amp;#8217; course. This course uses
  &lt;a href=&quot;http://racket-lang.org&quot;&gt;Racket&lt;/a&gt; language and we follow the &lt;a href=&quot;https://mitpress.mit.edu/sicp/full-text/book/book.html&quot;&gt;SICP&lt;/a&gt; book. So the question is always
  what are the good projects for the students. Getting data from
  different source and combine then in a flexible user interface is a
  very common idea. Today I decide to investigate the difficult of
  developing a simple web crawler in Racket. Since I usually code in
  Common Lisp, I was looking for something similar of CL libs like
  &lt;a href=&quot;http://www.weitz.de/drakma/&quot;&gt;drakma&lt;/a&gt; and &lt;a href=&quot;https://common-lisp.net/project/closure/closure-html/&quot;&gt;Closure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Using DrRacket only for teaching is not enough for being confortable
  with the Racket ecosystem. First I had to discover how to install the
  HTML parsing lib. This was done with:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bash&quot;&gt;
raco pkg install html-parsing
&lt;/pre&gt;
&lt;p&gt;After I have decided what libs to use, I had to understand their
  interfaces. My first code in Racket for retrieve and parse a simple
  HTML page is:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;scheme&quot;&gt;
#lang racket

(require net/http-client)
(require html-parsing)

(let-values (((a b c) (http-sendrecv &amp;quot;arademaker.github.io&amp;quot;
                                     &amp;quot;/about.html&amp;quot;)))
  (html-&amp;gt;xexp c))
&lt;/pre&gt;
&lt;p&gt;Not sure if this is the most efficiente way to make it, but surelly it
  is simple enough to start with.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Project Ideas</title>
   <link href="http://arademaker.github.com/blog/2016/09/17/projects.html"/>
   <updated>2016-09-17T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/09/17/projects</id>
   <content type="html">&lt;h1&gt;Knowledge Representation via ACE&lt;/h1&gt;
&lt;p&gt;Following the article https://arxiv.org/abs/1303.4293, we can think in
  a lot of possible extensions. For Portuguese support, we would need to
  develop the Portuguese concrete syntax in GF. The idea of code the
  translation of ACE to OWL in GF would be very interesting to explore.&lt;/p&gt;
&lt;h1&gt;Learning support tools&lt;/h1&gt;
&lt;p&gt;We would like to have an environment similar to
  https://www.hackerrank.com for receiving submissions of students
  projects.&lt;/p&gt;
&lt;h1&gt;SUMO to TFF and HTF&lt;/h1&gt;
&lt;p&gt;We have an SUMO to TPTP/FOF translation in
  https://github.com/own-pt/cl-krr. We would like to extend to TFF and
  later to TFF or THF. Alternatively, we can also translate to
  http://www.ai.sri.com/~stickel/snark.html language.&lt;/p&gt;
&lt;h1&gt;Semantic Web technologies&lt;/h1&gt;
&lt;p&gt;Both systems were realised opensource, much can be done to improve
  them! We need Common Lisp libraries for Linked Data and related
  technologies:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://json-ld.org&quot;&gt;json-ld&lt;/a&gt;: some initial attempt made from &lt;a href=&quot;https://github.com/RDProjekt/cl-json-ld&quot;&gt;cl-json-ld&lt;/a&gt; and
    maybe some references from &lt;a href=&quot;http://allegrograph.com/rdf-json/&quot;&gt;Franz&amp;#8217;s code&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://owlapi.sourceforge.net&quot;&gt;owlapi&lt;/a&gt;: some references at &lt;a href=&quot;http://www.cliki.net/rdf&quot;&gt;cliki&lt;/a&gt;. Common Lisp really need an OWLAPI
    library.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ha-mo-we/Racer&quot;&gt;Racer&lt;/a&gt; and &lt;a href=&quot;http://wilbur-rdf.sourceforge.net&quot;&gt;Wilbur&lt;/a&gt; are very nice libs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I would be very happy to supervise a work (undergrad or masters) to
  develop such libraries in Common Lisp.&lt;/p&gt;
&lt;h1&gt;Lisp library for metadata extraction&lt;/h1&gt;
&lt;p&gt;Contribute to projects like http://code.google.com/p/cl-jpegmeta/ and
  http://www.xach.com/lisp/zpb-exif/ to improve both library and the
  hability to handle IIM-style IPTC fields, EXIF fields and XMP
  metadata.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Simple queries in openWordnet-PT</title>
   <link href="http://arademaker.github.com/blog/2016/07/28/openwordnet-pt.html"/>
   <updated>2016-07-28T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/07/28/openwordnet-pt</id>
   <content type="html">&lt;p&gt;Our &lt;a href=&quot;http://wnpt.brlcloud.com/wn/&quot;&gt;OpenWordnet-PT&lt;/a&gt; is freely available for download and online use
  since its beginning. Nevertheless, some people still have difficulties
  to use the data without a proper introduction to our &amp;#8216;data
  model&amp;#8217;. Although we have already presented it in many conferences and
  articles, I believe some examples of queries can help people
  understand better our data.&lt;/p&gt;
&lt;p&gt;All relations are between synsets from PWN (Princeton). Since we
  haven&amp;#8217;t created any new synset yet, all our synsets are linked to
  Princeton Synsets via &lt;code&gt;owl:sameAs&lt;/code&gt; relation. Thus, our network is a
  projection of the PWN network, we have a injective map between our
  synsets and PWN synsets. Obviously, we have new senses and new words
  and these resources are linked to our synsets.&lt;/p&gt;
&lt;p&gt;In other words, to know the hypernyms of the word &amp;#8220;cachorro&amp;#8221; one must
  &amp;#8216;use&amp;#8217; the PWN synsets and relations:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sparql&quot;&gt;
select ?sspt ?otherpt ?otherword 
{
  ?word wn30:lexicalForm &amp;quot;cachorro&amp;quot;@pt .
  ?sspt wn30:containsWordSense/wn30:word ?word .
  ?ssen owl:sameAs ?sspt .   
  ?ssen wn30:hyponymOf+ ?other .
  ?other owl:sameAs ?otherpt .
  ?otherpt wn30:containsWordSense/wn30:word/wn30:lexicalForm ?otherword .
}
&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;hyponymOf+&lt;/code&gt; is a SPARQL 1.1 construction (&lt;a href=&quot;http://www.w3.org/TR/sparql11-query/#propertypaths&quot;&gt;property
  paths&lt;/a&gt;). It means the transitive closure of the &lt;code&gt;hyponymOf&lt;/code&gt;
  relation. The idea is to first get the synset in OWN-PT which contains
  &amp;#8220;cachorro&amp;#8221;, then find the equivalent synset in PWN. With the right
  synsets in PWN, we look for the related ones in OWN-PT and return
  them. Finally, we get the words from the OWN-PT synsets that we found.&lt;/p&gt;
&lt;p&gt;Note also that not all relations are between synsets, some relations
  such as &lt;code&gt;derivationallyRelated&lt;/code&gt; are relation between senses:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sparql&quot;&gt;
select ?s ?p
{
   ?s wn30:derivationallyRelated ?p.
}
&lt;/pre&gt;
&lt;p&gt;We associate synsets via &lt;code&gt;skos:inScheme&lt;/code&gt; to two special resources
  representing the PWN and OWN-PT wordnets to facilitate queries.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sparql&quot;&gt;
select distinct ?schema
{
   ?wsa wn30:derivationallyRelated ?wsb .
   ?ss skos:inScheme ?schema ;
       wn30:containsWordSense ?wsa .
}
&lt;/pre&gt;
&lt;p&gt;Our data model is described in the &lt;a href=&quot;https://github.com/own-pt/openWordnet-PT/blob/master/wn30.ttl&quot;&gt;wn30.ttl&lt;/a&gt; file. This is our
  &amp;#8216;vocabulary&amp;#8217; in Semantic Web terms. The queries above can be tested in
  our &lt;a href=&quot;http://wnpt.brlcloud.com:10035/repositories/wn30&quot;&gt;SPARQL endpoint&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sudoku as programming and logic exercise</title>
   <link href="http://arademaker.github.com/blog/2016/07/19/sudoku.html"/>
   <updated>2016-07-19T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/07/19/sudoku</id>
   <content type="html">&lt;p&gt;Last semester, at some point in my course on Data Structures and
  Algorithms, once again I mentioned the SUDOKU problem. I am sure that
  I haven&amp;#8217;t covered all about it yet, and this post is just to remember
  me about things that I would like to came back at some point:&lt;/p&gt;
&lt;p&gt;In Common Lisp, some libraries make the problem so easy that is hard
  to explain to the students why the logic based approaches are so
  challenging. Examples are: &lt;a href=&quot;https://common-lisp.net/project/computed-class/index-old.shtml&quot;&gt;computed-class&lt;/a&gt;, &lt;a href=&quot;https://github.com/kennytilton/cells/wiki&quot;&gt;cells&lt;/a&gt; and &lt;a href=&quot;http://www.cliki.net/screamer&quot;&gt;screamer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sudoku as SAT is documented in the article [3] and we know that a
  better encoding should be possible. I would love to continue the
  experiments with &lt;a href=&quot;http://www.ai.sri.com/~stickel/snark.html&quot;&gt;SNARK&lt;/a&gt; theorem prover following ideas from the
  papers [1] and [2].&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;T. Hillenbrand, D. Topic, and C. Weidenbach, &amp;#8220;Sudokus as Logical
    Puzzles&amp;#8221;, pp. 1–11, Apr. 2016.&lt;/li&gt;
  &lt;li&gt;G. Santos-García and M. Palomino, &amp;#8220;Solving Sudoku Puzzles with
    Rewriting Rules&amp;#8221;, pp. 1–16, 2006.&lt;/li&gt;
  &lt;li&gt;I. Lynce and J. Ouaknine, &amp;#8220;Sudoku as a SAT Problem&amp;#8221;, &lt;a href=&quot;http://anytime.cs.umass.edu/aimath06/proceedings/P34.pdf&quot;&gt;online&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So many interesting things to do!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Semantic Links for Portuguese</title>
   <link href="http://arademaker.github.com/blog/2016/06/13/nominalizations.html"/>
   <updated>2016-06-13T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/06/13/nominalizations</id>
   <content type="html">&lt;p&gt;We have presented the paper &lt;a href=&quot;/bibliography/lrec-2016-morpholinks.html&quot;&gt;Semantic Links for Portuguese&lt;/a&gt; in the
  LREC 2016. As we already know, the paper is not the end of this work,
  possible the contrary of that. We already know some improvements
  needed and some related works that we sill have to analyse. This post
  is to register these information and list possible future works for
  the article.&lt;/p&gt;
&lt;p&gt;Thanks, Diana Santos for suggesting me the works:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Medeiros, José Carlos, Rui Marques &amp;amp; Diana Santos. &lt;a href=&quot;http://www.linguateca.pt/Diana/download/Medeirosetal93.pdf&quot;&gt;Português
    Quantitativo&lt;/a&gt;, Actas do 1.o Encontro de Processamento de Língua
    Portuguesa (Escrita e Falada) - EPLP&amp;#8217;93, (Lisboa, 25-26 de Fevereiro
    de 1993).&lt;/li&gt;
  &lt;li&gt;Alina Villalva, &lt;a href=&quot;https://www.uam.es/gruposinv/upstairs/upstairs2/curricula/trabajos/villalva_1995_estructuras.pdf&quot;&gt;Estruturas Morfológicas&lt;/a&gt;, thesis, Lisboa, 1994.&lt;/li&gt;
  &lt;li&gt;The thesis &amp;#8220;A complementação da forma nominalizada deverbal sufixal
    e a conceituação do complemento nominal&amp;#8221; by Rosa Marina de Brito
    Meyer, see &lt;a href=&quot;http://lattes.cnpq.br/0344501785488249&quot;&gt;thesis&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Articles in PROPOR or EBRALC or ELC from Violeta Quertal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We still need more experiments with corpora. Possible verifying the
  cases of zero-derivational words. We can use a generator of verbs
  forms (conjugator) for help with the annotation of candidates.&lt;/p&gt;
&lt;p&gt;We have cases like the verb &amp;#8216;aparecer&amp;#8217; and its nominalizations:
  &amp;#8216;aparição&amp;#8217;, &amp;#8216;aparecimento&amp;#8217; and &amp;#8216;aparência&amp;#8217;. All of them must be in the
  resource for sure. Words as &amp;#8216;coberta&amp;#8217; and &amp;#8216;cobertura&amp;#8217; should be also
  in the resource, but what about the less frequent ones?&lt;/p&gt;
&lt;p&gt;Ambiguities versus fine-grained classification. The semantic links
  &lt;i&gt;ação&lt;/i&gt; and &lt;i&gt;resultado&lt;/i&gt; do make sense in Portuguese? If the introduce
  too many classes (using classes from other languages), we are creating
  a problem, not dealing with one real problem.&lt;/p&gt;
&lt;p&gt;Valeria has also posted recently about our work with &lt;a href=&quot;http://logic-forall.blogspot.pt/2016/06/nominalizations-and-zombie-nouns.html&quot;&gt;nominalizations&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Congratuatios Guilherme Passos</title>
   <link href="http://arademaker.github.com/blog/2016/06/10/guilherme.html"/>
   <updated>2016-06-10T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/06/10/guilherme</id>
   <content type="html">&lt;p&gt;Guilherme Passos presented last week his final undergrad
  project. Congratulations Guilherme, well done. The project is an
  extension of the ideas from the book:&lt;/p&gt;
&lt;p&gt;P. Blackburn and J. Bos, Representation and inference for natural
  language. 2005&lt;/p&gt;
&lt;p&gt;Guilherme expands the code from the book to use the Princeton Wordnet
  for making a simple conversational chatbot that can understand some
  sentences (basically all type of sentences considered in the book)
  module some hyponyms and hypernyms from PWN, and provide some answers
  to the user. The links for the code and texts will be available soon
  here.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Solving the Puzzle</title>
   <link href="http://arademaker.github.com/blog/2016/04/03/puzzle.html"/>
   <updated>2016-04-03T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2016/04/03/puzzle</id>
   <content type="html">&lt;p&gt;Solving the &lt;a href=&quot;http://fivethirtyeight.com/features/can-you-solve-the-impossible-puzzle/&quot;&gt;Can You Solve The Impossible Puzzle?&lt;/a&gt; with Common Lisp and
  &lt;a href=&quot;http://orgmode.org&quot;&gt;Org Mode&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our search space contains 45 possible candidates of pairs of
  numbers. The tables below will have one candidate of a pair on each
  row. In the first column, we have the multiple, in the second column
  the sum and the last two columns the candidate pair. The &lt;code&gt;prune&lt;/code&gt;
  function will help us to reduce the search space on each interaction,
  filtering the elements without repetition.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(ql:quickload :group-by)

(defun prune (data func)
  (let* ((pre (group-by:group-by-repeated data :keys (list func)))
         (input (remove-if (lambda (p) (&amp;lt;= (length (cdr p)) 1)) pre)))
    (reduce (lambda (acc a) (append acc (cdr a))) input
            :initial-value &amp;#39;((m s x y)))))

(cons &amp;#39;(m s x y)
      (loop for x from 1 to 9
            append (loop for y from 9 downto x
                         collect (list (* x y) (+ x y) x y))))
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[ca79f9a22ba8cd356c32197a5ac0ad93f8159c81]: start&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;27&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;21&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;32&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;28&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;20&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;45&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;40&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;35&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;30&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;25&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;54&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;48&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;42&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;63&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;56&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;49&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;72&lt;/td&gt;&lt;td&gt;17&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;64&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;81&lt;/td&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In the first time that Barack asked Pete, if Pete knew the answer his
  multiple would be unique defined in the candidate list, that was not
  the case, so we must remove the multiples without repetitions.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[6db81e23c88ea3483e1865437f8de0f9cca170cd]: step-1&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;36&lt;/td&gt;&lt;td&gt;13&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;When Barack asked Susan for the first time, she already knew that Pete
  didn&amp;#8217;t know the answer either. So the candidate list in her mind is
  the list above. But she didn&amp;#8217;t know the answer of Barack&amp;#8217;s question
  either, so her sum are not unique in this list too.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[2ac1f2a3d955fbf7a89f6db99a91c8f902775483]: step-2&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In the second time that Barack asked Pete, he still didn&amp;#8217;t know. So we
  have to exclude all unique multiples again.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[d5b928145f97d2bea7471b63383a9e34d6178b5a]: step-3&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The same again for the second time Barack asked Susan:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[17b25e5fc689d147eda2bd35c388cde44f310568]: step-4&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Pete in the third time still didn&amp;#8217;t know.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[44d455fea1e59e9db1788bb012f6cdd4abcc32f1]: step-5&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Susan in the third still didn&amp;#8217;t know.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[7a0a13546e37c1bd0d1fff1059eb069b381cbd30]: step-6&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Pete once more didn&amp;#8217;t know:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[079394364b443353af7d9353a8c0a38835b09ee2]: step-7&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;Susan in the fourth time didn&amp;#8217;t know either:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;second)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[e8f43474732654a4d80a659c50fe51b7ddba6a28]: step-8&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;At this moment, in the fifth time, Pete knew the answer. That is, his
  number should be 16, since this is the only multiple that unique
  defines the candidates: 2 and 8.&lt;/p&gt;
&lt;p&gt;If Pete didn&amp;#8217;t knew at this time, Barack would have asked once more to
  Susan and we would have to exclude the pair &lt;code&gt;(2,8)&lt;/code&gt; from the list of
  candidates:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
(prune (cdr data) #&amp;#39;first)
&lt;/pre&gt;
&lt;p&gt;#+RESULTS[b04f949aa4cfaea9ae9f63533fc50b207336f698]: step-9&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;m&lt;/td&gt;&lt;td&gt;s&lt;/td&gt;&lt;td&gt;x&lt;/td&gt;&lt;td&gt;y&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;24&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;18&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;8&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;10&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;9&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;In this candidate list, Susan would not be able to identify the
  numbers since no sum is unique.&lt;/p&gt;
&lt;h1&gt;Links&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jstatsoft.org/article/view/v046i03&quot;&gt;A Multi-Language Computing Environment for Literate Programming and
    Reproducible Research&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://orgmode.org&quot;&gt;http://orgmode.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://quicklisp.org&quot;&gt;http://quicklisp.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://common-lisp.net&quot;&gt;Common Lisp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Let over Lambda</title>
   <link href="http://arademaker.github.com/blog/2016/02/07/let-over-lambda.html"/>
   <updated>2016-02-07T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2016/02/07/let-over-lambda</id>
   <content type="html">&lt;iframe style=&quot;width:120px;height:240px;&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; src=&quot;//ws-na.amazon-adsystem.com/widgets/q?ServiceVersion=20070822&amp;OneJS=1&amp;Operation=GetAdHtml&amp;MarketPlace=US&amp;source=ac&amp;ref=tf_til&amp;ad_type=product_link&amp;tracking_id=alexanradema-20&amp;marketplace=amazon&amp;region=US&amp;placement=1435712757&amp;asins=1435712757&amp;linkId=2H7S2HFMK6B465XQ&amp;show_border=false&amp;link_opens_in_new_window=true&quot;&gt;
&lt;/iframe&gt;
&lt;p&gt;I am reading &amp;#8220;Let over Lambda&amp;#8221;. Amazing how truly is the comments and
  the words on the back cover. This book is really for Common Lisp
  developers with very solid background. I am still in Section 5.3, some
  part demands 1-2 days for real understanding. The problem isn&amp;#8217;t the
  text, actually, the book is very well-written, but the code blocks are
  hard to grasp: macros that define macros that define macros and so
  on&amp;#8230;&lt;/p&gt;
&lt;p&gt;I hope to update this post with more specific comments from my
  experiences through the book. So far, two observations:&lt;/p&gt;
&lt;p&gt;Don&amp;#8217;t try to use &lt;a href=&quot;http://sbcl.org&quot;&gt;SBCL&lt;/a&gt; for run the code. The way that SBCL encodes
  backquotes is incompatible with the code from Chapter 3. More in this
  &lt;a href=&quot;http://stackoverflow.com/questions/33724300/macros-that-write-macros-compile-error&quot;&gt;question&lt;/a&gt; of Stackoverflow.  I didn&amp;#8217;t find any information about it in
  the official &lt;a href=&quot;http://letoverlambda.com&quot;&gt;book website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I haven&amp;#8217;t investigated it further but it looks like CCL does not
  handle very well the &lt;a href=&quot;http://clhs.lisp.se/Body/02_dhq.htm&quot;&gt;read-time conditionalization&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;lisp&quot;&gt;
#+nil
(progn
  (defvar test (counter-class))
  (funcall test)
  (toogle-counter-direction)
  (funcall test))
&lt;/pre&gt;
&lt;p&gt;That is, the selection of no feature - one method to comment the
  following expression. I am still investigating this issue since it
  only appeared when I loaded the code using an ASDF system definition
  and &lt;a href=&quot;http://quicklisp.org&quot;&gt;quicklisp&lt;/a&gt;. Comments are welcome.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Building a shared world: Mapping distributional to model-theoretic semantic spaces</title>
   <link href="http://arademaker.github.com/blog/2016/01/03/building-shared-world.html"/>
   <updated>2016-01-03T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2016/01/03/building-shared-world</id>
   <content type="html">&lt;p&gt;The introduction of this papers is fascinating. The complementarity
  mentioned in the introduction has been in my mind for a while:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In recent years, the complementarity of distributional and formal
    semantics has become increasingly evident&amp;#8230; A number of proposals
    have emerged from these considerations, suggesting that an overarching
    semantics integrating both distributional and formal aspects would be
    desirable&amp;#8230;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another interesting part of the paper is its section 4.2, when the
  authors introduce their definition of &amp;#8216;model-theoretic spaces&amp;#8217;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Ontologies can be represented in various ways, but in this paper, we
    assume they are formalised in terms of sets of entities&amp;#8230; In our
    account, we do not have an a priori model of the world: we wish to
    infer it from our observation of language data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the next paragraphs, the authors precisely defined their
  &amp;#8216;model-theoretic spaces&amp;#8217;. The article is worth to reading.&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;bibtex&quot;&gt;
@InProceedings{herbelot-vecchi:2015:EMNLP,
    author    = {Herbelot, Aur\&amp;#39;{e}lie  and  Vecchi, Eva Maria},
    title     = {Building a shared world: mapping distributional to
                    model-theoretic semantic spaces},
    booktitle = {Proceedings of the 2015 Conference on Empirical Methods
                    in Natural Language Processing},
    month     = {September},
    year      = {2015},  address   = {Lisbon, Portugal},
    publisher = {Association for Computational Linguistics},
    pages     = {22--32},
    url       = {http://aclweb.org/anthology/D15-1003}
}
&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Merging RDF files</title>
   <link href="http://arademaker.github.com/blog/2015/08/18/combine-rdf.html"/>
   <updated>2015-08-18T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2015/08/18/combine-rdf</id>
   <content type="html">&lt;p&gt;How to merge multiple RDF files into a single RDF file? The first idea
  would be to convert each RDF file in &lt;a href=&quot;http://www.w3.org/2001/sw/RDFCore/ntriples/&quot;&gt;ntriples&lt;/a&gt; and just concatenate
  them using unix &lt;code&gt;cat&lt;/code&gt; utility, right? No, it doesn&amp;#8217;t work with blank
  nodes (or BNodes)! BNodes from different files with the same ID would
  be merged as a single resource and this is not the expected semantics,
  BNodes from different files are different resources, even if they have
  the same id.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rapper&lt;/code&gt; is a utility from the package &lt;a href=&quot;http://librdf.org&quot;&gt;Redland&lt;/a&gt;. Below I am
  presented the files and the number of triples on each one.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rapper -c -i ntriples wordnet-en-fixed.nt
rapper: Parsing returned 3517504 triples

$ rapper -c -i ntriples own-pt-fixed.nt
rapper: Parsing returned  824916 triples
&lt;/pre&gt;
&lt;p&gt;The oldest tool to support merging of RDF files is &lt;a href=&quot;http://www.w3.org/2000/10/swap/doc/cwm.html&quot;&gt;CWM&lt;/a&gt;. CWM is written
  in python and its performance is really bad. The command below hasn&amp;#8217;t
  finish after 5 minutes.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
/usr/local/cwm-1.2.1/cwm --ntriples own-pt-fixed.nt wordnet-en-fixed.nt &amp;gt; tudo-cwm.nt
&lt;/pre&gt;
&lt;p&gt;Next tool that I tried was &lt;a href=&quot;http://rdfpro.fbk.eu&quot;&gt;$RDF&lt;sub&gt;pro&lt;/sub&gt;$&lt;/a&gt;. The performance was excellent,
  only 11 seconds! But we must add a parameter &lt;code&gt;-w&lt;/code&gt; to force BNodes in
  input files to be renamed to avoid possible clashes. Actually, it
  doesn&amp;#8217;t make sense to me why this is not the default behaviour.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rdfpro @r -w own-pt-fixed.nt wordnet-en-fixed.nt @w tudo-pro.nt
14:45:53(I) 4342420 triples read (377077 tr/s avg)
14:45:53(I) 4342420 triples written (377077 tr/s avg)
14:45:53(I) Done in 11 s
&lt;/pre&gt;
&lt;p&gt;Next tool, &lt;code&gt;riot&lt;/code&gt; from the &lt;a href=&quot;https://jena.apache.org&quot;&gt;Jena&lt;/a&gt; library. The performance was not bad,
  it took twice the time of $RDF&lt;sub&gt;pro&lt;/sub&gt;$ but it finished. The only
  problem is that it complained about some IRI that no other tool
  complained.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ time riot own-pt-fixed.nt wordnet-en-fixed.nt &amp;gt; tudo-riot.nt
14:51:14 WARN riot :: [line: 282756, col: 1 ] Bad IRI: &amp;lt;https://w3id.org/own-pt/wn30-pt/instances/word-Ĳsselmeer&amp;gt; Code: 47/NOT_NFKC in PATH: The IRI is not in Unicode Normal Form KC.
14:51:14 WARN riot :: [line: 282756, col: 1 ] Bad IRI: &amp;lt;https://w3id.org/own-pt/wn30-pt/instances/word-Ĳsselmeer&amp;gt; Code: 56/COMPATIBILITY_CHARACTER in PATH: Bad character
...

real	0m27.398s
user	0m29.905s
sys	0m1.751s
&lt;/pre&gt;
&lt;p&gt;I don&amp;#8217;t like warnnings so I tried the safe path. I converted the
  ntriple file with these strange IRIs to RDF/XML and called &lt;code&gt;riot&lt;/code&gt;
  again. No warnnings this time, good!&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rapper -i ntriples -o rdfxml own-pt-fixed.nt  &amp;gt; own-pt-fixed.rdf
rapper: Serializing with serializer rdfxml
rapper: Parsing returned 824916 triples

$ riot --time own-pt-fixed.rdf wordnet-en-fixed.nt &amp;gt; tudo-riot.nt
own-pt-fixed.rdf : 14.84 sec  824,916 triples  55,602.32 TPS
wordnet-en-fixed.nt : 21.82 sec  3,517,504 triples  161,175.95 TPS
Total : 36.66 sec  4,342,420 triples  118,451.17 TPS
&lt;/pre&gt;
&lt;p&gt;But the output produced does have some errors! The IRIs are not
  encoded as the way the ntriples specification requires.&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
$ rapper -c -i ntriples tudo-riot.nt

rapper: Parsing URI tudo-riot.nt with parser ntriples
rapper: Error - URI tudo-riot.nt:117668 column 55 - Non-printable ASCII character 195 (0xC3) found.
rapper: Error - URI tudo-riot.nt:117668 column 56 - Non-printable ASCII character 162 (0xA2) found.
&lt;/pre&gt;
&lt;p&gt;By the way, for the future, I will use $RDF&lt;sub&gt;pro&lt;/sub&gt;$.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>VIVO Apps and Tools Webinar</title>
   <link href="http://arademaker.github.com/blog/2014/04/30/webinar.html"/>
   <updated>2014-04-30T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2014/04/30/webinar</id>
   <content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Yerterday I presented for the Apps and Tools working group my workflow
  to prepared data to be inserted into FGV VIVO instance. Since some
  people asked be to share the links and the file that I used to guide
  the presentation, I made this post.&lt;/p&gt;
&lt;p&gt;This page is generated from a &lt;a href=&quot;http://orgmode.org&quot;&gt;org file&lt;/a&gt; that I export to HTML and
  further processed with &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. The process of use org files with
  jekyll is outlined &lt;a href=&quot;http://orgmode.org/worg/org-tutorials/org-jekyll.html&quot;&gt;here&lt;/a&gt;. I plan to improve this workflow, but it is
  working for my personal website and for the websites of the courses
  that I teach at FGV.&lt;/p&gt;
&lt;h1&gt;The Toolset&lt;/h1&gt;
&lt;p&gt;This is the non comprehensive list of tools that I use. I am listing
  here the main tools that come up into my mind that should be
  interesting to others.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gnu.org/software/emacs/&quot;&gt;Emacs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://orgmode.org&quot;&gt;Org-Mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://common-lisp.net/project/slime/&quot;&gt;Slime&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Common Lisp compilers and interpreters: &lt;a href=&quot;http://franz.com/products/allegro-common-lisp/&quot;&gt;Allegro CL&lt;/a&gt;, &lt;a href=&quot;http://www.sbcl.org&quot;&gt;SBCL&lt;/a&gt; and &lt;a href=&quot;http://abcl.org&quot;&gt;ABCL&lt;/a&gt;
    (Lisp on JVM, so I can use Java RDF libraries).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.w3.org/2000/10/swap/doc/cwm.html&quot;&gt;CWM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://franz.com/agraph/allegrograph/&quot;&gt;Allegro Graph Triplestore&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://franz.com/agraph/gruff/&quot;&gt;Gruff&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Git&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.r-project.org&quot;&gt;R&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Python&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://xmlsoft.org&quot;&gt;xsltproc and xmllint&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://tidy.sourceforge.net&quot;&gt;tidy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Data Sources&lt;/h1&gt;
&lt;p&gt;We have to main sources of data: (1) the FGV researchers&amp;#8217; curricula
  vitae from &lt;a href=&quot;http://lattes.cnpq.br&quot;&gt;Lattes Platform&lt;/a&gt; and; (2) the FGV digital library.&lt;/p&gt;
&lt;p&gt;During the webinar I shared my screen and presented the web interface
  that &lt;a href=&quot;http://cnpq.br&quot;&gt;CNPq&lt;/a&gt; provides for researchers update their resumes. I also shown
  &lt;a href=&quot;http://lattes.cnpq.br/0675365413696898&quot;&gt;my curriculum vitae&lt;/a&gt; and discussed why I do not consider Brazilian
  Researchers curricula vitae open data given the captcha that blocks
  crawlers to get XML files from the Lattes website in a batch mode.&lt;/p&gt;
&lt;p&gt;I forgot to mention during the webinar but one of my old dreams is to
  convice CNPq to add &lt;a href=&quot;http://www.w3.org/TR/xhtml-rdfa-primer/&quot;&gt;RDFa&lt;/a&gt; or https://schema.org microformats into the
  HTML pages of the curricula vitae. This would not only allow crawlers
  to easier process the data but could also facilitate the maintainance
  of the system. My current XSLT transformation of Lattes XML to RDF
  cloud provide the starting point to RDFa embeeding.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://bibliotecadigital.fgv.br/dspace&quot;&gt;FGV Digital Library&lt;/a&gt; runs Dspace. The publications and collections
  metadata are easly collected from Dspace using the &lt;a href=&quot;http://www.openarchives.org/&quot;&gt;OAI-PMH protocol&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Lattes XML Files&lt;/h1&gt;
&lt;p&gt;As I said in the previous section, the curricula vitae of Brazilian
  Researchers are not open data, that is, they are not public available
  in structured format. They are only public available as HTML pages in
  the Lattes website, with limited search interface. The only way to
  universities and research institutions get the curricula vitae of
  their researchers in a structured format is to sign an &lt;a href=&quot;http://www.cnpq.br/web/portal-lattes/acordos-institucionais&quot;&gt;aggrement&lt;/a&gt; with
  CNPq. FGV has signed this aggrement and we have one server authorized
  to access CNPq web server to retrive the XML files of all curriculae
  that have informed any professional activity with FGV.&lt;/p&gt;
&lt;p&gt;To transform the Lattes files to RDF, I use a XSLT transformation that
  I developed few years ago. The XSLT is freely available at github in
  the repository &lt;a href=&quot;https://github.com/arademaker/SLattes&quot;&gt;Semantic Lattes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this repo, I made also available the DTD that I try hard to keep
  up-to-date. Unfortunately, until recently, CNPq did not make public
  annoucement of chances in the structure of the XML files that they
  produce, so I had to adapt the DTD whenever I identify changes. I have
  just found in the end of &lt;a href=&quot;http://www.cnpq.br/web/portal-lattes/extracoes-de-dados&quot;&gt;this page&lt;/a&gt; that CNPq finally realized the
  importance of making available the updated DTD. Nevertheless, the DTD
  in the link of this page is outdated. At least, using this DTD to
  validate the 489 FGV&amp;#8217;s curriculae I got more than 100 erros but using
  my DTD to validate the same files I got only 2 errors. Considering
  that those 489 files were produced my CNPq, we have two options: (1)
  the DTD is outdated; or (2) the code that produces the XML files has
  bugs. The two erros that I find using my DTD occur only with two
  curriculae that were not updated in the last 2 years.&lt;/p&gt;
&lt;p&gt;After download the XML files, the general idea to process them and
  produce the RDF files is outlined in the code below:&lt;/p&gt;
&lt;pre class=&quot;src&quot; lang=&quot;sh&quot;&gt;
for f in $ROOT/ontos/xml/*.xml; do
    ID=$(basename $f .xml)
    echo Processing $ID
    xmllint --noout --dtdvalid $REPO/LMPLCurriculo.DTD $f 2&amp;gt;&amp;gt; error.log
    xsltproc --stringparam ID $ID $REPO/lattes.xsl $f &amp;gt; $ID.rdf  
done
&lt;/pre&gt;
&lt;p&gt;After that, I import the RDF files to Allegro Graph making each
  curriculum a separated graph so I can easly identify the provenance of
  each triple. The importation is done using the &lt;a href=&quot;http://franz.com/agraph/support/documentation/current/agload.html&quot;&gt;agload&lt;/a&gt; utility. The
  load process takes aprox. 2 minutes:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
Load finished 487 sources in 00:02:03 (123.02 seconds).  
Triples added: 1,690,538 
Average Rate: 13742.00 tps
&lt;/pre&gt;
&lt;h1&gt;Data Deduplication&lt;/h1&gt;
&lt;p&gt;I briefly commented about the deduplication of records during the
  webinar. I do have to take care of removing duplicated resources about
  the same entity. Considering a thesis defended by and student at FGV
  whom have as advisor a professor at FGV. I will have metadata
  (triples) about this thesis from three different sources: (1) the RDF
  produced from the advisor&amp;#8217;s curriculum lattes XML; (2) the RDF produce
  from the student&amp;#8217;s curriculum lattes XML; and (3) the RDF obtained
  from the FGV Digital Library.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://github.com/arademaker/vivo-code&quot;&gt;current code&lt;/a&gt; that I use to identify duplicated resources is a
  Common Lisp library that is easily used if placed inside the
  local-projects directory of a &lt;a href=&quot;http://www.quicklisp.org/&quot;&gt;Quicklisp&lt;/a&gt; instalation.&lt;/p&gt;
&lt;p&gt;I can write an entire article only about deduplication in RDF. I am
  still thinking hard about this problem and really would like to find
  better alternatives.  One can note that deduplication of nodes in a
  RDF graph should not be done type by type as I am doing now. The rules
  to identify resources as being refering the same entity could
  dependent each other. That is, the deduplication of instances of
  &lt;code&gt;foaf:Person&lt;/code&gt; can activate the rule to deduplicate instances of
  &lt;code&gt;bibo:Article&lt;/code&gt; and vice-versa. It would be better to have a kind of
  fixed point transformation in the RDF graph that could keep clustering
  nodes until nothing more can be done. As a logician, I am very
  interested in approach this problem in a more declarative and
  deductive way.&lt;/p&gt;
&lt;p&gt;I also have to note that &lt;code&gt;owl:sameAs&lt;/code&gt; semantics doesn&amp;#8217;t help here. I
  do use &lt;code&gt;owl:sameAs&lt;/code&gt; to mark the nodes that should be merged but I have
  to merge the nodes after all &lt;code&gt;owl:sameAs&lt;/code&gt; triples are produced. I do
  this with two SPARQL construct queries:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
delete { ?s1 ?p ?o . }
insert { ?s2 ?p ?o . }
where {
  ?s1 owl:sameAs ?s2 .
  ?s1 ?p ?o .
  filter( !sameTerm(?p, owl:sameAs) )
}
&lt;/pre&gt;
&lt;pre class=&quot;example&quot;&gt;
delete { ?x ?p ?o1 . }
insert { ?x ?p ?o2 . }
where {
  ?o1 owl:sameAs ?o2 .
  ?x ?p ?o1 .
  filter( !sameTerm(?p, owl:sameAs) )
}
&lt;/pre&gt;
&lt;p&gt;Note that the filters block the propagation of the &lt;code&gt;owl:sameAs&lt;/code&gt;
  triples.&lt;/p&gt;
&lt;h1&gt;Mapping Lattes RDF to VIVO RDF&lt;/h1&gt;
&lt;p&gt;To map the Lattes RDF model produced by my XSLT to the expected VIVO
  RDF model, I have to look carefully to each instance of data. This
  mapping is not completed but at this point I have already mapped most
  of the data about people, publication, research areas and
  departaments.&lt;/p&gt;
&lt;p&gt;To work on the rules and queries to transform the data, I used the
  query and data browsing tools developed by Franz: Gruff and
  AllegroGraph WebView. During the webinar I presented both systems.&lt;/p&gt;
&lt;p&gt;The mapping is developed as rules that were easly tested with CWM. One
  example of rules is&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
{ ?dept foaf:member ?person ;
        rdf:type foaf:Group . } =&amp;gt; 
{ [ vivo:relates ?dept ;
    vivo:relates ?person ;
    a vivo:FacultyPosition ;
    rdfs:label &amp;quot;Professor Adjunto&amp;quot;@pt ] . } .
&lt;/pre&gt;
&lt;p&gt;Rules like the one above are placed in an n3 file and executed by CWM
  that receives the rule file and the data file and produces the data
  output file. Unfortunately, CWM does not have good performance and I
  haven&amp;#8217;t even tried to use it with all the data. I develop the rules
  and test them with only one curriculum vitae file.&lt;/p&gt;
&lt;p&gt;Once I finish to test the rules, I rewrite them as SPARQL queries. The
  one above becomes:&lt;/p&gt;
&lt;pre class=&quot;example&quot;&gt;
insert 
{ graph &amp;lt;http://www.fgv.br/vivo/import/&amp;gt; 
  {            
   [ vivo:relates ?dept ;
     vivo:relates ?person ;
      a vivo:FacultyPosition ;
     rdfs:label &amp;quot;Professor Adjunto&amp;quot;@pt ] . 
  }
}
where
{ ?dept foaf:member ?person ;
        rdf:type foaf:Group . 
}
&lt;/pre&gt;
&lt;p&gt;Note that: (1) the query produces blank nodes that need to be
  transformed into normal nodes before loaded into VIVO; (2) All created
  triples are placed in a separated graph; and (3) if this query is
  executed twice it will generate duplicated and dispensable
  triples. This is the most important limitation of using SPARQL for
  me. CWM will only execute a rule whenever necessary and the rules do
  not have to explicit declare any condition to avoid unnecessary
  creation of triples.&lt;/p&gt;
&lt;p&gt;It is still not clear to me if all SPARQL queries can be rewrited to
  prevent non necessary creation of triples. Moreover, I don&amp;#8217;t want to
  have too complicated SPARQL queries to maintain.&lt;/p&gt;
&lt;p&gt;More on the next post.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>We found a bug in the split command of Mac OS</title>
   <link href="http://arademaker.github.com/blog/2013/07/18/we-found-a-bug-in-macos-split.html"/>
   <updated>2013-07-18T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2013/07/18/we-found-a-bug-in-macos-split</id>
   <content type="html">&lt;p&gt;Yesterday my friend
&lt;a href=&quot;http://researcher.ibm.com/researcher/view.php?person=br-mnerys&quot;&gt;Marcelo Nery&lt;/a&gt;
and I found a bug in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;split&lt;/code&gt; command of Mac OS. At first, I was
surprised (we don’t expect to find bugs in core tools like grep, ls,
split, wc…, right?) and almost expected to find the same bug in the
Linux version of split. At least in the split of Ubuntu distribution,
that was not the case. The bug is presented only in the Mac OS
version.&lt;/p&gt;

&lt;h2 id=&quot;the-bug&quot;&gt;The bug&lt;/h2&gt;

&lt;p&gt;Consider the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; created with the following Common Lisp code
(actually for the rest of the post you don’t need to understand the
code):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-common-lisp&quot; data-lang=&quot;common-lisp&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;with-open-file&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;zero.log&quot;&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:element-type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned-byte&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			      &lt;span class=&quot;ss&quot;&gt;:direction&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:output&lt;/span&gt;
			      &lt;span class=&quot;ss&quot;&gt;:if-exists&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:supersede&lt;/span&gt;
			      &lt;span class=&quot;ss&quot;&gt;:external-format&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:utf-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;across&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;nil&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;AB_CDE~%F~%G~%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		     &lt;span class=&quot;nb&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;equal&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;#\_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;do&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;write-byte&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
			      &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;write-byte&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;char-code&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This file content could be inspected with hexdump command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hexdump &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; zero.log
00000000  41 42 00 00 00 43 44 45  0a 46 0a 47 0a     |AB...CDE.F.G.|
0000000d&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That is, the file has three 0 bytes in the first line right after the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; letter and before the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; letter. Now we want to split this file
one line per file.&lt;/p&gt;

&lt;p&gt;The Linux version of split works as expected, it splits the file
keeping the zero bytes unchanged.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;split&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; zero.log
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;f &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;x??&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---Begin: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---End: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xaa
ABCDE
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xaa
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xab
F
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xab
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xac
G
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xac&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Moreover, the sum of bytes of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x??&lt;/code&gt; files is equal the number of
bytes in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; file, 13 bytes.&lt;/p&gt;

&lt;p&gt;Nevertheless, the Mac OS version of split produces an unexpected
output. The letter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F&lt;/code&gt; is merged with the begining of the first line
althouth it is in the second line of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; file. Besides
that, the zero bytes causes the Mac OS split to ignore the rest of the
first line of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zero.log&lt;/code&gt; causing a lost of data. The sum of the bytes
of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x??&lt;/code&gt; files in Mac OS is only 6 bytes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;split&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; zero.log
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;f &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;x??&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---Begin: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;---End: &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xaa
ABF
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xaa
&lt;span class=&quot;nt&quot;&gt;---Begin&lt;/span&gt;: xab
G
&lt;span class=&quot;nt&quot;&gt;---End&lt;/span&gt;: xab&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;reporting-the-bug&quot;&gt;Reporting the bug&lt;/h2&gt;

&lt;p&gt;I reported the bug to Apple using the
&lt;a href=&quot;http://www.apple.com/feedback/macosx.html&quot;&gt;Mac OS Feedback form&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>The GeTFun Research Project</title>
   <link href="http://arademaker.github.com/blog/2013/01/17/GeTFun.html"/>
   <updated>2013-01-17T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2013/01/17/GeTFun</id>
   <content type="html">&lt;p&gt;January, 1 2013 marked the official beginning of the
&lt;a href=&quot;http://sqig.math.ist.utl.pt/GeTFun/&quot;&gt;GeTFun&lt;/a&gt; project. I am very
excited about this project and willing to contributed and share ideas
with all collaborators. The first workshop of the project will happen
during the &lt;a href=&quot;http://www.uni-log.org/&quot;&gt;4th UNILOG 2013&lt;/a&gt; in Rio de
Janeiro.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Pacote R dicionariosIBGE</title>
   <link href="http://arademaker.github.com/blog/2012/09/20/pacote-dicionariosIBGE.html"/>
   <updated>2012-09-20T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2012/09/20/pacote-dicionariosIBGE</id>
   <content type="html">&lt;p&gt;Disponibilizei hoje no github e submeti para o CRAN a versão 1.5 do
pacote &lt;a href=&quot;https://github.com/arademaker/dicionariosIBGE/&quot;&gt;dicionariosIBGE&lt;/a&gt;.
Este pacote contém os dicionários das principais pesquisas do
&lt;a href=&quot;http://www.ibge.gov.br/&quot;&gt;IBGE&lt;/a&gt;: PNAD (1983-2009), POF (1987-1996) e
PME. Também incluimos nesta versão variáveis adicionais com rótulos
para as variáveis categóricas de cada pesquisa.&lt;/p&gt;

&lt;p&gt;Nesta nova versão, incorporamos a função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; original do
pacote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IBGEPesq&lt;/code&gt; que foi desenvolvido no IBGE mas nunca submetido ao
CRAN. O pacote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IBGEPesq&lt;/code&gt; é distribuído pelo IBGE nos CDs e DVDs
vendidos na &lt;a href=&quot;http://loja.ibge.gov.br&quot;&gt;loja virtual do IBGE&lt;/a&gt; e
disponibilizo no site do IBGE. Vide link “Leitura em R” na
&lt;a href=&quot;http://www.ibge.gov.br/home/estatistica/populacao/trabalhoerendimento/pnad2009/microdados.shtm&quot;&gt;página da última PNAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Fizemos várias otimizações na função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; original e
incluímos ainda um argumento extra para leitura dos rótulos das
variáveis categóricas. A partir desta versão do dicionariosIBGE, o
pacote &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IBGEPesq&lt;/code&gt; torna-se desnecessário para leitura dos microdados
das pesquisas do IBGE contempladas pelo dicionariosIBGE.&lt;/p&gt;

&lt;p&gt;Observo, no entanto, que a novidade de disponibilizarmos os rótulos
para as variáveis categóricas e adaptarmos a função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; para
usar os rótulos para construir factors ainda é
experimental. Pessoalmente, questiono sua utilidade em geral. Existem
variáveis categóricas cujos rótulos são realmente úteis. Por exemplo,
variáveis como a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UF&lt;/code&gt; da tabela de PESSOA da PNAD de 2009. Certamente
trabalharmos com um factor com rótulo “Rio de Janeiro” ao invés de
apenas o valor 33 torna o manuseio dos dados mais fácil. No entanto,
existem variáveis categóricas como a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V2927&lt;/code&gt; cujos rótulos são tão
grandes e verbosos que provavelmente não facilitam em nada o trabalho
com os dados. Vide:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; rotpes2009[rotpes2009$cod == &quot;V2927&quot;,]
       cod valor                                  rotulo
1070 V2927     1                     Custaria muito caro
1071 V2927     2                         Era muito longe
1072 V2927     3                     Por falta de provas
1073 V2927     4                         Demoraria muito
1074 V2927     5 Cabia as outras partes iniciarem a acao
1075 V2927     6    Por medo de outras partes envolvidas
1076 V2927     7     por meio de mediacao ou conciliacao
1077 V2927     8                 Nao acredita na justica
1078 V2927     9  Nao sabia que podia utilizar a Justica
1079 V2927    10                                  Outros
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;De qualquer modo, ao invés de usar os rótulos que distribuímos no
pacote, nada impede que os usuários do pacote de: (1) passarem para a
função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;le.pesquisa&lt;/code&gt; seus próprios data.frames com rótulos, bastando
fornecer um data.frame com os rótulos no formato esperado (três
colunas: cod, valor, rotulo); ou (2) passar o valor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NULL&lt;/code&gt; para os
rótulos (default da função) retornando os dados como numéricos e/ou
string e não factors.&lt;/p&gt;

&lt;p&gt;O pacote foi desenvolvido e testado no MacOS mas deve funcionar sem
problemas no Linux e Windows.&lt;/p&gt;

&lt;p&gt;Finalmente, cabe destacar que desenvolvemos o pacote usando os
dicionários que encontramos nos CDs e DVDs disponíveis na FGV. Já
tivemos casos de diferentes CDs da mesma pesquisa, adquiridos em
diferentes momentos do IBGE, terem conteúdos diferentes (diferenças
nos dicionários e diferenças nos arquivos de dados). O IBGE parece
produzir os CDs e DVDs das pesquisas por demanda, o que pode explicar
as diferenças entre CDs da mesma pesquisa.&lt;/p&gt;

&lt;p&gt;A falta de padrão do IBGE na distribuição dos arquivos das pesquisas é
um grande problema para os pesquisadores e atrabalha bastante nossa
iniciativa de facilitar a leitura dos dados do IBGE em R. Para
contornar os problemas, poderíamos tentar distribuir os dados do IBGE
já em formato RData, para carga no R diretamente. No entanto, além das
possíveis questões legais (não está claro no site do IBGE qual a
licença de uso adotada pelo IBGE), certamente a credibilidade dos
dados é maior se os dados são obtidos diretamente do site ou mídia do
IBGE.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>O problema dos galões</title>
   <link href="http://arademaker.github.com/blog/2012/05/06/problema-dos-galoes.html"/>
   <updated>2012-05-06T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2012/05/06/problema-dos-galoes</id>
   <content type="html">&lt;p&gt;Na primeira prova do curso de &lt;a href=&quot;/CA-2012-1/&quot;&gt;estrutura de dados&lt;/a&gt; que
dei este ano, coloquei a seguinte questão para os alunos.&lt;/p&gt;

&lt;p&gt;Nós temos 3 galões de tamanho 10, 7 e 4 litros. Os galões de 7 e 4
litros começam cheios e o galão de 10 litros vazio. Só podemos
realizar um tipo de operação com os galões: derramar todo o conteúdo
de um galão em outro, parando apenas quando o galão sendo derramado
ficar vazio ou o destino ficar cheio. Queremos saber se existe uma
sequência de operações que termine com 2 litros no galão de 7 ou 4
litros. Pede-se:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Modele o problema como um problema de grafo, descrevendo
precisamente a definição do grafo envolvido e descrevendo a solução
em função do grafo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Qual algorítmo de grafo deverá ser usado?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Este é um problema clássico que pode ser encontrado em vários livros
de estrutura de dados e algorítmos (Ex 3.8 do Algorithms by Dasgupta,
Papadimitriou e Vazirani
&lt;a href=&quot;http://www.amazon.com/Algorithms-Sanjoy-Dasgupta/dp/0073523402&quot;&gt;Amazon&lt;/a&gt;,
&lt;a href=&quot;http://www.cs.berkeley.edu/~vazirani/algorithms.html&quot;&gt;site do livro&lt;/a&gt;,
&lt;a href=&quot;http://demonstrations.wolfram.com/WaterPouringProblem/&quot;&gt;Wolfram&lt;/a&gt;). Trata-se
de ver os estados do sistema (a quantidaade de líquido em cada galão)
como um nó de um grafo. As arestas representam as possíveis transições
de estado após transferência de líquido entre dois galões.&lt;/p&gt;

&lt;p&gt;Um formalismo bem interessante para implementar a solução deste
problema é lógica de reescrita. Em particular, resolvi usar
&lt;a href=&quot;http://maude.cs.uiuc.edu/&quot;&gt;Maude&lt;/a&gt;. Maude é uma implementação bastante
eficiente e conhecida de lógica de reescrita, o nome refere-se a
linguagem e ao sistema ao mesmo tempo.&lt;/p&gt;

&lt;p&gt;A implementação em Maude é muito simples:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mod GAL is
  inc INT .

  sorts Galon System .
  subsort Galon &amp;lt; System .

  op &amp;lt;_,_&amp;gt; : Int Int -&amp;gt; Galon .
  op __ : System System -&amp;gt; System [assoc comm id: null] . 
  op null : -&amp;gt; System .

  vars N1 N2 M1 M2 : Int .

  crl [transfer-1] :
     &amp;lt; N1 , M1 &amp;gt; &amp;lt; N2 , M2 &amp;gt; =&amp;gt; 
     &amp;lt; 0 , M1 &amp;gt; &amp;lt; (N1 + N2) , M2 &amp;gt; 
   if N1 &amp;lt; (M2 - N2) .

  crl [transfer-2] :
     &amp;lt; N1 , M1 &amp;gt; &amp;lt; N2 , M2 &amp;gt; =&amp;gt; 
     &amp;lt; (N1 - (M2 - N2)) , M1 &amp;gt; &amp;lt; M2 , M2 &amp;gt; 
   if N1 &amp;gt; (M2 - N2) .

  op initial : -&amp;gt; System .
  eq initial = &amp;lt; 0 , 10 &amp;gt; &amp;lt; 7 , 7 &amp;gt; &amp;lt; 4 , 4 &amp;gt; .
endm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Trata-se de uma especificação algébrica do estado do sistema como um
multiset de galões onde cada galão é uma dupla de números inteiros:
primeiro componente é a quantidade de líquido e segundo a
capacidade. A operação &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initial&lt;/code&gt; serve apenas para criar uma constante
que representa o estado inicial do sistema.&lt;/p&gt;

&lt;p&gt;As duas regras de reescrita fazem deste módulo um módulo de sistema
ou, mais formalmente, uma etoria de reescrita. Estas regras
implementam as duas possibilidades de transferência de líquido entre
um galão e outro. Na primeira regra, todo o líquido é transferido de
um galão para outro esvaziando o galão origem. Na segunda, a
transferência é interrompida quando o galão destino torna-se
cheio. Isto poderia ainda ser simplificado para uso de apenas uma
regra, certo?!&lt;/p&gt;

&lt;p&gt;O interessante de implementar em Maude é que as regras de reescrita
podem ser entendidas, computacionalmente, como transições de estado de
um sistema. Logicamente, as regras podem ser entendidas como regras de
inferência de um sistema de reescrita.&lt;/p&gt;

&lt;p&gt;Na prática, podemos aplicar as reescritas de estado usando o comando
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rew&lt;/code&gt; e realizar uma busca por estados a partir de um estado inicial
usando o comando &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;search&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Para testarmos a implementação, podemos aplicar algumas regras a
partir do estado initial. Por exemplo, para aplicarmos 4 regras de
reescrita a partir do estado inicial usamos:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Maude&amp;gt; rew [4] initial .
rewrite [4] in GAL : initial .
rewrites: 33 in 0ms cpu (0ms real) (131474 rewrites/second)
result System: &amp;lt; 0,4 &amp;gt; &amp;lt; 4,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note-se que não controlamos que regras são aplicadas (estratégia de
aplicação). Para encontrarmos a solução, usamos:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;search initial =&amp;gt;* &amp;lt; 2 , X:Int &amp;gt; S:System such that X:Int &amp;lt; 10 .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Este comando acima efetua uma busca (BFS, busca em largura), a partir
do estado inicial, por algum estado atingido com zero ou mais
transições (aplicações de alguma das duas regras que definimos), por
um estado onde exista algum galão com dois litros e cuja capacidade
seja menor que 10.&lt;/p&gt;

&lt;p&gt;A resposta do sistema são 4 possíveis soluções:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Solution 1 (state 16)
states: 17  rewrites: 501 in 3ms cpu (3ms real) (139244 rewrites/second)
S:System --&amp;gt; &amp;lt; 2,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
X:Int --&amp;gt; 4

Solution 2 (state 18)
states: 19  rewrites: 559 in 4ms cpu (4ms real) (127334 rewrites/second)
S:System --&amp;gt; &amp;lt; 0,7 &amp;gt; &amp;lt; 9,10 &amp;gt;
X:Int --&amp;gt; 4

Solution 3 (state 19)
states: 20  rewrites: 610 in 5ms cpu (5ms real) (119210 rewrites/second)
S:System --&amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 5,10 &amp;gt;
X:Int --&amp;gt; 7

Solution 4 (state 20)
states: 21  rewrites: 627 in 5ms cpu (5ms real) (112164 rewrites/second)
S:System --&amp;gt; &amp;lt; 0,4 &amp;gt; &amp;lt; 9,10 &amp;gt;
X:Int --&amp;gt; 7

No more solutions.
states: 21  rewrites: 721 in 6ms cpu (6ms real) (109824 rewrites/second)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para examinarmos a primeira solução, pedimos para o sistema mostrar o
caminho, isto é, as reescritas executadas.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Maude&amp;gt; show path 16 .
state 0, System: &amp;lt; 0,10 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 7,7 &amp;gt;
===[ crl ... [label transfer-1] . ]===&amp;gt;
state 1, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 4,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 4, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 1,7 &amp;gt; &amp;lt; 10,10 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 8, System: &amp;lt; 1,7 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 6,10 &amp;gt;
===[ crl ... [label transfer-1] . ]===&amp;gt;
state 12, System: &amp;lt; 0,4 &amp;gt; &amp;lt; 5,7 &amp;gt; &amp;lt; 6,10 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 14, System: &amp;lt; 2,10 &amp;gt; &amp;lt; 4,4 &amp;gt; &amp;lt; 5,7 &amp;gt;
===[ crl ... [label transfer-2] . ]===&amp;gt;
state 16, System: &amp;lt; 2,4 &amp;gt; &amp;lt; 2,10 &amp;gt; &amp;lt; 7,7 &amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;O estado encontrado é onde o galão de 4 litros está com 2 litros, o
galão de 7 litros está completo e o galão de 10 litros está com 2
litros.&lt;/p&gt;

&lt;p&gt;Bem legal, não acham!?&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Lattes to BibTeX</title>
   <link href="http://arademaker.github.com/blog/2012/02/15/lattes-to-bibtex.html"/>
   <updated>2012-02-15T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2012/02/15/lattes-to-bibtex</id>
   <content type="html">&lt;p&gt;Disponibilizei hoje online no github uma transformação do &lt;a href=&quot;http://lattes.cnpq.br/&quot;&gt;Lattes&lt;/a&gt; para
&lt;a href=&quot;http://en.wikipedia.org/wiki/BibTeX&quot;&gt;BibTeX&lt;/a&gt;, vejam o repositório:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/arademaker/SLattes&quot;&gt;http://github.com/arademaker/SLattes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Eu acabei fazendo esta transformação por dois motivos. O primeiro para
meu uso pessoal, eu já estava querendo faz muito tempo conseguir gerar
um BibTex com minhas produções. O segundo foi como parte do projeto
Semantic Lattes. A idéia é que uma transformação dos dados do Lattes
para algum padrão de referências como o
&lt;a href=&quot;http://www.loc.gov/standards/mods/&quot;&gt;XML/MODS da Library of Congress&lt;/a&gt;,
ajuda a validar os dados do Lattes.&lt;/p&gt;

&lt;p&gt;Intruções de como usar o transformador estão no README do repositório,
mas como o texto lá está em inglês, segue a idéia geral. O que fiz foi
uma transformação XSLT do XML/Lattes para o XML/MODS. Este último pode
ser então facilmente convertido para BibTex usando o xml2bib, programa
do pacote &lt;a href=&quot;http://sourceforge.net/p/bibutils/&quot;&gt;Bibutils&lt;/a&gt; disponivel no
Linux e no MacOS (MacPorts). Para executar a transformação e validar
um XML em relação ao seu DTD/Schema, ainda são necessários os
programas xsltproc e xmllint.&lt;/p&gt;

&lt;h2 id=&quot;instalação-dos-programas-necessários&quot;&gt;Instalação dos programas necessários&lt;/h2&gt;

&lt;p&gt;Para quem usa o MacPorts no MacOS:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo port -v install bibtool bibutils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;O xmllint e xsltproc já estão instalados no MacOS (acho que no XCode).&lt;/p&gt;

&lt;p&gt;Para quem usa Linux/Ubuntu:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install bibtool bibutils xsltproc libxml2-utils
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Para quem usa Windows, consultar o site destas ferramentas, não
trivial! Esquece o Windows e instala o Ubuntu! ;-)&lt;/p&gt;

&lt;h2 id=&quot;usando-a-transformação&quot;&gt;Usando a transformação&lt;/h2&gt;

&lt;p&gt;Com as ferramentas instaladas, o primeiro passo é acesssar o sistema
&lt;a href=&quot;http://lattes.cnpq.br&quot;&gt;Lattes do CNPq&lt;/a&gt;, link &lt;strong&gt;atualizar&lt;/strong&gt;, logar-se
no sistema e escolher a opção de exportar para XML. Será iniciado o
download de um arquivo ZIP. Abra ZIP e extraia o arquivo XML dentro
dele que tem mesmo nome, seu lattes ID. Imagine então que vc renomeou
este arquivo XML para &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LATTES.xml&lt;/code&gt; e o moveu para o mesmo diretório
onde está o arquivo com &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lattes2mods.xsl&lt;/code&gt; que você pegou do
repositório no github.&lt;/p&gt;

&lt;p&gt;Agora basta rodar:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xsltproc lattes2mods.xsl LATTES.xml &amp;gt; LATTES.mods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E em seguida:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xml2bib -b -w LATTES.mods &amp;gt; LATTES.bib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mas o interessante é antes de rodar o xml2bib, validar o arquivo mods
gerado contra o
&lt;a href=&quot;http://www.loc.gov/standards/mods/mods-schemas.html&quot;&gt;XML Schema do MODS&lt;/a&gt;,
disponibilizado no site da Biblioteca do Congresso Americano, baixe a
versão 3.4:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;xmllint --schema mods.xsd LATTTES.mods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Esta validação serve para verificar não apenas erros na estrutura do
arquivo, que seriam bugs no meu código, mas também erros nos dados, em
função de informações erradas (faltantes, em lugar errado etc) no
Lattes.&lt;/p&gt;

&lt;p&gt;Comentários são sempre bem vindos. Problemas podem ser reportados
diretamente no &lt;a href=&quot;https://github.com/arademaker/SLattes/issues&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>R Package SSOAP</title>
   <link href="http://arademaker.github.com/blog/2012/01/02/package-SSOAP.html"/>
   <updated>2012-01-02T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2012/01/02/package-SSOAP</id>
   <content type="html">&lt;p&gt;Last year, during a summer course that I gave at FGV, I taught the
students how to use SOAP protocol do retrive data from
&lt;a href=&quot;http://bcb.gov.br/&quot;&gt;Banco Central do Brasil&lt;/a&gt; using R. BCB has a
system called SGS (Sistema Gerenciador de Séries Temporais) that has a
SOAP interface.&lt;/p&gt;

&lt;p&gt;At that time, the &lt;a href=&quot;http://www.omegahat.org/SSOAP/&quot;&gt;package SSOAP&lt;/a&gt; had
a small bug that I contributed to fix. Today I found that my
contribution was incorporated in
&lt;a href=&quot;http://www.omegahat.org/SSOAP/Changes.html&quot;&gt;version 0.5-5&lt;/a&gt; of this
package whish makes my
&lt;a href=&quot;https://github.com/arademaker/SSOAP&quot;&gt;repository at github&lt;/a&gt; outdated.&lt;/p&gt;

&lt;p&gt;It took my a couple of minutes to test the new version of this
package. Since I am running the last version of R, 2.14, the general
procedure for install packages didn’t work.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; install.packages(&quot;SSOAP&quot;)
Warning message:
In getDependencies(pkgs, dependencies, available, lib) :
  package ‘SSOAP’ is not available (for R version 2.14.0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I finnaly figured out how to install the last version from source
using the Omegahat repository version with the command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; install.packages(&quot;SSOAP&quot;, repos = &quot;http://www.omegahat.org/R&quot;, 
                   dependencies = TRUE, 
                   type = &quot;source&quot;)
trying URL &apos;http://www.omegahat.org/R/src/contrib/SSOAP_0.8-1.tar.gz&apos;
Content type &apos;application/x-gzip&apos; length 195424 bytes (190 Kb)
opened URL
==================================================
downloaded 190 Kb

* installing *source* package ‘SSOAP’ ...
** R
** inst
** preparing package for lazy loading
Creating a new generic function for ‘help’ in package ‘SSOAP’
Warning in .NonstandardGenericTest(body(fdef), name, stdGenericBody) :
  the supplied generic function definition for toSOAP does not
  seem to call &apos;standardGeneric&apos;; no methods will be dispatched!
** help
*** installing help indices
** building package indices ...
** testing if installed package can be loaded

* DONE (SSOAP)

The downloaded packages are in
	‘/private/var/.../downloaded_packages’
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After that, I was prepared to actually test the package running the
code that I created during the
&lt;a href=&quot;https://github.com/arademaker/IR-2011/&quot;&gt;course&lt;/a&gt; (lesson 7 directory
aula-07). But some changes in RCurl package requires a change in how
we ask for not verify the ssl certificate. That is, I had to replace
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssl.verifypeer = FALSE&lt;/code&gt; argument by a list of options in the call
of the function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ff@functions$getValoresSeriesXML&lt;/code&gt;. The last version
of this script is now available as a gist here:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/1550651.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssl.verifypeer&lt;/code&gt; argument is necessary because the
certificate used in BCB website is invalid! What a shame!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2012-01-02-bcb-certificate.png&quot; alt=&quot;BCB certificate&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>GitHub Pages and Jekyll plugins</title>
   <link href="http://arademaker.github.com/blog/2011/12/01/github-pages-jekyll-plugins.html"/>
   <updated>2011-12-01T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/12/01/github-pages-jekyll-plugins</id>
   <content type="html">&lt;p&gt;Everyone that use &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; and wants to host the site at GitHub should know that GitHub Pages does not allow custom plugins, right? Using Jekyll for a little more than a blog site, like &lt;a href=&quot;http://emap.fgv.br&quot;&gt;EMAp/FGV&lt;/a&gt; will require plugins. In my case, avoid the use of custom plugins is not an option.&lt;/p&gt;

&lt;p&gt;The solution is trivial, one has to run Jekyll locally and post the produzed files into a master branch of a git repo, following the conventions described at &lt;a href=&quot;http://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; por person and organization pages. The problem that I faced was to choose the best way to organize and keep tracking of the source and produzed files.&lt;/p&gt;

&lt;p&gt;I read a couple of posts with possible solutions. The solution by &lt;a href=&quot;http://charliepark.org/jekyll-with-plugins/&quot;&gt;Charlie Park&lt;/a&gt; force us to have two distinct git repositories. I don’t like this approach because of that. Our website will be maintained by more than one person, having two distinct repositories since that I will not using all git features. The solution by &lt;a href=&quot;http://tech.hugr.fr/blog/2011/08/07/how-to-host-a-jekyll-app-on-github-pages-with-plugins/&quot;&gt;Jean Denis&lt;/a&gt; is a little bit better but keeps me thinking about why he need to keep the produced files under version control in the two branches, gh-pages and the master.&lt;/p&gt;

&lt;p&gt;My final solution is to keep the directory produced by jekyll out of git control. This simple thing allows me to switch the branches, from source to master, and still have access to the produced files, directory &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site&lt;/code&gt;. Once in the master branch, I only have to move the files under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_site/&lt;/code&gt; to the root directory in the master branch and update the master branch before push it to GitHub.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git checkout source
// do whatever you need
git status / git add / git commit
jekyll
checkout master
cp -r _site/* . &amp;amp;&amp;amp; rm -rf _site/ &amp;amp;&amp;amp; touch .nojekyll
git status &amp;gt; git add &amp;gt; git commit
git push -all origin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>Comentários sobre ECLM 2011 e ISWC 2011 (1/2)</title>
   <link href="http://arademaker.github.com/blog/2011/11/27/ultimas-conferencias.html"/>
   <updated>2011-11-27T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/11/27/ultimas-conferencias</id>
   <content type="html">&lt;p&gt;Desde que voltei da minha última viagem para participar da
&lt;a href=&quot;http://weitz.de/eclm2011/&quot;&gt;ECLM 2011&lt;/a&gt; e
&lt;a href=&quot;http://iswc2011.semanticweb.org/&quot;&gt;ISWC 2011&lt;/a&gt;, estou pensando em
escrever sobre o assunto. Por estar escrevendo em português, acho que
tenho lá ainda alguma chance de contribuir com algo novo. Neste post,
vou falar da ISWC 2011. Vou começar listando alguns blogs que já
escreveram sobre estas conferências em inglês.&lt;/p&gt;

&lt;p&gt;Das duas conferências, certamente a ISWC é a maior e, por isso, também
foi a mais comentada. São vários os posts de pessoas que escreveram
sobre ela. Ivan Herman escreveu
&lt;a href=&quot;http://ivan-herman.name/2011/11/02/some-notes-on-iswc2011.../&quot;&gt;Some notes on ISWC2011&lt;/a&gt;.
Também sobre a ISWC-2011 vale a pena ler o post
&lt;a href=&quot;http://blog.phenoscape.org/2011/11/03/notes-from-iswc-2011/&quot;&gt;Notes from ISWC 2011&lt;/a&gt;
e a série de 5 posts de
&lt;a href=&quot;http://semanticweb.com/report-from-day-1-at-iswc_b24150&quot;&gt;Juan Sequeda&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Como Ivan escreveu no post dele, diferentes pessoas certamente
relatarão diferentes experiências da ISWC 2011. Me chamou atenção os
comentários dele sobre os trabalhos apresentados relacionados à
visualização de ontologias. Embora o assunto me interesse, acho que
perdi os workshops ou seções relacionadas a este tema. Foi bom ter
lido o post dele.&lt;/p&gt;

&lt;p&gt;Da minha parte, gostei bastante da ISWC 2011. A conferência foi bem
organizada, o hotel muito bom e a cidade muito agradável. Dos
workshops, o mais interessante para mim certamente foi o
&lt;a href=&quot;http://www.om2011.ontologymatching.org/&quot;&gt;Ontology Matching&lt;/a&gt;, afinal,
é o assunto que mais me interessa e sobre o qual tenho artigos
publicados com Isabel Cafezeiro e Hermann. Assistir ao workshop me
motivou a voltar a este assunto e tentar implementar efetivamente as
idéias que formalizamos nos artigos.&lt;/p&gt;

&lt;p&gt;Das seções da conferência, gostei bastante da “Ontology Matching,
Mapping” e da “KR - Semantics”. No mais, vale dizer que os posters
também estavam ótimos e a idéia de cada poster ser apresentado em 1
minuto foi bem divertida embora apenas alguns apresentadores tenham
entendido o espírito da coisa! Das apresentações dos convidados, o que
mais fez sucesso foi Frank van Harmelen com o título
&lt;a href=&quot;http://www.cs.vu.nl/~frankh/spool/ISWC2011Keynote/&quot;&gt;10 Years of Semantic Web: does it work in theory?&lt;/a&gt;.
Para mim, sendo minha área de pesquisa exatamente lógicas e, em
particular, nos últimos anos, lógicas de descritivas, ter uma
apresentação sugerindo que a comunidade de web semântica deve voltar a
atenção para os fundamentos teóricos da área, é bastante motivador.
Infelizmente assisti apenas parte da apresentação. O painel “Semantic
Web Death Match” foi meio sem graça, embora na sala, assistindo as
discussões, tenho que dizer que foi uma experiência única participar
do painel pelo twitter. No twitter as discussões foram até mais
interessantes. Finalmente, da seção “MANCHustifications and
Provenance”, tive a idéia de revisar o texto da minha tese que será
publicado pela Springer. A idéia é que pode-se obter diretamente de
uma prova formal, usando os sistemas dedutivos que apresento para
algumas DL em minha tese, a tal “justification of an entailment”. O
termo refere-se ao conjunto mínimo de axiomas usados para justificar
uma conclusão lógica (qual seria a melhor tradução para
“entailment”?). Obrigado
&lt;a href=&quot;http://manchester.academia.edu/SamanthaBail&quot;&gt;Samantha Bail&lt;/a&gt; por me
ajudar a confirmar a impressão que tive durante a apresentação dos
trabalhos na seção.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Gráficos de séries temporais no R</title>
   <link href="http://arademaker.github.com/blog/2011/10/31/time-series-R.html"/>
   <updated>2011-10-31T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2011/10/31/time-series-R</id>
   <content type="html">&lt;p&gt;Pergunta de dois alunos: como postar duas séries temporáis usando o
ggplot? Resolvi responder usando este post. Vou aproveitar então para
mostrar como postar séries temporáis usando o ggplot, lattice e o plot
padrão do R.&lt;/p&gt;

&lt;h2 id=&quot;criando-os-dados&quot;&gt;Criando os dados&lt;/h2&gt;

&lt;p&gt;Para começar, criei um objeto série temporal conforme exemplo da função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts&lt;/code&gt;
do R. Este objeto na realidade é uma matriz, são três séries temporais
ou uma série temporal multivariável.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; dados &amp;lt;- ts(matrix(rnorm(300), 100, 3), start=c(1961, 1), 
              frequency=12)
&amp;gt; dados[1:4,]
       Series 1   Series 2   Series 3
[1,]  1.4165848  2.1049293  1.0155993
[2,] -0.4264193 -0.2730903  0.8754992
[3,]  0.5120809 -0.4023986  1.9757084
[4,]  0.1375277 -0.5043973 -0.7795633
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;plot-básico&quot;&gt;Plot básico&lt;/h2&gt;

&lt;p&gt;Se usarmos o comando padrão de plot do pacote basic do R, temos o
seguinte plot.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; plot(ts)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-basic.png&quot; alt=&quot;plot-basic&quot; title=&quot;plot basic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Obviamente, neste e nos próximos exemplos, estou mostrando o uso mais
básico das funções, sem me preocupar com nenhum ajuste de formatação,
legenda, rótulos, cores etc.&lt;/p&gt;

&lt;h2 id=&quot;no-pacote-lattice&quot;&gt;No pacote lattice&lt;/h2&gt;

&lt;p&gt;No pacote Lattice, temos a função xyplot que contém vários
parâmetros para geração do gráfico. Explorei apenas a forma de gerar
uma série por painel e todas as séries em um painel.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; xyplot(dados)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-lattice-1.png&quot; alt=&quot;plot-lattice-1&quot; title=&quot;plot lattice&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; xyplot(dados, superpose = TRUE) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-lattice-2.png&quot; alt=&quot;plot-lattice-2&quot; title=&quot;plot lattice&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A página de help da função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xyplot&lt;/code&gt; contém muito mais informação sobre
os demais parâmetros da função. A função &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xyplot&lt;/code&gt; também tem um método
específico para lidar com objetos da classe Zoo do pacote
&lt;a href=&quot;http://cran.r-project.org/web/packages/zoo/index.html&quot;&gt;zoo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;no-pacote-ggplot&quot;&gt;No pacote ggplot&lt;/h2&gt;

&lt;p&gt;Finalmente, como fazer no ggplot? Este foi o mais difícil. Demorei
bastante a encontrar referências via google. As mais relevantes que
encontrei foram:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://had.co.nz/ggplot2/scale_date.html&quot;&gt;scale_date&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://learnr.wordpress.com/2009/05/05/ggplot2-two-time-series-with-different-dates/&quot;&gt;ggplot2: two time series with different dates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://goo.gl/Kr5wP&quot;&gt;Using ggplot, how to have the x-axis of time series plots set up automatically?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;O último link é a dica de como transformar um objeto &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts&lt;/code&gt; em um
data.frame:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;datas &amp;lt;- seq(as.Date(paste(c(start(dados),1), collapse=&quot;/&quot;)), 
             by = &quot;month&quot;, length.out = length(dados))
dados.df &amp;lt;- data.frame(date = datas, value = dados)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E finalmente consegui plotar uma série das três com:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; ggplot(data=dados.df) + geom_line(aes(date, value.Series.1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-ggplot-single.png&quot; alt=&quot;plot-ggplot&quot; title=&quot;plot ggplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Para conseguir em um único gráfico as três séries temporais, tive mais
trabalho. Primeiro em transformar os dados em um data.frame que
pudesse ser entendido pelo ggplot.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; tmp &amp;lt;- stack(dados.df, select = -1)
&amp;gt; tmp$date &amp;lt;- dados.df[,1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E finalmente, o novo plot:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; ggplot(data=dados.df) + geom_line(aes(date, value.Series.1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/2011-10-31-plot-ggplot-multi.png&quot; alt=&quot;plot-ggplot-multi&quot; title=&quot;plot ggplot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusão&quot;&gt;Conclusão&lt;/h2&gt;

&lt;p&gt;Não existe pacote melhor ou pior, cada um é mais adequado para cada
situação. No particular problema de plotar multiplas séries temporais,
sem nenhuma dúvida, preferi a facilidade do lattice em lidar com
objetos &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts&lt;/code&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Verifying the ISSN's check digit in Common Lisp</title>
   <link href="http://arademaker.github.com/blog/2011/09/13/checking-issn-lisp.html"/>
   <updated>2011-09-13T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2011/09/13/checking-issn-lisp</id>
   <content type="html">&lt;p&gt;The code below is my first approach to create a lisp function that
test the ISSN &lt;a href=&quot;http://en.wikipedia.org/wiki/Check_digit&quot;&gt;check
digit&lt;/a&gt;. Unfortunately, the
code runs only in Allegro CL due the requirement of regexp2
library. Nevertheless, the regexp2 library is easly replaced by an opensource
regexp library, which makes this not a real constraint.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/1215526.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;That is it! Comments are welcome!&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Construindo tabelas verdade no R</title>
   <link href="http://arademaker.github.com/blog/2011/03/02/tabela-verdade-no-R.html"/>
   <updated>2011-03-02T00:00:00-03:00</updated>
   <id>http://arademaker.github.com/blog/2011/03/02/tabela-verdade-no-R</id>
   <content type="html">&lt;p&gt;Durante a preparação de alguns exercícios de lógica, me deparei com a
necessidade de construir tabelas verdade. Lembrando do pacote xtable
do R, pensei como seria construir uma tabela verdade usando o R. Minha
solução em R está no github, gist abaixo, com o exemplo de como seria
a tabela da expressão&lt;/p&gt;

\[\neg (a \lor b) \lor c\]

&lt;script src=&quot;https://gist.github.com/852194.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Observem que a saída do primeiro comando xtable é bastante bizarra,
certamente um bug do pacote xtable. Editei a saída mantendo apenas o
início da tabela gerada e incluíndo “…” no final.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;xtable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;c&quot;&gt;% latex table generated in R 2.11.1 by xtable 1.5-6 package&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% Wed Mar  2 23:18:40 2011&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{table}&lt;/span&gt;[ht]
&lt;span class=&quot;nt&quot;&gt;\begin{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{tabular}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;rllll&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; a &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; b &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; ! (a &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; b) &lt;span class=&quot;p&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;$&lt;/span&gt; c &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
1 &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
    c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
2 &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c(FALSE, FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE) &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; 
...
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{tabular}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{table}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Para fazer com que cada célula da tabela tivesse apenas o valor lógico
correspondente, não um vetor, converti o data.frame em caracteres
antes de usar o xtable. Minha solução original convertia em números,
Bruno Lopes me lembrou de converter em caracteres diretamente.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{table}&lt;/span&gt;[ht]
&lt;span class=&quot;nt&quot;&gt;\begin{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\begin{tabular}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;llll&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
a &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; b &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; c &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\~&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt; (a OR b) OR c &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; FALSE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
  TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt; TRUE &lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt; 
   &lt;span class=&quot;k&quot;&gt;\hline&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{tabular}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{center}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{table}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</content>
 </entry>
 
 <entry>
   <title>Barras e Linhas no R</title>
   <link href="http://arademaker.github.com/blog/2010/11/18/barras-e-linhas.html"/>
   <updated>2010-11-18T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2010/11/18/barras-e-linhas</id>
   <content type="html">&lt;p&gt;Ontem um aluno me perguntou como produzir um gráfico de barras com
linhas. Minha primeira idéia foi recorrer a pacotes como
&lt;a href=&quot;http://cran.r-project.org/web/packages/lattice/&quot;&gt;Lattice&lt;/a&gt; ou
&lt;a href=&quot;http://had.co.nz/ggplot2/&quot;&gt;ggplot2&lt;/a&gt;, imaginando tratar-se de um
gráfico pouco usual. Depois de um pouco de pesquisa, acabei
descobrindo que o gráfico em questão não é tão usual assim e pode ser
facilmente produzido com os comandos básicos de gráficos do R.&lt;/p&gt;

&lt;p&gt;Digamos que seus dados sejam uma data.frame composto por duas
variáveis.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;O que desejamos é representar no gráfico os valores da variável &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt;
como barras e os valores da variável &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; como pontos conectados por
linhas. O comando abaixo produz o gráfico de barras e retorna um
vetor com as coordenadas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; dos meios das barras produzidas.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Agora é fácil criar os pontos e linhas:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dados&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Image que seus dados são temporais, onde cada observação está
relacionada a uma ano. Pode-se incluir os anos como rótulos do eixo
“x” com o comando:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2009&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/2010-11-18-fig.png&quot; alt=&quot;bar-and-line&quot; title=&quot;bar and line&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Datasets no Brasil</title>
   <link href="http://arademaker.github.com/blog/2010/01/30/datasets.html"/>
   <updated>2010-01-30T00:00:00-02:00</updated>
   <id>http://arademaker.github.com/blog/2010/01/30/datasets</id>
   <content type="html">&lt;p&gt;Estão começando a surgir no Brasil iniciativas para real
transparência no acesso aos dados do governo. A
estruturação dos dados em formatos abertos (RDF, CSV
etc), acompanhados de metadados e indexados em interfaces de busca e
navegação que facilitem o download dos arquivos
são para mim as condições necessárias para
o livre acesso à informação. Uma destas
iniciativas é o &lt;a href=&quot;http://www.lexml.gov.br/&quot;&gt;LeXML&lt;/a&gt;.  Nos EUA
existem os projetos &lt;a href=&quot;http://data.gov/&quot;&gt;data.gov&lt;/a&gt; e
&lt;a href=&quot;http://datasf.org/&quot;&gt;datasf.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Na linha oposta, estão serviços como o da ANP de
levantamento de &lt;a href=&quot;http://www.anp.gov.br/preco/&quot;&gt;preços de
combustíveis&lt;/a&gt;. Que tipo de
pesquisa pode ser feita com estes dados? Para começar,
só a construção de um datasets a partir deste
site demanda um bom trabalho de desenvolvimento de um crawler e
transformadores. Afinal, eu me pergunto, qual é o objetivo
deste site da ANP? Se for para um cidadão comum pesquisar qual
o melhor posto para abastecer seu carro, a interface deixa a
desejar. Se for para a sociedade acompanhar os preços de
combustíveis no Brasil, isto implica acessibilidade as
séries de dados, e todos meus comentários anteriores
fazem sentido, não?&lt;/p&gt;

</content>
 </entry>
 
 
</feed>
