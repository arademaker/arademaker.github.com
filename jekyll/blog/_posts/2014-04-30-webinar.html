---
layout: post
title: VIVO Apps and Tools Webinar
tags:
 - vivo
 - lattes
---

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Yerterday I presented for the Apps and Tools working group my workflow
to prepared data to be inserted into FGV VIVO instance. Since some
people asked be to share the links and the file that I used to guide
my presentation, I made this post. 
</p>

<p>
This page is generated from a <a href="http://orgmode.org">org file</a> that I export to HTML and
further processed with <a href="http://jekyllrb.com">Jekyll</a>. The process of use org files with
jekyll is outlined <a href="http://orgmode.org/worg/org-tutorials/org-jekyll.html">here</a>. I am stil not very confortable with this
workflow but I am using it for my personal website and for the
websites of the courses that I teach at FGV.
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> My Toolset</h2>
<div class="outline-text-2" id="text-2">
<p>
This is the non comprehensive list of tools that I use. I am listing
here the main tools that come up into my mind that should be
interesting to others.
</p>

<ul class="org-ul">
<li><a href="http://www.gnu.org/software/emacs/">Emacs</a>
</li>
<li><a href="http://orgmode.org">Org-Mode</a>
</li>
<li><a href="http://common-lisp.net/project/slime/">Slime</a>
</li>
<li>Common Lisp compilers and interpreters: <a href="http://franz.com/products/allegro-common-lisp/">Allegro CL</a>, <a href="http://www.sbcl.org">SBCL</a> and <a href="http://abcl.org">ABCL</a>
  (Lisp on JVM, so I can use Java RDF libraries).
</li>
<li><a href="http://www.w3.org/2000/10/swap/doc/cwm.html">CWM</a>
</li>
<li><a href="http://franz.com/agraph/allegrograph/">Allegro Graph Triplestore</a>
</li>
<li><a href="http://franz.com/agraph/gruff/">Gruff</a>
</li>
<li>Git
</li>
<li><a href="http://www.r-project.org">R</a>
</li>
<li>Python
</li>
<li><a href="http://xmlsoft.org">xsltproc and xmllint</a>
</li>
<li><a href="http://tidy.sourceforge.net">tidy</a>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Datasources</h2>
<div class="outline-text-2" id="text-3">
<p>
We have to main sources of data: (1) the FGV researchers' curricula
vitae from <a href="http://lattes.cnpq.br">Lattes Platform</a> and; (2) the FGV digital library.
</p>

<p>
During the webinar I shared my screen and presented the web interface
that <a href="http://cnpq.br">CNPq</a> provides for researchers update their resumes. I also shown
<a href="http://lattes.cnpq.br/0675365413696898">my curriculum vitae</a> and discussed why I do not consider Brazilian
Researchers curricula vitae open data given the captcha that blocks
crawlers to get XML files from the Lattes website in a batch mode.
</p>

<p>
I forgot to mention during the webinar but one of my old dreams is to
convice CNPq to add <a href="http://www.w3.org/TR/xhtml-rdfa-primer/">RDFa</a> or <a href="https://schema.org">https://schema.org</a> microformats into the
HTML pages of the curricula vitae. This would not only allow crawlers
to easier process the data but could also facilitate the maintainance
of the system. My current XSLT transformation of Lattes XML to RDF
cloud provide the starting point to RDFa embeeding.
</p>

<p>
The <a href="http://bibliotecadigital.fgv.br/dspace">FGV Digital Library</a> runs Dspace. The publications and collections
metadata are easly collected from Dspace using the <a href="http://www.openarchives.org/">OAI-PMH protocol</a>.
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Lattes XML Files</h2>
<div class="outline-text-2" id="text-4">
<p>
As I said in the previous section, the curricula vitae of Brazilian
Researchers are not open data, that is, they are not public available
in structured format. They are only public available as HTML pages in
the Lattes website, with limited search interface. The only way to
universities and research institutions get the curricula vitae of
their researchers in a structured format is to sign an <a href="http://www.cnpq.br/web/portal-lattes/acordos-institucionais">aggrement</a> with
CNPq. FGV has signed this aggrement and we have one server authorized
to access CNPq web server to retrive the XML files of all curriculae
that have informed any professional activity with FGV.
</p>

<p>
To transform the Lattes files to RDF, I use a XSLT transformation that
I developed few years ago. The XSLT is freely available at github in
the repository <a href="https://github.com/arademaker/SLattes">Semantic Lattes</a>. 
</p>

<p>
In this repo, I made also available the DTD that I try hard to keep
up-to-date. Unfortunately, until recently, CNPq did not make public
annoucement of chances in the structure of the XML files that they
produce, so I had to adapt the DTD whenever I identify changes. I have
just found in the end of <a href="http://www.cnpq.br/web/portal-lattes/extracoes-de-dados">this page</a> that CNPq finally realized the
importance of making available the updated DTD. Nevertheless, the DTD
in the link of this page is outdated. At least, using this DTD to
validate the 489 FGV's curriculae I got more than 100 erros but using
my DTD to validate the same files I got only 2 errors. Considering
that those 489 files were produced my CNPq, we have two options: (1)
the DTD is outdated; or (2) the code that produces the XML files has
bugs. The two erros that I find using my DTD occur only with two
curriculae that were not updated in the last 2 years.
</p>

<p>
After download the XML files, the general idea to process them and
produce the RDF files is outlined in the code below:
</p>

<div class="org-src-container">

<pre class="src src-sh"><span style="color: #a020f0;">for</span> f<span style="color: #a020f0;"> in</span> $<span style="color: #a0522d;">ROOT</span>/ontos/xml/*.xml; <span style="color: #a020f0;">do</span>
    <span style="color: #a0522d;">ID</span>=$(<span style="color: #ff00ff;">basename</span> $<span style="color: #a0522d;">f</span> .xml)
    <span style="color: #483d8b;">echo</span> Processing $<span style="color: #a0522d;">ID</span>
    xmllint --noout --dtdvalid $<span style="color: #a0522d;">REPO</span>/LMPLCurriculo.DTD $<span style="color: #a0522d;">f</span> 2&gt;&gt; error.log
    xsltproc --stringparam ID $<span style="color: #a0522d;">ID</span> $<span style="color: #a0522d;">REPO</span>/lattes.xsl $<span style="color: #a0522d;">f</span> &gt; $<span style="color: #a0522d;">ID</span>.rdf  
<span style="color: #a020f0;">done</span>
</pre>
</div>

<p>
After that, I import the RDF files to Allegro Graph making each
curriculum a separated graph so I can easly identify the provenance of
each triple. The importation is done using the <a href="http://franz.com/agraph/support/documentation/current/agload.html">agload</a> utility. The
load process takes aprox. 2 minutes:
</p>

<pre class="example">
Load finished 487 sources in 00:02:03 (123.02 seconds).  
Triples added: 1,690,538 
Average Rate: 13742.00 tps
</pre>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Deduplication of instances</h2>
<div class="outline-text-2" id="text-5">
<p>
I briefly commented about the deduplication of records during the
webinar. I do have to take care of removing duplicated resources about
the same entity. Considering a thesis defended by and student at FGV
whom have as advisor a professor at FGV. I will have metadata
(triples) about this thesis from three different sources: (1) the RDF
produced from the advisor's curriculum lattes XML; (2) the RDF produce
from the student's curriculum lattes XML; and (3) the RDF obtained
from the FGV Digital Library. 
</p>

<p>
The <a href="http://github.com/arademaker/vivo-code">current code</a> that I use to identify duplicated resources is a
Common Lisp library that is easily used if placed inside the
local-projects directory of a <a href="http://www.quicklisp.org/">Quicklisp</a> instalation.
</p>

<p>
I can write an entire article only about deduplication in RDF. I am
still thinking hard about this problem and really would like to find
better alternatives.  One can note that deduplication of nodes in a
RDF graph should not be done type by type as I am doing now. The rules
to identify resources as being refering the same entity could
dependent each other. That is, the deduplication of instances of
<code>foaf:Person</code> can activate the rule to deduplicate instances of
<code>bibo:Article</code> and vice-versa. It would be better to have a kind of
fixed point transformation in the RDF graph that could keep clustering
nodes until nothing more can be done. As a logician, I am very
interested in approach this problem in a more declarative and
deductive way.
</p>

<p>
I also have to note that <code>owl:sameAs</code> semantics doesn't help here. I
do use <code>owl:sameAs</code> to mark the nodes that should be merged but I have
to merge the nodes after all <code>owl:sameAs</code> triples are produced. I do
this with two SPARQL construct queries:
</p>

<pre class="example">
delete { ?s1 ?p ?o . }
insert { ?s2 ?p ?o . }
where {
  ?s1 owl:sameAs ?s2 .
  ?s1 ?p ?o .
  filter( !sameTerm(?p, owl:sameAs) )
}
</pre>

<pre class="example">
delete { ?x ?p ?o1 . }
insert { ?x ?p ?o2 . }
where {
  ?o1 owl:sameAs ?o2 .
  ?x ?p ?o1 .
  filter( !sameTerm(?p, owl:sameAs) )
}
</pre>

<p>
Note that the filters block the propagation of the <code>owl:sameAs</code>
triples. Anyway, my current code is available for suggestions.
</p>
</div>
</div>

<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> Mapping Lattes RDF to VIVO RDF</h2>
<div class="outline-text-2" id="text-6">
<p>
To map the Lattes RDF model produced by my XSLT to the expected VIVO
RDF model, I have to look carefully to each instance of data. This
mapping is not completed but at this point I have already mapped most
of the data about people, publication, research areas and
departaments.
</p>

<p>
To work on the rules and queries to transform the data, I used to
excellent query and data browsers tools developed by Franz: Gruff and
AllegroGraph WebView. During the webinar I presented both systems.
</p>

<ul class="org-ul">
<li>CWM and file rules.n3 (performance issues, used for prototype)
</li>
<li>CL + SPARQL (show saved queries in Allegro Graph WebView)
<ul class="org-ul">
<li>Problems:
<ul class="org-ul">
<li>blank nodes
</li>
<li>can't repeate a query without duplicate triples
</li>
</ul>
</li>
</ul>
</li>
<li>Next steps: 
<ul class="org-ul">
<li>validation (OWL Reasoner?)
</li>
<li>Better rule approach (like cwm) but with better performance?
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Working with OAI-PMH</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li><a href="http://www.openarchives.org">http://www.openarchives.org</a>
</li>
<li><a href="http://re.cs.uct.ac.za">http://re.cs.uct.ac.za</a>
</li>
<li><a href="http://cran.r-project.org/web/packages/OAIHarvester/index.html">http://cran.r-project.org/web/packages/OAIHarvester/index.html</a>
</li>
<li><a href="https://pypi.python.org/pypi/pyoai/2.4.4">https://pypi.python.org/pypi/pyoai/2.4.4</a>
</li>
</ul>
</div>

<div id="outline-container-sec-7-1" class="outline-3">
<h3 id="sec-7-1"><span class="section-number-3">7.1</span> Repository Exploring</h3>
<div class="outline-text-3" id="text-7-1">
<div class="org-src-container">

<pre class="src src-R"><span style="color: #008b8b;">library</span>(OAIHarvester)
baseurl <span style="color: #008b8b;">&lt;-</span> <span style="color: #8b2252;">"http://bibliotecadigital.fgv.br/oai/request"</span>
oaih_list_metadata_formats(baseurl)
</pre>
</div>

<div class="org-src-container">

<pre class="src src-R" id="sets">sets <span style="color: #008b8b;">&lt;-</span> oaih_list_sets(baseurl)
head(sets)
</pre>
</div>

<div class="org-src-container">

<pre class="src src-R">records <span style="color: #008b8b;">&lt;-</span> oaih_list_records(baseurl, set = series[3,1])
dim(records)
colnames(records)
</pre>
</div>

<div class="org-src-container">

<pre class="src src-R">records[1,<span style="color: #8b2252;">"metadata"</span>]
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-7-2" class="outline-3">
<h3 id="sec-7-2"><span class="section-number-3">7.2</span> Current Workflow</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>shell script 
<ul class="org-ul">
<li>python script getdata.py
</li>
<li>XSLT transformation over dc metadata records
</li>
</ul>
</li>
<li>show files
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-7-3" class="outline-3">
<h3 id="sec-7-3"><span class="section-number-3">7.3</span> Future directions</h3>
<div class="outline-text-3" id="text-7-3">
<ul class="org-ul">
<li>CL package for OAI-PMH (forget Python!)
</li>
<li>retrieve using mets instead of dc
</li>
<li>see <a href="http://bibliotecadigital.fgv.br/dspace/handle/10438/11552?show=full">http://bibliotecadigital.fgv.br/dspace/handle/10438/11552?show=full</a>
</li>
</ul>

<div class="org-src-container">

<pre class="src src-R">r1 <span style="color: #008b8b;">&lt;-</span> oaih_list_records(baseurl, prefix = <span style="color: #8b2252;">"mets"</span>, set = series[3,1])
r1[1,<span style="color: #8b2252;">"metadata"</span>]
</pre>
</div>
</div>
</div>
</div>
