---
title: Processando o DHBB -- o que temos até agora?
layout: post
tags:
 - projects
---
#+PROPERTY: cache yes
#+PROPERTY: results output
#+OPTIONS: toc:nil
#+PROPERTY: exports both

Hoje recebi uma mensagem bem interessante perguntando sobre o que
tenho feito em relação ao processamento do [[http://cpdoc.fgv.br/acervo/dhbb][DHBB]]. Resolvi aproveitar
para escrever este post.

Na lista das minhas [[http://arademaker.github.io/publications.html][publicações]], interessados podem consultar todos os
artigos que escrevemos sobre trabalhos com o DHBB. Ao longo dos anos,
fizemos diferentes experimentos ‘exploratórios’ no DHBB, alguns com
objetivo, não de processar o DHBB, mas de usa-lo para expandir
recursos que precisávamos criar para o processamento de textos em PT,
além mesmo do DHBB.

Recentemente, agora sim ja tendo recursos para tal, pudemos começar a
pensar no processamento do DHBB propriamente dito. Mas os recursos
necessários ainda estão longe de completos ou com tamanho adequado
para garantir boa qualidade, logo penso no processamento do DHBB como
um projeto de longo prazo que passará por refinamentos
sucessivos. Quando digo recursos, falo de coisas como:

1. Nossa [[http://openwordnet-pt.org][WordNet do português]].
2. O corpus [[http://github.com/universaldependencies/UD_Portuguese-Bosque][Bosque]] e os outros de PT que também mantemos como parte do
   projeto [[http://universaldependencies.org][Universal Dependencies]]. Estes dados usados para parser das
   sentenças.
3. O [[https://github.com/LFG-PTBR/MorphoBr][dicionário morfológico]].
4. O [[https://github.com/System-T/UniversalPropositions/tree/master/UP_Portuguese-Bosque][corpus com anotações de papéis semânticos]] para treino de SRL.

No [[http://arademaker.github.io/bibliography/propor-2020.html][artigo do último PROPOR]], mostramos que mesmo a segmentação de
sentenças não é trivial e que certamente vários erros de análise
sintática (usamos o UDPipe) existem dado o reduzido tamanho do corpus
Bosque e diferenças entre o estilo textual do corpus Bosque
(jornalístico) para o DHBB (enciclopédico). O que estamos fazendo é
criar um workflow que permita o trabalho de longo prazo onde revisões
humanas sejam integradas em ciclos de treino e avaliação dos
diferentes componentes que estamos usando. Eu poderia falar muito mais
sobre as dificuldades reais na segmentação de sentenças, mas vou
deixar isso para outros posts.

Para a segmentação de sentenças, temos tido melhores resultados com o
[[https://opennlp.apache.org][OpenNLP]], que comparamos com outras ferramentas no paper mencionado
acima, dentre elas o [[https://github.com/TALP-UPC/FreeLing][Freeling]], que encontramos muitas limitações de
configuração (abreviações no final de sentenças são um problema sem
solução para o Freeling). Mas aprendemos que o modelo de segmentação
[[http://opennlp.sourceforge.net/models-1.5/][disponibilizado]] em também erra várias sentenças do DHBB. Com isso,
retreinamos o OpenNLP com um subconjunto de sentenças do DHBB
segmentadas manualmente. Ainda estamos investigando alternativa ao
OpenNLP, talvez o próprio UDPipe ou NLTK, este [[https://github.com/cpdoc/dhbb-nlp/issues/39][issue]] trata deste
assunto. Não sou muito fã de Python, mas começamos a testar o NLTK
principalmente por causa do modulo de segmentação de sentenças que
implementa o algoritmo Punkt, especialmente util para lidar com
abreviações.

Falando da análise sintática. Treinamos o [[http://lindat.mff.cuni.cz/services/udpipe/][UDPipe]] com o corpus
[[https://github.com/universaldependencies/UD_Portuguese-Bosque][UD_Portuguese-Bosque]], que estamos ainda constantemente revisando,
expandido com as sentenças do DHBB analisadas que já revisamos. Com
este modelo, aplicamos no restante do DHBB e repetimos o ciclo. Nas
últimas semanas, focamos nas sentenças dos primeiros parágrafos dos
verbetes, temos atualmente umas 200-300 sentenças revisadas. Estas
sentenças, quando adicionadas ao Bosque, conseguiram ‘ensinar’ o
UDPipe a análise dos primeiros parágrafos dos verbetes (os parágrafos
que falam sobre relações familiares).

Em paralelo ao nosso trabalho, Diana e Suemi incluiram o DHBB no
acervo da [[https://www.linguateca.pt][Linguateca]], seguindo os métodos de processamento dos textos
que a Linguateca adota. Eu não estou mais participando diretamente
deste esforço embora tenha colaborado em um [[http://arademaker.github.io/bibliography/dhn-2019.html][artigo do ano
passado]]. Para mim, a principal limitação da abordagem da Linguateca
são as ferramentas adotadas. Algumas proprietárias, como o parser
[[http://visl.sdu.dk/visl/pt/parsing/automatic/dependency.php][PALAVRAS]] que produzem analises pouco ’standard’ e consequentemente de
difícil integração com outras ferramentas. Por isso prefiro outras
abordagens.

Quanto a colaboração com interessados. Uma coisa que precisamos muito
é UI. Nosso ciclo de revisão e treino com human-in-the-loop precisa
evoluir para que os usuários possam fazer revisões em interfaces mais
visuais sem lidar com arquivos. Isto permitiria maior agilidade em
cada ciclo de revisão. Na parte mais de NLP, ter a análise sintática é
apenas uma etapa para extração de informações. Precisamos de muitas
outras ‘camadas’ de anotação como SRL, NER, expressões temporais
etc. E talvez continuar combinando técnicas. Por exemplo, para
reconhecimento de entidades nomeadas, temos um modelo treinado no [[https://www.ibm.com/cloud/watson-knowledge-studio][IBM
Watson Knowledge Studio]] (WKS). Nosso [[http://dhbb.mybluemix.net/dhbb/home][demo]] ainda está super básico e
também precisa evoluir. Mas o modelo treinado no WKS ainda precisa ser
melhorado e ainda precisamos pensar como integrar as anotações do WKS
com estes processamentos mais ‘linguisticos’ que estamos fazendo
agora. Enfim, muita coisa para fazer.

Com sorte, o que está sendo feito pode ser útil para processamento de
outros textos no futuro, além do DHBB, como wikipedia.

São vários colaboradores participando deste projeto, em diferentes
momentos e de diferentes formas. Em especial, não posso deixar de
mencionar a Valeria de Paiva, amiga que me introduziu na área de
linguistica computacional e me apresentou tantas pessoas ao longo
destes anos.
