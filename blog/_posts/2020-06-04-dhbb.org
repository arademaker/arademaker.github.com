---
title: DHBB -- nossos passos até agora
layout: post
tags:
 - projects
---
#+PROPERTY: cache yes
#+PROPERTY: results output
#+OPTIONS: toc:nil
#+PROPERTY: exports both

Hoje recebi uma mensagem bem interessante perguntando sobre o que
tenho feito em relação ao processamento do [[http://cpdoc.fgv.br/acervo/dhbb][DHBB]]. Resolvi aproveitar
para escrever este post.

Na lista das minhas publicações,
http://arademaker.github.io/publications.html, interessados podem
consultar todos os artigos que escrevemos sobre trabalhos com o
DHBB. Ao longo dos anos, fizemos diferentes experimentos
‘exploratórios’ no DHBB, alguns com objetivo, não de processar o DHBB,
mas de usa-lo para expandir recursos que precisávamos criar para o
processamento de textos em PT, além mesmo do DHBB.

Recentemente, agora sim ja tendo recursos para tal, pudemos começar a
pensar no processamento do DHBB propriamente dito. Mas os recursos
necessários ainda estão longe de completos ou com tamanho adequado
para garantir boa qualidade, logo penso no processamento do DHBB como
um projeto de longo prazo que passará por refinamentos
sucessivos. Quando digo recursos, falo de coisas como:

1. Nossa WordNet do português (http://openwordnet-pt.org).
2. O corpus
   http://github.com/universaldependencies/UD_Portuguese-Bosque e os
   outros de PT que também mantemos como parte do projeto
   http://universaldependencies.org. Estes dados usados para parser
   das sentenças.
3. https://github.com/LFG-PTBR/MorphoBr dicionário morfológico.
4. https://github.com/System-T/UniversalPropositions/tree/master/UP_Portuguese-Bosque
   para SRL.

No [[http://arademaker.github.io/bibliography/propor-2020.html][artigo do último PROPOR]], mostramos que mesmo a segmentação de
sentenças não é trivial e que certamente vários erros de análise
sintática (usamos o UDPipe) existem dado o reduzido tamanho do corpus
Bosque e diferenças entre o estilo textual do corpus Bosque
(jornalístico) para o DHBB (enciclopédico). O que estamos fazendo é
criar um workflow que permita o trabalho de longo prazo onde revisões
humanas sejam integradas em ciclos de treino e avaliação dos
diferentes componentes que estamos usando. Eu poderia falar muito mais
sobre as dificuldades reais na segmentação de sentenças, mas vou
deixar isso para outros posts.

Para a segmentação de sentenças, temos tido melhores resultados com o
[[https://opennlp.apache.org][OpenNLP]], que comparamos com outras ferramentas no paper mencionado
acima, dentre elas o [[https://github.com/TALP-UPC/FreeLing][Freeling]], que encontramos muitas limitações de
configuração (abreviações no final de sentenças são um problema sem
solução para o Freeling). Mas aprendemos que o modelo de segmentação
disponibilizado em http://opennlp.sourceforge.net/models-1.5/ também
erra várias sentenças do DHBB. Com isso, retreinamos o OpenNLP com um
subconjunto de sentenças do DHBB segmentadas manualmente. Ainda
estamos investigando alternativa ao OpenNLP, talvez o próprio UDPipe
ou NLTK, o issue https://github.com/cpdoc/dhbb-nlp/issues/39 trata
deste assunto. Não sou muito fã de Python, mas começamos a testar o
NLTK principalmente por causa do modulo de segmentação de sentenças
que implementa o algoritmo Punkt, especialmente util para lidar com
abreviações.

Falando da análise sintática. Treinamos o [[http://lindat.mff.cuni.cz/services/udpipe/][UDPipe]] com o corpus
[[https://github.com/universaldependencies/UD_Portuguese-Bosque][UD_Portuguese-Bosque]], que estamos ainda constantemente revisando,
expandido com as sentenças do DHBB analisadas que já revisamos. Com
este modelo, aplicamos no restante do DHBB e repetimos o ciclo. Nas
últimas semanas, focamos nas sentenças dos primeiros parágrafos dos
verbetes, temos atualmente umas 200-300 sentenças revisadas. Estas
sentenças, quando adicionadas ao Bosque, conseguiram ‘ensinar’ o
UDPipe a análise dos primeiros parágrafos dos verbetes (os parágrafos
que falam sobre relações familiares).

Em paralelo ao nosso trabalho, Diana e Suemi incluiram o DHBB no
acervo da https://www.linguateca.pt, seguindo os métodos de
processamento dos textos que a Linguateca adota. Eu não estou mais
participando diretamente deste esforço embora tenha colaborado em um
[[http://arademaker.github.io/bibliography/dhn-2019.html][artigo do ano passado]]. Para mim, a principal limitação da abordagem da
Linguateca são as ferramentas adotadas. Algumas proprietárias, como o
parser [[http://visl.sdu.dk/visl/pt/parsing/automatic/dependency.php][PALAVRAS]] que produzem analises pouco ’standard’ e
consequentemente de difícil integração com outras ferramentas. Por
isso prefiro outras abordagens.

Quanto a colaboração com interessados. Uma coisa que precisamos muito
é UI. Nosso ciclo de revisão e treino com human-in-the-loop precisa
evoluir para que os usuários possam fazer revisões em interfaces mais
visuais sem lidar com arquivos. Isto permitiria maior agilidade em
cada ciclo de revisão. Na parte mais de NLP, ter a análise sintática é
apenas uma etapa para extração de informações. Precisamos de muitas
outras ‘camadas’ de anotação como SRL, NER, expressões temporais
etc. E talvez continuar combinando técnicas. Por exemplo, para
reconhecimento de entidades nomeadas, temos um modelo treinado no
https://www.ibm.com/cloud/watson-knowledge-studio (WKS). Demo em
http://dhbb.mybluemix.net/dhbb/home que ainda está super básico e
também precisa evoluir. Mas o modelo treinado no WKS ainda precisa ser
melhorado e ainda precisamos pensar como integrar as anotações do WKS
com estes processamentos mais ‘linguisticos’ que estamos fazendo
agora. Enfim, muita coisa para fazer.

Com sorte, o que está sendo feito pode ser útil para processamento de
outros textos no futuro, além do DHBB, como wikipedia.

